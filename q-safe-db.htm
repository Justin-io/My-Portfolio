<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Academic Document</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
            counter-reset: section;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
            padding: 40px;
        }

        header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #e0e0e0;
        }

        h1 {
            font-size: 2.5rem;
            color: #2c3e50;
            margin-bottom: 10px;
        }

        .subtitle {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 15px;
        }

        .author-info {
            display: flex;
            justify-content: space-between;
            margin-top: 15px;
            font-size: 0.9rem;
            color: #7f8c8d;
        }

        .section {
            margin-bottom: 30px;
            padding: 20px 0;
            border-bottom: 1px solid #eee;
        }

        .section-title {
            font-size: 1.8rem;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 1px solid #eee;
            color: #2c3e50;
            position: relative;
            counter-increment: section;
        }

        .section-title::before {
            content: counter(section) ". ";
            color: #3498db;
        }

        .content {
            padding: 10px 0;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .equation {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background-color: #f9f9f9;
            border-left: 4px solid #3498db;
            font-size: 1.2rem;
        }

        .table-container {
            overflow-x: auto;
            margin: 20px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 0.9rem;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        .highlight {
            background-color: #e3f2fd;
            padding: 2px 4px;
            border-radius: 3px;
        }

        .definition {
            border-left: 4px solid #27ae60;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #f8fff8;
        }

        .theorem {
            border-left: 4px solid #e74c3c;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #fff8f8;
        }

        .proof {
            border-left: 4px solid #9b59b6;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #f8f8ff;
            font-style: italic;
        }

        .references {
            margin-top: 30px;
        }

        .ref-title {
            font-weight: bold;
            margin-bottom: 10px;
        }

        .ref-item {
            margin-bottom: 8px;
            padding-left: 20px;
            text-indent: -20px;
        }

        footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            color: #7f8c8d;
            font-size: 0.9rem;
        }

        @media print {
            body {
                background-color: white;
                padding: 0;
            }
            
            .container {
                box-shadow: none;
                padding: 20px;
                max-width: 100%;
            }
            
            button {
                display: none;
            }
        }

        .print-btn {
            display: block;
            margin: 20px auto;
            padding: 12px 30px;
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background-color 0.3s;
        }

        .print-btn:hover {
            background-color: #2980b9;
        }

        .print-btn i {
            margin-right: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Mathematical Analysis and Applications</h1>
            <div class="subtitle">A Comprehensive Study of Fundamental Concepts</div>
            <div class="author-info">
                <div>Dr. Jane Smith</div>
                <div>Department of Mathematics</div>
                <div>University of Excellence</div>
            </div>
        </header>

        <section class="section">
    <h2 class="section-title">Quantum Bits (Qubits)</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing Fundamentals</em></p>
        
        <h3>Definition and Properties of Qubits</h3>
        <h4>What is a Qubit?</h4>
        <p>A <strong>qubit</strong> (quantum bit) is the fundamental unit of quantum information, analogous to the classical bit in classical computing.</p>
        <p>Unlike a classical bit, which can only be in state <strong>0</strong> or <strong>1</strong>, a qubit can exist in a <strong>superposition</strong> of both states simultaneously.</p>
        
        <h4>Key Properties of Qubits</h4>
        <ol>
            <li><strong>Superposition</strong>: Can be in a linear combination of |0⟩ and |1⟩.</li>
            <li><strong>Entanglement</strong>: Qubits can be correlated in ways that classical bits cannot (though this is more relevant for multi-qubit systems).</li>
            <li><strong>Measurement Collapse</strong>: When measured, the qubit collapses probabilistically to either |0⟩ or |1⟩.</li>
            <li><strong>No-Cloning Theorem</strong>: An unknown quantum state cannot be perfectly copied.</li>
            <li><strong>Unitary Evolution</strong>: Qubit states evolve via reversible, unitary operations (quantum gates).</li>
        </ol>
        
        <h4>Physical Realizations</h4>
        <p>Qubits can be implemented using:</p>
        <ul>
            <li>Electron or nuclear spin (e.g., in NMR or quantum dots)</li>
            <li>Photon polarization</li>
            <li>Superconducting circuits (e.g., transmon qubits)</li>
            <li>Trapped ions</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Superposition Principle and Its Mathematical Representation</h2>
    <div class="content">
        <h3>Superposition Principle</h3>
        <p>A qubit can be in a state that is a <strong>linear combination</strong> of the basis states |0⟩ and |1⟩:</p>
        <div class="equation">
            \[ |\psi\rangle = \alpha |0\rangle + \beta |1\rangle \]
        </div>
        <p>where:</p>
        <ul>
            <li>\(\alpha, \beta \in \mathbb{C}\) (complex numbers),</li>
            <li>\(|\alpha|^2 + |\beta|^2 = 1\) (normalization condition).</li>
        </ul>
        
        <h3>Interpretation</h3>
        <ul>
            <li>\(|\alpha|^2\) = probability of measuring the qubit as <strong>0</strong>.</li>
            <li>\(|\beta|^2\) = probability of measuring the qubit as <strong>1</strong>.</li>
            <li>The relative <strong>phase</strong> between \(\alpha\) and \(\beta\) is physically significant (e.g., for interference).</li>
        </ul>
        
        <h3>Example</h3>
        <p>Equal superposition state:</p>
        <div class="equation">
            \[ |+\rangle = \frac{1}{\sqrt{2}}|0\rangle + \frac{1}{\sqrt{2}}|1\rangle \]
        </div>
        <p>→ 50% chance of measuring 0 or 1.</p>
        
        <p>Another example:</p>
        <div class="equation">
            \[ |\psi\rangle = \frac{\sqrt{3}}{2}|0\rangle + \frac{1}{2}|1\rangle \]
        </div>
        <p>→ Probability of 0: \( \left|\frac{\sqrt{3}}{2}\right|^2 = \frac{3}{4} \),<br>
        Probability of 1: \( \left|\frac{1}{2}\right|^2 = \frac{1}{4} \).</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Bloch Sphere Visualization of Qubit States</h2>
    <div class="content">
        <h3>Why the Bloch Sphere?</h3>
        <p>Any single-qubit pure state can be represented as a point on the surface of a unit sphere called the <strong>Bloch sphere</strong>.</p>
        <p>Provides an intuitive geometric representation of qubit states.</p>
        
        <h3>Mathematical Mapping</h3>
        <p>A general qubit state can be written as:</p>
        <div class="equation">
            \[ |\psi\rangle = \cos\left(\frac{\theta}{2}\right)|0\rangle + e^{i\phi}\sin\left(\frac{\theta}{2}\right)|1\rangle \]
        </div>
        <p>where:</p>
        <ul>
            <li>\(0 \leq \theta \leq \pi\)</li>
            <li>\(0 \leq \phi < 2\pi\)</li>
        </ul>
        
        <h3>Coordinates on the Bloch Sphere</h3>
        <ul>
            <li><strong>North pole</strong>: |0⟩</li>
            <li><strong>South pole</strong>: |1⟩</li>
            <li><strong>Equator</strong>: Superposition states with equal probabilities (e.g., |+⟩, |−⟩, |i⟩, |−i⟩)</li>
            <li><strong>θ (theta)</strong>: Polar angle from z-axis → determines probability amplitudes.</li>
            <li><strong>φ (phi)</strong>: Azimuthal angle in xy-plane → determines relative phase.</li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: Global phase (e.g., multiplying the entire state by \(e^{i\gamma}\)) has no physical effect and is ignored.
        </div>
        
        <h3>Visualization</h3>
        <p>X, Y, Z axes correspond to measurement bases:</p>
        <ul>
            <li>Z-basis: {|0⟩, |1⟩}</li>
            <li>X-basis: {|+⟩, |−⟩}</li>
            <li>Y-basis: {|i⟩, |−i⟩}</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Measurement and Collapse of Quantum States</h2>
    <div class="content">
        <h3>Quantum Measurement Postulate</h3>
        <p>When a qubit in state \(|\psi\rangle = \alpha|0\rangle + \beta|1\rangle\) is measured in the computational (Z) basis:</p>
        <ul>
            <li>Outcome is <strong>0</strong> with probability \(|\alpha|^2\)</li>
            <li>Outcome is <strong>1</strong> with probability \(|\beta|^2\)</li>
        </ul>
        <p><strong>Immediately after measurement</strong>, the state <strong>collapses</strong> to the observed basis state:</p>
        <ul>
            <li>If result is 0 → state becomes |0⟩</li>
            <li>If result is 1 → state becomes |1⟩</li>
        </ul>
        
        <h3>Irreversibility</h3>
        <p>Measurement is <strong>non-unitary</strong> and <strong>irreversible</strong>—information about the original superposition is lost.</p>
        
        <h3>Basis Dependence</h3>
        <p>Measurement can be performed in any orthonormal basis (e.g., X-basis).</p>
        <p>Example: Measuring |+⟩ in X-basis always yields "+", but in Z-basis yields 0 or 1 with 50% probability each.</p>
        
        <h3>Expectation Value</h3>
        <p>For an observable (e.g., Pauli-Z operator), the expectation value is:</p>
        <div class="equation">
            \[ \langle Z \rangle = \langle \psi | Z | \psi \rangle = |\alpha|^2 - |\beta|^2 \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Comparison with Classical Bits</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Classical Bit</th>
                        <th>Qubit</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>State</td>
                        <td>0 <strong>or</strong> 1</td>
                        <td>\(\alpha|0\rangle + \beta|1\rangle\) (superposition)</td>
                    </tr>
                    <tr>
                        <td>State Space</td>
                        <td>Discrete (2 states)</td>
                        <td>Continuous (infinite states on Bloch sphere)</td>
                    </tr>
                    <tr>
                        <td>Measurement</td>
                        <td>Reveals existing value (non-destructive)</td>
                        <td>Probabilistic; <strong>collapses</strong> the state</td>
                    </tr>
                    <tr>
                        <td>Copying</td>
                        <td>Can be copied perfectly</td>
                        <td><strong>Cannot</strong> be copied (No-Cloning Theorem)</td>
                    </tr>
                    <tr>
                        <td>Information Capacity</td>
                        <td>1 bit per bit</td>
                        <td>Infinite info in state, but only <strong>1 bit</strong> extractable per measurement</td>
                    </tr>
                    <tr>
                        <td>Operations</td>
                        <td>Logic gates (AND, OR, NOT)</td>
                        <td>Reversible <strong>unitary gates</strong> (e.g., Hadamard, Pauli)</td>
                    </tr>
                    <tr>
                        <td>Parallelism</td>
                        <td>No inherent parallelism</td>
                        <td><strong>Quantum parallelism</strong>: operations on all states in superposition</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Key Insight</strong>: While a single qubit holds more <em>potential</em> information than a classical bit, <strong>only one classical bit</strong> can be extracted per measurement due to collapse. The power of quantum computing arises from <strong>interference</strong> and <strong>entanglement</strong> across many qubits—not from storing more data per qubit.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <ul>
            <li>A <strong>qubit</strong> generalizes the classical bit using quantum mechanics.</li>
            <li>It leverages <strong>superposition</strong> to process multiple states at once.</li>
            <li>The <strong>Bloch sphere</strong> offers a geometric view of all possible single-qubit states.</li>
            <li><strong>Measurement</strong> is probabilistic and destructive to superposition.</li>
            <li>Qubits are <strong>not just probabilistic bits</strong>—phase and interference are crucial for quantum advantage.</li>
        </ul>
    </div>
</section>
<section class="section">
    <h2 class="section-title">Quantum Entanglement</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Information and Computing</em></p>
        
        <h3>Definition and Properties of Entangled States</h3>
        <h4>What is Quantum Entanglement?</h4>
        <p><strong>Entanglement</strong> is a uniquely quantum phenomenon where two or more particles become <strong>correlated</strong> in such a way that the quantum state of each particle <strong>cannot be described independently</strong> of the others—even when separated by large distances.</p>
        <p>The combined system is described by a <strong>single quantum state</strong>, but individual subsystems <strong>do not have definite states</strong> on their own.</p>
        
        <h4>Formal Definition</h4>
        <p>A pure bipartite state \(|\psi\rangle_{AB}\) is <strong>entangled</strong> if it <strong>cannot</strong> be written as a <strong>product state</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle_{AB} \neq |\phi\rangle_A \otimes |\chi\rangle_B \]
        </div>
        <p>If such a factorization is possible, the state is <strong>separable</strong> (not entangled).</p>
        
        <div class="definition">
            <strong>Example of entangled state</strong>:<br>
            \(|\Phi^+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)\) → cannot be split into individual qubit states.
        </div>
        
        <h4>Key Properties of Entangled States</h4>
        <ol>
            <li><strong>Non-separability</strong>: The whole system's state is global; parts lack individual pure states.</li>
            <li><strong>Non-locality</strong>: Measurement outcomes on one subsystem <strong>instantaneously affect</strong> the other (though no faster-than-light communication is possible).</li>
            <li><strong>Monogamy</strong>: If qubit A is maximally entangled with B, it <strong>cannot</strong> be entangled with C.</li>
            <li><strong>Conservation under local unitary operations</strong>: Entanglement is invariant under local operations (but can be changed by measurements or noise).</li>
            <li><strong>Fragility</strong>: Entanglement is easily destroyed by <strong>decoherence</strong> (interaction with environment).</li>
        </ol>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Bell States and Their Significance</h2>
    <div class="content">
        <h3>What are Bell States?</h3>
        <p>The <strong>Bell states</strong> (or <strong>EPR pairs</strong>) are four specific <strong>maximally entangled</strong> two-qubit states that form an orthonormal basis for the 2-qubit Hilbert space (\(\mathbb{C}^2 \otimes \mathbb{C}^2\)).</p>
        
        <h3>The Four Bell States</h3>
        <div class="equation">
            \[ \begin{aligned}
            |\Phi^+\rangle &= \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle) \\
            |\Phi^-\rangle &= \frac{1}{\sqrt{2}}(|00\rangle - |11\rangle) \\
            |\Psi^+\rangle &= \frac{1}{\sqrt{2}}(|01\rangle + |10\rangle) \\
            |\Psi^-\rangle &= \frac{1}{\sqrt{2}}(|01\rangle - |10\rangle)
            \end{aligned} \]
        </div>
        
        <h3>Significance</h3>
        <ul>
            <li><strong>Maximal entanglement</strong>: Each Bell state has <strong>1 ebit</strong> (entanglement bit) of entanglement.</li>
            <li><strong>Basis for protocols</strong>: Used in quantum teleportation, superdense coding, and entanglement swapping.</li>
            <li><strong>Test of quantum non-locality</strong>: Violate <strong>Bell inequalities</strong>, distinguishing quantum mechanics from local hidden-variable theories.</li>
            <li><strong>Resource states</strong>: Serve as fundamental resources in quantum communication and computation.</li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: \(|\Psi^-\rangle\) is also called the <strong>singlet state</strong>—antisymmetric under particle exchange.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Non-locality and Quantum Correlations</h2>
    <div class="content">
        <h3>What is Non-locality?</h3>
        <p><strong>Quantum non-locality</strong> refers to correlations between distant particles that <strong>cannot be explained</strong> by any <strong>local realistic theory</strong> (i.e., theories assuming:
        <ul>
            <li><strong>Locality</strong>: No influence faster than light.</li>
            <li><strong>Realism</strong>: Physical properties exist prior to measurement.)</li>
        </ul>
        
        <h3>Bell's Theorem (1964)</h3>
        <p>John Bell showed that <strong>any local hidden-variable theory</strong> must obey certain statistical constraints (<strong>Bell inequalities</strong>).</p>
        <p>Quantum mechanics <strong>violates</strong> these inequalities.</p>
        <p>Experiments (e.g., by Aspect, 1982; loophole-free tests, 2015) confirm these violations → <strong>local realism is false</strong>.</p>
        
        <h3>Example: CHSH Inequality</h3>
        <p>For observables \(A_1, A_2\) on qubit A and \(B_1, B_2\) on qubit B:</p>
        <div class="equation">
            \[ S = \langle A_1B_1 \rangle + \langle A_1B_2 \rangle + \langle A_2B_1 \rangle - \langle A_2B_2 \rangle \]
        </div>
        <ul>
            <li><strong>Local hidden-variable theories</strong>: \(|S| \leq 2\)</li>
            <li><strong>Quantum mechanics</strong>: \(|S| \leq 2\sqrt{2} \approx 2.828\) (Tsirelson's bound)</li>
            <li>Achieved using entangled states (e.g., \(|\Phi^+\rangle\)) and specific measurement bases.</li>
        </ul>
        
        <h3>Important Clarifications</h3>
        <ul>
            <li><strong>No faster-than-light communication</strong>: Outcomes are random; only <strong>correlations</strong> are non-local. Cannot transmit information instantaneously (<strong>no-signaling theorem</strong>).</li>
            <li><strong>Correlation ≠ Causation</strong>: Measurement on A doesn't "cause" B's state—it reveals pre-existing (but non-classical) correlation.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Mathematical Representation of Entanglement</h2>
    <div class="content">
        <h3>Pure Bipartite States</h3>
        <p>A state \(|\psi\rangle_{AB}\) is <strong>entangled</strong> iff its <strong>Schmidt rank > 1</strong>.</p>
        <p><strong>Schmidt decomposition</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle_{AB} = \sum_i \lambda_i |u_i\rangle_A \otimes |v_i\rangle_B \]
        </div>
        <p>where \(\lambda_i \geq 0\), \(\sum_i \lambda_i^2 = 1\).</p>
        <p><strong>Entangled</strong> ⇔ more than one non-zero \(\lambda_i\).</p>
        
        <h3>Mixed States and Entanglement Criteria</h3>
        <p>For mixed states (\(\rho_{AB}\)), entanglement is harder to detect:</p>
        <ul>
            <li><strong>Separable state</strong>: \(\rho_{AB} = \sum_k p_k \, \rho_A^{(k)} \otimes \rho_B^{(k)}\), with \(p_k \geq 0\), \(\sum_k p_k = 1\).</li>
            <li><strong>Entangled</strong> ⇔ not separable.</li>
        </ul>
        
        <h4>Entanglement Detection Tools</h4>
        <ol>
            <li><strong>Positive Partial Transpose (PPT) Criterion</strong> (Peres-Horodecki):
            <ul>
                <li>If \(\rho^{T_B}\) (partial transpose over B) has <strong>negative eigenvalues</strong> → state is entangled.</li>
                <li><strong>Necessary and sufficient</strong> for 2×2 and 2×3 systems.</li>
            </ul>
            </li>
            <li><strong>Entanglement Witnesses</strong>: Hermitian operators \(W\) such that \(\text{Tr}(W\sigma) \geq 0\) for all separable \(\sigma\), but \(\text{Tr}(W\rho) < 0\) for some entangled \(\rho\).</li>
        </ol>
        
        <h3>Quantifying Entanglement (Pure States)</h3>
        <ul>
            <li><strong>Entanglement entropy</strong>: \(S(\rho_A) = -\text{Tr}(\rho_A \log_2 \rho_A)\), where \(\rho_A = \text{Tr}_B(|\psi\rangle\langle\psi|)\).
            <ul>
                <li>\(S = 0\) → separable</li>
                <li>\(S = 1\) → maximally entangled (e.g., Bell states)</li>
            </ul>
            </li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications in Quantum Information Processing</h2>
    <div class="content">
        <h3>1. Quantum Teleportation</h3>
        <ul>
            <li><strong>Goal</strong>: Transmit an unknown quantum state using classical communication and shared entanglement.</li>
            <li><strong>Resource</strong>: One shared Bell pair (e.g., \(|\Phi^+\rangle\)) + 2 classical bits.</li>
            <li><strong>Process</strong>:
                <ol>
                    <li>Alice performs Bell measurement on her qubit (state to send) and her half of Bell pair.</li>
                    <li>Sends 2 classical bits to Bob.</li>
                    <li>Bob applies correction based on bits → recovers original state.</li>
                </ol>
            </li>
            <li><strong>Key point</strong>: No cloning; original state is destroyed.</li>
        </ul>
        
        <h3>2. Superdense Coding</h3>
        <ul>
            <li><strong>Goal</strong>: Send <strong>two classical bits</strong> by transmitting <strong>one qubit</strong>.</li>
            <li><strong>Resource</strong>: One shared Bell pair.</li>
            <li><strong>Process</strong>:
                <ul>
                    <li>Alice applies one of four unitary operations to her qubit (encoding 00, 01, 10, 11).</li>
                    <li>Sends her qubit to Bob.</li>
                    <li>Bob performs Bell measurement → decodes 2 bits.</li>
                </ul>
            </li>
            <li><strong>Demonstrates</strong>: Entanglement enhances classical communication capacity.</li>
        </ul>
        
        <h3>3. Quantum Key Distribution (QKD)</h3>
        <ul>
            <li><strong>Example</strong>: E91 protocol (Ekert, 1991)
            <ul>
                <li>Uses entangled photon pairs.</li>
                <li>Security based on <strong>violation of Bell inequalities</strong>—eavesdropping disturbs entanglement and is detectable.</li>
            </ul>
            </li>
        </ul>
        
        <h3>4. Quantum Computing</h3>
        <ul>
            <li><strong>Entangling gates</strong> (e.g., CNOT) create entanglement between qubits.</li>
            <li>Essential for quantum speedup (e.g., in Shor's algorithm, Grover's search).</li>
            <li><strong>Measurement-based quantum computing</strong> (MBQC): Computation driven by measurements on a highly entangled "cluster state".</li>
        </ul>
        
        <h3>5. Quantum Metrology & Sensing</h3>
        <p>Entangled states (e.g., NOON states) enable <strong>Heisenberg-limited precision</strong>, surpassing classical limits in interferometry and imaging.</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Takeaway</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Entanglement</td>
                        <td>Non-separable quantum correlations with no classical analog.</td>
                    </tr>
                    <tr>
                        <td>Bell States</td>
                        <td>Maximally entangled 2-qubit states; foundational for protocols.</td>
                    </tr>
                    <tr>
                        <td>Non-locality</td>
                        <td>Proven via Bell inequality violations; rules out local hidden variables.</td>
                    </tr>
                    <tr>
                        <td>Math Representation</td>
                        <td>Schmidt decomposition (pure), PPT criterion (mixed), entanglement entropy.</td>
                    </tr>
                    <tr>
                        <td>Applications</td>
                        <td>Teleportation, superdense coding, QKD, quantum computing, enhanced sensing.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Remember</strong>: Entanglement is a <strong>resource</strong>, not just a curiosity—it enables tasks impossible classically.
        </div>
    </div>
</section>

        <section class="section">
    <h2 class="section-title">Quantum Gates and Circuits</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing</em></p>
        
        <h3>Single-Qubit Gates</h3>
        <p>Single-qubit gates are <strong>unitary operators</strong> acting on a single qubit. They correspond to rotations on the <strong>Bloch sphere</strong>.</p>
        
        <h4>Pauli Gates</h4>
        <p>These are fundamental and form the Pauli group.</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Gate</th>
                        <th>Matrix</th>
                        <th>Action on Basis States</th>
                        <th>Bloch Sphere Rotation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>X (NOT)</strong></td>
                        <td>\(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\)</td>
                        <td>\(X|0\rangle = |1\rangle\), \(X|1\rangle = |0\rangle\)</td>
                        <td>180° about <strong>x-axis</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Y</strong></td>
                        <td>\(\begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix}\)</td>
                        <td>\(Y|0\rangle = i|1\rangle\), \(Y|1\rangle = -i|0\rangle\)</td>
                        <td>180° about <strong>y-axis</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Z (Phase-flip)</strong></td>
                        <td>\(\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}\)</td>
                        <td>\(Z|0\rangle = |0\rangle\), \(Z|1\rangle = -|1\rangle\)</td>
                        <td>180° about <strong>z-axis</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Note</strong>: \(X, Y, Z\) are <strong>Hermitian</strong> and <strong>unitary</strong>: \(X^\dagger = X\), \(X^2 = I\), etc.
        </div>
        
        <h4>Hadamard Gate (H)</h4>
        <p>Creates superposition from computational basis:</p>
        <div class="equation">
            \[ H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \]
        </div>
        <p>Action:</p>
        <div class="equation">
            \[ H|0\rangle = |+\rangle = \frac{|0\rangle + |1\rangle}{\sqrt{2}}, \quad
            H|1\rangle = |-\rangle = \frac{|0\rangle - |1\rangle}{\sqrt{2}} \]
        </div>
        <ul>
            <li>Also maps \(|+\rangle \to |0\rangle\), \(|-\rangle \to |1\rangle\) → <strong>basis change</strong> between Z and X.</li>
            <li>Bloch sphere: Rotation by 180° about axis \((\hat{x} + \hat{z})/\sqrt{2}\).</li>
        </ul>
        
        <h4>Phase Gate (S) and T Gate</h4>
        <p><strong>Phase (S) gate</strong>:</p>
        <div class="equation">
            \[ S = \begin{pmatrix} 1 & 0 \\ 0 & i \end{pmatrix}, \quad S^2 = Z \]
        </div>
        <p>Adds a <strong>+90° phase</strong> to |1⟩: \(S|1\rangle = i|1\rangle\)</p>
        
        <p><strong>T gate (π/8 gate)</strong>:</p>
        <div class="equation">
            \[ T = \begin{pmatrix} 1 & 0 \\ 0 & e^{i\pi/4} \end{pmatrix}, \quad T^2 = S \]
        </div>
        <p>Adds <strong>+45° phase</strong> to |1⟩.</p>
        
        <div class="definition">
            These are <strong>non-Clifford</strong> gates (T is especially important for universality).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Multi-Qubit Gates</h2>
    <div class="content">
        <p>Multi-qubit gates enable <strong>entanglement</strong> and <strong>conditional operations</strong>.</p>
        
        <h3>CNOT (Controlled-NOT) Gate</h3>
        <ul>
            <li><strong>2-qubit gate</strong>: flips target qubit if control is |1⟩.</li>
            <li>Matrix (in basis |00⟩, |01⟩, |10⟩, |11⟩):</li>
        </ul>
        <div class="equation">
            \[ \text{CNOT} = 
            \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 1 & 0
            \end{pmatrix} \]
        </div>
        <ul>
            <li>Action:</li>
        </ul>
        <div class="equation">
            \[ \text{CNOT}|a,b\rangle = |a, a \oplus b\rangle \]
        </div>
        <ul>
            <li><strong>Creates entanglement</strong>:<br>
            \( \text{CNOT}(H \otimes I)|00\rangle = \text{CNOT}|+\!0\rangle = \frac{|00\rangle + |11\rangle}{\sqrt{2}} = |\Phi^+\rangle \)</li>
        </ul>
        
        <h3>Toffoli Gate (CCNOT)</h3>
        <ul>
            <li><strong>3-qubit gate</strong>: flips target if <strong>both controls</strong> are |1⟩.</li>
            <li>Universal for <strong>classical reversible computing</strong>.</li>
            <li>Can implement AND, OR, etc.</li>
            <li>Quantum universality: With single-qubit gates, Toffoli enables universal quantum computation.</li>
            <li>Not natively available on all hardware; often <strong>decomposed</strong> into CNOTs and T gates.</li>
        </ul>
        
        <h3>SWAP Gate</h3>
        <p>Exchanges states of two qubits:</p>
        <div class="equation">
            \[ \text{SWAP}|a,b\rangle = |b,a\rangle \]
        </div>
        <p>Matrix:</p>
        <div class="equation">
            \[ \text{SWAP} =
            \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1
            \end{pmatrix} \]
        </div>
        <p>Can be built from <strong>3 CNOTs</strong>:</p>
        <div class="equation">
            \[ \text{SWAP} = \text{CNOT}_{12} \cdot \text{CNOT}_{21} \cdot \text{CNOT}_{12} \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Universal Gate Sets</h2>
    <div class="content">
        <p>A set of quantum gates is <strong>universal</strong> if it can approximate <strong>any unitary operation</strong> on \(n\) qubits to arbitrary precision.</p>
        
        <h3>Common Universal Sets</h3>
        <ol>
            <li><strong>{CNOT, H, T}</strong>
            <ul>
                <li>Most widely used in fault-tolerant quantum computing.</li>
                <li>T is non-Clifford; enables approximation of any rotation via <strong>Solovay-Kitaev theorem</strong>.</li>
            </ul>
            </li>
            <li><strong>{CNOT, single-qubit rotations}</strong>
            <ul>
                <li>Any single-qubit unitary can be decomposed into rotations: \(R_x(\theta), R_y(\phi), R_z(\lambda)\).</li>
            </ul>
            </li>
            <li><strong>{Toffoli, Hadamard}</strong>
            <ul>
                <li>Sufficient for universal quantum computation (though less efficient).</li>
            </ul>
            </li>
        </ol>
        
        <div class="definition">
            <strong>Key Insight</strong>: You need <strong>at least one non-Clifford gate</strong> (like T) to achieve universality. The Clifford group (generated by H, S, CNOT) alone is <strong>not universal</strong> (Gottesman-Knill theorem).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Circuit Diagrams and Notation</h2>
    <div class="content">
        <p>Quantum circuits provide a <strong>visual representation</strong> of quantum algorithms.</p>
        
        <h3>Basic Conventions</h3>
        <ul>
            <li><strong>Horizontal lines</strong>: Qubits (top to bottom = qubit 0 to qubit \(n-1\)).</li>
            <li><strong>Time flows left to right</strong>.</li>
            <li><strong>Gates</strong> are boxes or symbols applied to qubit lines.</li>
            <li><strong>Multi-qubit gates</strong> connect control and target with dots and ⊕.</li>
        </ul>
        
        <h3>Examples</h3>
        <ul>
            <li><strong>Hadamard on qubit 0</strong>:
            <pre>q0: ──H──</pre>
            </li>
            <li><strong>CNOT (q0 control, q1 target)</strong>:
            <pre>q0: ──●──
      │
q1: ──⊕──</pre>
            </li>
            <li><strong>Toffoli (q0,q1 control; q2 target)</strong>:
            <pre>q0: ──●──
      │
q1: ──●──
      │
q2: ──⊕──</pre>
            </li>
            <li><strong>Measurement</strong>:
            <pre>q0: ──M──</pre>
            </li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: Classical bits (from measurement) are often shown as double lines.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Circuit Depth and Complexity</h2>
    <div class="content">
        <h3>Circuit Depth</h3>
        <ul>
            <li><strong>Definition</strong>: The <strong>longest path</strong> (in time steps) from input to output, considering gates that can be applied <strong>in parallel</strong>.</li>
            <li><strong>Why it matters</strong>: Depth correlates with <strong>execution time</strong> and <strong>susceptibility to noise</strong> (decoherence).</li>
            <li>Example:
            <pre>q0: ──H──●──────
         │
q1: ─────⊕──H──</pre>
            - Depth = 2 (H and CNOT cannot be parallelized; second H on q1 comes after CNOT).</li>
        </ul>
        
        <h3>Circuit Size</h3>
        <p>Total number of gates (often counts CNOTs separately due to higher error rates).</p>
        
        <h3>Complexity Classes</h3>
        <ul>
            <li><strong>BQP (Bounded-error Quantum Polynomial time)</strong>: Problems solvable by quantum circuits of <strong>polynomial size and depth</strong> with bounded error.</li>
            <li><strong>Quantum advantage</strong> often hinges on <strong>exponentially smaller depth/size</strong> vs. classical counterparts (e.g., Shor's algorithm).</li>
        </ul>
        
        <h3>Optimization Goals</h3>
        <ul>
            <li>Minimize <strong>CNOT count</strong> (most error-prone gate on NISQ devices).</li>
            <li>Reduce <strong>depth</strong> to stay within coherence time.</li>
            <li>Use <strong>gate decomposition</strong> to map to hardware-native gates.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Key Quantum Gates</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Gate</th>
                        <th>Type</th>
                        <th>Matrix / Action</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>X, Y, Z</strong></td>
                        <td>Single-qubit</td>
                        <td>Pauli matrices</td>
                        <td>Bit/phase flips, rotations</td>
                    </tr>
                    <tr>
                        <td><strong>H</strong></td>
                        <td>Single-qubit</td>
                        <td>\(\frac{1}{\sqrt{2}}\begin{bmatrix}1&1\\1&-1\end{bmatrix}\)</td>
                        <td>Superposition, basis change</td>
                    </tr>
                    <tr>
                        <td><strong>S, T</strong></td>
                        <td>Single-qubit</td>
                        <td>Phase rotations</td>
                        <td>Add complex phases; T enables universality</td>
                    </tr>
                    <tr>
                        <td><strong>CNOT</strong></td>
                        <td>2-qubit</td>
                        <td>\(|a,b\rangle \to |a, a\oplus b\rangle\)</td>
                        <td>Entanglement, conditional logic</td>
                    </tr>
                    <tr>
                        <td><strong>Toffoli</strong></td>
                        <td>3-qubit</td>
                        <td>CCNOT</td>
                        <td>Classical logic, universality</td>
                    </tr>
                    <tr>
                        <td><strong>SWAP</strong></td>
                        <td>2-qubit</td>
                        <td>\(|a,b\rangle \to |b,a\rangle\)</td>
                        <td>Qubit exchange</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li>Single-qubit gates manipulate state on the Bloch sphere.</li>
            <li>Multi-qubit gates (especially CNOT) <strong>generate entanglement</strong>.</li>
            <li><strong>{H, T, CNOT}</strong> is a standard universal gate set.</li>
            <li>Circuit <strong>depth</strong> is critical for near-term hardware.</li>
            <li>Quantum circuits are the <strong>"assembly language"</strong> of quantum algorithms.</li>
        </ul>
    </div>
</section>

       <section class="section">
    <h2 class="section-title">Quantum Algorithms and Complexity</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing</em></p>
        
        <h3>Grover's Search Algorithm</h3>
        <h4>Problem Statement</h4>
        <p>Given an unstructured database of \(N = 2^n\) items and a <strong>black-box oracle</strong> \(f(x)\) such that:</p>
        <div class="equation">
            \[ f(x) = 
            \begin{cases}
            1 & \text{if } x = x_0 \text{ (target item)} \\
            0 & \text{otherwise}
            \end{cases} \]
        </div>
        <p>Goal: Find \(x_0\) with as few queries to \(f\) as possible.</p>
        
        <h4>Classical vs. Quantum</h4>
        <ul>
            <li><strong>Classical</strong>: Requires \(O(N)\) queries (on average \(N/2\)).</li>
            <li><strong>Grover's algorithm</strong>: Requires only \(O(\sqrt{N})\) queries → <strong>quadratic speedup</strong>.</li>
        </ul>
        
        <h4>Key Principles</h4>
        <ol>
            <li><strong>Amplitude Amplification</strong>:
            <ul>
                <li>Start with uniform superposition:<br>
                \[ |\psi\rangle = H^{\otimes n}|0\rangle^{\otimes n} = \frac{1}{\sqrt{N}} \sum_{x=0}^{N-1} |x\rangle \]
                </li>
                <li>Repeatedly apply the <strong>Grover iteration</strong> \(G = (2|\psi\rangle\langle\psi| - I) \cdot O_f\), where:
                <ul>
                    <li>\(O_f\) = oracle (flips sign of target: \(|x_0\rangle \to -|x_0\rangle\))</li>
                    <li>\(2|\psi\rangle\langle\psi| - I\) = <strong>diffusion operator</strong> (inverts amplitudes about mean)</li>
                </ul>
                </li>
            </ul>
            </li>
            <li><strong>Geometric Interpretation</strong>:
            <ul>
                <li>State vector rotates in 2D plane spanned by \(|x_0\rangle\) and \(|\psi_{\perp}\rangle\) (uniform superposition of non-targets).</li>
                <li>Each Grover iteration rotates by angle \(2\theta\), where \(\sin\theta = 1/\sqrt{N}\).</li>
                <li>Optimal number of iterations: \(R \approx \frac{\pi}{4} \sqrt{N}\)</li>
            </ul>
            </li>
        </ol>
        
        <h4>Applications</h4>
        <ul>
            <li><strong>Unstructured search</strong>: Database search, collision detection.</li>
            <li><strong>Optimization</strong>: Speeds up brute-force search in NP problems (e.g., SAT solvers).</li>
            <li><strong>Amplitude estimation</strong>: Core subroutine in quantum Monte Carlo methods.</li>
            <li><strong>Cryptanalysis</strong>: Reduces effective key length (e.g., AES-128 → ~64-bit security).</li>
        </ul>
        
        <div class="definition">
            <strong>Limitation</strong>: Only provides <strong>quadratic speedup</strong>—not exponential.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Fourier Transform (QFT)</h2>
    <div class="content">
        <h3>Definition</h3>
        <p>The <strong>QFT</strong> is the quantum analog of the classical Discrete Fourier Transform (DFT).</p>
        <p>Maps computational basis state \(|j\rangle\) to superposition:</p>
        <div class="equation">
            \[ \text{QFT}|j\rangle = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} e^{2\pi i j k / N} |k\rangle, \quad N = 2^n \]
        </div>
        
        <h3>Circuit Implementation</h3>
        <ul>
            <li>Efficiently implemented using <strong>Hadamard</strong> and <strong>controlled-phase</strong> gates.</li>
            <li><strong>Circuit depth</strong>: \(O(n^2)\) (can be reduced to \(O(n \log n)\) with approximations).</li>
            <li><strong>Structure</strong>:
            <ul>
                <li>Apply Hadamard to qubit 0.</li>
                <li>Apply controlled-\(R_k\) gates (phase rotations) from higher qubits.</li>
                <li>Repeat for each qubit.</li>
                <li>Reverse qubit order at the end (swap gates).</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Properties</h3>
        <ul>
            <li><strong>Unitary</strong>: QFT\(^{-1}\) = QFT\(^{\dagger}\)</li>
            <li><strong>Exponentially faster</strong> than classical FFT for preparing Fourier state (but <strong>output is not directly readable</strong> due to measurement collapse).</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Core subroutine</strong> in:
            <ul>
                <li><strong>Shor's algorithm</strong> (period finding)</li>
                <li><strong>Phase estimation</strong></li>
                <li><strong>Hidden subgroup problems</strong></li>
                <li><strong>Quantum simulations</strong> (e.g., solving linear systems via HHL algorithm)</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: QFT does <strong>not</strong> provide exponential speedup for classical FFT tasks because extracting all Fourier coefficients requires exponential measurements.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Shor's Algorithm for Integer Factorization</h2>
    <div class="content">
        <h3>Problem</h3>
        <p>Given composite integer \(N\), find non-trivial factors.</p>
        <ul>
            <li><strong>Classically</strong>: Best known algorithm (GNFS) runs in <strong>sub-exponential</strong> time → infeasible for large \(N\) (basis of RSA cryptography).</li>
            <li><strong>Shor's algorithm</strong>: Solves in <strong>polynomial time</strong> → breaks RSA.</li>
        </ul>
        
        <h3>Algorithm Overview</h3>
        <ol>
            <li><strong>Classical reduction</strong>:
            <ul>
                <li>Pick random \(a < N\), compute \(\gcd(a, N)\). If >1, done.</li>
                <li>Else, find <strong>order \(r\)</strong> of \(a \mod N\): smallest \(r\) such that \(a^r \equiv 1 \mod N\).</li>
            </ul>
            </li>
            <li><strong>Quantum subroutine: Period Finding</strong>
            <ul>
                <li>Use <strong>quantum phase estimation</strong> or <strong>QFT-based period finding</strong>:
                <ul>
                    <li>Prepare state: \(\frac{1}{\sqrt{Q}} \sum_{x=0}^{Q-1} |x\rangle |a^x \mod N\rangle\)</li>
                    <li>Measure second register → collapses first to periodic superposition with period \(r\).</li>
                    <li>Apply <strong>QFT</strong> to first register → peaks at multiples of \(Q/r\).</li>
                    <li>Classical post-processing (continued fractions) extracts \(r\).</li>
                </ul>
                </li>
            </ul>
            </li>
            <li><strong>Classical post-processing</strong>:
            <ul>
                <li>If \(r\) even and \(a^{r/2} \not\equiv -1 \mod N\), then<br>
                \(\gcd(a^{r/2} \pm 1, N)\) yields non-trivial factors.</li>
            </ul>
            </li>
        </ol>
        
        <h3>Complexity</h3>
        <ul>
            <li><strong>Time</strong>: \(O((\log N)^3)\) → <strong>exponential speedup</strong> over classical.</li>
            <li><strong>Qubits</strong>: \(O(\log N)\)</li>
        </ul>
        
        <h3>Significance</h3>
        <ul>
            <li>First <strong>exponentially faster</strong> quantum algorithm for a practical problem.</li>
            <li>Motivated global investment in <strong>post-quantum cryptography</strong>.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Walks</h2>
    <div class="content">
        <h3>What is a Quantum Walk?</h3>
        <p>Quantum analog of classical random walks.</p>
        <p>Two main types:</p>
        <ol>
            <li><strong>Discrete-time quantum walk (DTQW)</strong></li>
            <li><strong>Continuous-time quantum walk (CTQW)</strong></li>
        </ol>
        
        <h3>Discrete-Time Quantum Walk (DTQW)</h3>
        <ul>
            <li>Requires <strong>coin</strong> and <strong>position</strong> registers.</li>
            <li>Evolution:<br>
            \(|\psi(t+1)\rangle = S \cdot (C \otimes I) |\psi(t)\rangle\)
            <ul>
                <li>\(C\): <strong>Coin operator</strong> (e.g., Hadamard) → creates superposition of directions.</li>
                <li>\(S\): <strong>Shift operator</strong> → moves particle based on coin state.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Properties</h3>
        <ul>
            <li><strong>Quadratic speedup</strong> in hitting time vs. classical walks (e.g., on a line: spreads as \(t\) vs. \(\sqrt{t}\)).</li>
            <li><strong>Interference and entanglement</strong> between coin and position.</li>
            <li><strong>Universal for quantum computation</strong>: Any quantum circuit can be simulated by a quantum walk.</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Search algorithms</strong>: Spatial search on graphs (e.g., finding marked node on hypercube).</li>
            <li><strong>Graph isomorphism</strong>, <strong>element distinctness</strong>.</li>
            <li><strong>Quantum simulation</strong> of physical processes (e.g., energy transport).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Complexity Classes</h2>
    <div class="content">
        <h3>BQP (Bounded-error Quantum Polynomial Time)</h3>
        <ul>
            <li><strong>Definition</strong>: Class of decision problems solvable by a quantum computer in <strong>polynomial time</strong>, with error probability ≤ 1/3.</li>
            <li><strong>Formally</strong>:<br>
            \(L \in \text{BQP}\) if ∃ uniform family of quantum circuits \(\{C_n\}\) such that:
            <ul>
                <li>Size of \(C_n\) = poly(n)</li>
                <li>For \(x \in L\): \(\Pr[C_n(x) = 1] \geq 2/3\)</li>
                <li>For \(x \notin L\): \(\Pr[C_n(x) = 0] \geq 2/3\)</li>
            </ul>
            </li>
            <li><strong>Contains</strong>: P, BPP</li>
            <li><strong>Contained in</strong>: PSPACE</li>
            <li><strong>Believed not to contain</strong>: NP-complete problems (but not proven)</li>
            <li><strong>Key problems in BQP</strong>:
            <ul>
                <li>Factoring (Shor)</li>
                <li>Discrete log</li>
                <li>Simulation of quantum systems</li>
            </ul>
            </li>
        </ul>
        
        <h3>QMA (Quantum Merlin-Arthur)</h3>
        <ul>
            <li><strong>Quantum analog of NP</strong>.</li>
            <li><strong>Definition</strong>: A problem is in QMA if, for every "yes" instance, there exists a <strong>polynomial-size quantum witness</strong> \(|\psi\rangle\) that a <strong>polynomial-time quantum verifier</strong> accepts with high probability; for "no" instances, no such witness exists.</li>
            <li><strong>Formally</strong>:<br>
            \(L \in \text{QMA}\) if ∃ quantum verifier \(V\) (poly-time) such that:
            <ul>
                <li>If \(x \in L\): ∃ \(|\psi\rangle\) with \(\Pr[V(x, |\psi\rangle) = 1] \geq 2/3\)</li>
                <li>If \(x \notin L\): ∀ \(|\psi\rangle\), \(\Pr[V(x, |\psi\rangle) = 1] \leq 1/3\)</li>
            </ul>
            </li>
            <li><strong>Complete problem</strong>: <strong>k-local Hamiltonian problem</strong> (quantum analog of SAT).</li>
            <li><strong>Contains</strong>: BQP, NP</li>
            <li><strong>Contained in</strong>: PP</li>
        </ul>
        
        <div class="definition">
            <strong>Analogy</strong>:<br>
            - <strong>NP</strong>: Merlin sends classical proof → Arthur verifies classically.<br>
            - <strong>QMA</strong>: Merlin sends <strong>quantum proof</strong> → Arthur verifies <strong>quantumly</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm/Concept</th>
                        <th>Speedup</th>
                        <th>Key Idea</th>
                        <th>Application</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Grover's Search</strong></td>
                        <td>Quadratic (\(O(\sqrt{N})\))</td>
                        <td>Amplitude amplification</td>
                        <td>Unstructured search, optimization</td>
                    </tr>
                    <tr>
                        <td><strong>QFT</strong></td>
                        <td>Exponential state prep</td>
                        <td>Quantum Fourier basis</td>
                        <td>Shor's, phase estimation</td>
                    </tr>
                    <tr>
                        <td><strong>Shor's Algorithm</strong></td>
                        <td>Exponential</td>
                        <td>Period finding via QFT</td>
                        <td>Integer factorization, RSA break</td>
                    </tr>
                    <tr>
                        <td><strong>Quantum Walks</strong></td>
                        <td>Quadratic (hitting time)</td>
                        <td>Coherent walk with interference</td>
                        <td>Graph search, simulation</td>
                    </tr>
                    <tr>
                        <td><strong>BQP</strong></td>
                        <td>—</td>
                        <td>Efficient quantum decision problems</td>
                        <td>Captures power of quantum computers</td>
                    </tr>
                    <tr>
                        <td><strong>QMA</strong></td>
                        <td>—</td>
                        <td>Quantum proofs + verification</td>
                        <td>Quantum satisfiability, physics</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Grover</strong>: Quadratic speedup for search—broadly applicable but not exponential.</li>
            <li><strong>Shor + QFT</strong>: Exponential speedup for structured problems (factoring, period finding).</li>
            <li><strong>Quantum walks</strong>: Offer alternative algorithmic framework with speedups on graphs.</li>
            <li><strong>BQP ≠ NP</strong>: Quantum computers likely <strong>cannot solve NP-complete problems efficiently</strong>.</li>
            <li><strong>QMA</strong>: Natural class for quantum many-body problems and verification.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum-Inspired Algorithms</h2>
    <div class="content">
        <p><em>Classical Simulation of Quantum Phenomena</em></p>
        
        <h3>Simulating Quantum Phenomena: Overview</h3>
        <h4>What Are Quantum-Inspired Algorithms?</h4>
        <p><strong>Definition</strong>: Classical algorithms that borrow ideas from quantum mechanics (e.g., superposition, interference, entanglement) to solve problems more efficiently—<strong>without requiring a quantum computer</strong>.</p>
        <p><strong>Goal</strong>: Leverage quantum-like structures to gain speedups or new insights on classical hardware.</p>
        <p><strong>Important distinction</strong>:</p>
        <ul>
            <li><strong>Quantum simulation</strong>: Simulating a quantum system <em>on a classical computer</em>.</li>
            <li><strong>Quantum-inspired</strong>: Using quantum concepts to design <em>better classical algorithms</em>.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Note</strong>: True quantum advantage (e.g., exponential speedup) generally <strong>cannot</strong> be replicated classically for arbitrary quantum systems—due to the exponential cost of representing quantum states.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Classical Simulation of Superposition</h2>
    <div class="content">
        <h3>How Superposition Is Represented Classically</h3>
        <p>A single qubit in superposition:</p>
        <div class="equation">
            \[ |\psi\rangle = \alpha|0\rangle + \beta|1\rangle, \quad |\alpha|^2 + |\beta|^2 = 1 \]
        </div>
        <p><strong>Classical representation</strong>: Store complex amplitudes \(\alpha, \beta\) as floating-point numbers → <strong>2 complex numbers</strong> per qubit.</p>
        
        <h3>Scaling Challenge</h3>
        <ul>
            <li>For \(n\) qubits, the state vector has \(2^n\) complex amplitudes.</li>
            <li><strong>Memory requirement</strong>: \(O(2^n)\) → becomes infeasible beyond ~45–50 qubits on supercomputers.</li>
        </ul>
        
        <h3>Simulation Techniques</h3>
        <ol>
            <li><strong>State-vector simulation</strong>:
            <ul>
                <li>Exact simulation using full \(2^n\)-dimensional vector.</li>
                <li>Used in simulators like Qiskit Aer, QuTiP.</li>
            </ul>
            </li>
            <li><strong>Sparse state representation</strong>:
            <ul>
                <li>If the state has few non-zero amplitudes (e.g., after few gates), store only non-zero entries.</li>
            </ul>
            </li>
            <li><strong>Tensor network methods</strong> (see Section 5): Avoid full state vector when entanglement is limited.</li>
        </ol>
        
        <div class="definition">
            ✅ <strong>Feasible for</strong>: Small circuits, educational purposes, verification of quantum hardware.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Simulating Entanglement on Classical Hardware</h2>
    <div class="content">
        <h3>The Core Challenge</h3>
        <ul>
            <li>Entanglement implies <strong>non-separability</strong>: The full \(2^n\)-dimensional state cannot be factored.</li>
            <li>Classical simulation must track <strong>global correlations</strong>, which grow exponentially with qubit count.</li>
        </ul>
        
        <h3>Approaches to Simulate Entanglement</h3>
        <h4>(a) Full State-Vector Simulation</h4>
        <ul>
            <li>Stores entire wavefunction → captures all entanglement exactly.</li>
            <li><strong>Cost</strong>: \(O(2^n)\) memory and \(O(2^n)\) time per gate (for dense gates like CNOT).</li>
        </ul>
        
        <h4>(b) Stabilizer Formalism (Gottesman-Knill Theorem)</h4>
        <ul>
            <li>Efficiently simulates circuits using only <strong>Clifford gates</strong> (H, S, CNOT).</li>
            <li>Represents state via <strong>stabilizer generators</strong> (not amplitudes).</li>
            <li><strong>Complexity</strong>: \(O(n^2)\) per gate for \(n\) qubits.</li>
            <li><strong>Limitation</strong>: Cannot simulate non-Clifford gates (e.g., T gate) efficiently → not universal.</li>
        </ul>
        
        <h4>(c) Matrix Product States (MPS) / Tensor Networks</h4>
        <p>Represents state as a chain of tensors:</p>
        <div class="equation">
            \[ |\psi\rangle = \sum_{i_1,\dots,i_n} A^{[1]}_{i_1} A^{[2]}_{i_2} \cdots A^{[n]}_{i_n} |i_1 i_2 \dots i_n\rangle \]
        </div>
        <ul>
            <li><strong>Efficient when entanglement is low</strong> (e.g., 1D systems with area law).</li>
            <li><strong>Bond dimension \(\chi\)</strong> controls accuracy and cost: memory = \(O(n \chi^2)\).</li>
            <li>Used in DMRG (Density Matrix Renormalization Group) for condensed matter physics.</li>
        </ul>
        
        <div class="definition">
            🔑 <strong>Key Insight</strong>: Entanglement entropy limits classical simulability. High entanglement → classical simulation becomes intractable.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Interference Simulation Techniques</h2>
    <div class="content">
        <h3>What Is Quantum Interference?</h3>
        <p>Amplitudes (complex numbers) can <strong>constructively or destructively interfere</strong>.</p>
        <p>Critical for quantum speedups (e.g., in Grover's or Shor's algorithms).</p>
        
        <h3>Classical Simulation Methods</h3>
        <ol>
            <li><strong>Path Integral / Sum-over-Paths</strong>:
            <ul>
                <li>Compute final amplitude as sum over all computational paths.</li>
                <li>Each path contributes a complex phase.</li>
                <li><strong>Problem</strong>: \(2^{\text{#gates}}\) paths → exponential in circuit depth.</li>
            </ul>
            </li>
            <li><strong>Feynman's Path Simulator</strong>:
            <ul>
                <li>For a circuit with \(m\) gates, simulate by summing over all possible intermediate measurement outcomes.</li>
                <li>Time complexity: \(O(4^m)\) → only feasible for shallow circuits.</li>
            </ul>
            </li>
            <li><strong>Monte Carlo with Sign Problem</strong>:
            <ul>
                <li>Sample paths probabilistically.</li>
                <li><strong>Obstacle</strong>: Negative/complex weights → <strong>sign problem</strong> → high variance, inefficient.</li>
            </ul>
            </li>
            <li><strong>Stabilizer Rank Decomposition</strong> (for non-Clifford circuits):
            <ul>
                <li>Approximate a state as sum of \(R\) stabilizer states:<br>
                \[ |\psi\rangle \approx \sum_{k=1}^R c_k |\phi_k\rangle \]
                </li>
                <li>Simulate each \(|\phi_k\rangle\) efficiently (via Gottesman-Knill), then combine.</li>
                <li>Cost scales with <strong>stabilizer rank \(R\)</strong> (e.g., \(R = 2^t\) for \(t\) T-gates).</li>
            </ul>
            </li>
        </ol>
        
        <div class="definition">
            🌊 <strong>Interference is hard to simulate classically</strong> because it requires tracking <strong>global phase relationships</strong> across exponentially many states.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Approximation Methods for Quantum States</h2>
    <div class="content">
        <p>Exact simulation is often impossible → use <strong>approximations</strong>.</p>
        
        <h3>(a) Tensor Networks</h3>
        <ul>
            <li><strong>MPS (1D)</strong>, <strong>PEPS (2D)</strong>, <strong>MERA</strong>: Represent states with limited entanglement.</li>
            <li><strong>Accuracy controlled by bond dimension</strong>.</li>
            <li>Widely used in quantum chemistry and condensed matter.</li>
        </ul>
        
        <h3>(b) Variational Methods</h3>
        <ul>
            <li><strong>Quantum-inspired classical ansatz</strong>: Use parameterized classical models (e.g., neural networks) to approximate quantum states.
            <ul>
                <li>Example: <strong>Restricted Boltzmann Machines (RBMs)</strong> to represent wavefunctions.</li>
            </ul>
            </li>
            <li>Train via energy minimization (e.g., for ground states).</li>
        </ul>
        
        <h3>(c) Low-Rank Approximations</h3>
        <ul>
            <li>Assume density matrix \(\rho\) has low rank → store via SVD.</li>
            <li>Useful for <strong>mixed states</strong> with limited entanglement.</li>
        </ul>
        
        <h3>(d) Clifford+T Approximation</h3>
        <ul>
            <li>Decompose arbitrary gates into Clifford + T.</li>
            <li>Simulate using <strong>stabilizer decomposition</strong> (as above).</li>
            <li>Error controlled by number of T gates.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Computational Trade-offs in Simulation</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Memory</th>
                        <th>Time</th>
                        <th>Accuracy</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>State-vector</strong></td>
                        <td>\(O(2^n)\)</td>
                        <td>\(O(2^n)\)/gate</td>
                        <td>Exact</td>
                        <td>Small \(n\) (< 45 qubits)</td>
                    </tr>
                    <tr>
                        <td><strong>Stabilizer (Clifford)</strong></td>
                        <td>\(O(n^2)\)</td>
                        <td>\(O(n^2)\)/gate</td>
                        <td>Exact (Clifford only)</td>
                        <td>Clifford circuits</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Networks (MPS)</strong></td>
                        <td>\(O(n \chi^2)\)</td>
                        <td>\(O(n \chi^3)\)</td>
                        <td>Approximate</td>
                        <td>Low-entanglement 1D systems</td>
                    </tr>
                    <tr>
                        <td><strong>Path Integral</strong></td>
                        <td>\(O(1)\)</td>
                        <td>\(O(4^m)\)</td>
                        <td>Exact</td>
                        <td>Shallow circuits (small \(m\))</td>
                    </tr>
                    <tr>
                        <td><strong>Stabilizer Rank</strong></td>
                        <td>\(O(R n^2)\)</td>
                        <td>\(O(R n^2)\)</td>
                        <td>Approximate</td>
                        <td>Circuits with few T gates</td>
                    </tr>
                    <tr>
                        <td><strong>Neural Quantum States</strong></td>
                        <td>\(O(\text{params})\)</td>
                        <td>Training cost</td>
                        <td>Approximate</td>
                        <td>Ground states, optimization</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Trade-offs</h3>
        <ul>
            <li><strong>Accuracy vs. Scalability</strong>: Exact methods don't scale; approximations introduce error.</li>
            <li><strong>Entanglement vs. Efficiency</strong>: High entanglement → tensor networks fail; state-vector needed.</li>
            <li><strong>Circuit Depth vs. Path Methods</strong>: Deep circuits → path integral infeasible.</li>
            <li><strong>Hardware Limits</strong>: Even with compression, simulating 50+ qubits with high entanglement requires exascale computing.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Rule of Thumb</strong>:<br>
            If a quantum circuit can be simulated efficiently classically, it likely <strong>does not provide quantum advantage</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <ul>
            <li><strong>Superposition</strong> and <strong>interference</strong> require tracking complex amplitudes → exponential classical cost.</li>
            <li><strong>Entanglement</strong> is the main barrier to efficient classical simulation.</li>
            <li><strong>Special cases</strong> (Clifford circuits, low entanglement) can be simulated efficiently.</li>
            <li><strong>Approximation methods</strong> (tensor networks, stabilizer decompositions) enable simulation of larger systems at the cost of accuracy.</li>
            <li><strong>Quantum-inspired algorithms</strong> may offer practical speedups for specific problems (e.g., linear algebra, optimization), but <strong>do not replicate exponential quantum advantage</strong>.</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Reality Check</strong>: Classical simulation is crucial for <strong>verifying quantum hardware</strong>, <strong>developing algorithms</strong>, and <strong>exploring quantum-classical boundaries</strong>—but it cannot replace scalable, fault-tolerant quantum computers for truly hard problems.
        </div>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Tensor Networks</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Simulation and High-Dimensional Data Representation</em></p>
        
        <h3>Tensor Algebra Fundamentals</h3>
        <h4>What is a Tensor?</h4>
        <p>A <strong>tensor</strong> is a multi-dimensional array of numbers that generalizes scalars (0D), vectors (1D), and matrices (2D).</p>
        <ul>
            <li><strong>Order (or rank)</strong>: Number of indices (e.g., a 3D tensor \(T_{ijk}\) has order 3).</li>
            <li><strong>Dimensions (modes)</strong>: Size along each index (e.g., \(T \in \mathbb{C}^{d_1 \times d_2 \times \cdots \times d_n}\)).</li>
        </ul>
        
        <h4>Key Operations</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Operation</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Tensor product</strong> (\(\otimes\))</td>
                        <td>Combines tensors: \((A \otimes B)_{i,j,k,l} = A_{i,j} B_{k,l}\)</td>
                    </tr>
                    <tr>
                        <td><strong>Contraction</strong></td>
                        <td>Sum over shared indices (generalizes matrix multiplication).<br>Example: \(C_{ik} = \sum_j A_{ij} B_{jk}\)</td>
                    </tr>
                    <tr>
                        <td><strong>Reshaping</strong></td>
                        <td>Reinterpret tensor as different shape (e.g., vector ↔ matrix) without changing data.</td>
                    </tr>
                    <tr>
                        <td><strong>Trace</strong></td>
                        <td>Contraction of a tensor with itself over one or more pairs of indices.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h4>Einstein Notation (Implicit Summation)</h4>
        <p>Repeated indices are summed over:<br>
        \(C_{ik} = A_{ij} B_{jk}\) means \(\sum_j A_{ij} B_{jk}\).</p>
        <p><strong>Crucial for tensor network diagrams</strong>.</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Tensor Decomposition Methods</h2>
    <div class="content">
        <p>Decomposing high-order tensors into simpler components reduces storage and enables efficient computation.</p>
        
        <h3>(a) CANDECOMP/PARAFAC (CP) Decomposition</h3>
        <p>Expresses a tensor as a <strong>sum of rank-1 tensors</strong>:</p>
        <div class="equation">
            \[ \mathcal{T} \approx \sum_{r=1}^R \lambda_r \, a^{(1)}_r \circ a^{(2)}_r \circ \cdots \circ a^{(N)}_r \]
        </div>
        <p>where \(\circ\) denotes outer product.</p>
        <ul>
            <li><strong>Parameters</strong>: \(R \cdot \sum_{n=1}^N d_n\) (vs. \(\prod d_n\) for full tensor).</li>
            <li><strong>Pros</strong>: Compact, interpretable.</li>
            <li><strong>Cons</strong>: Rank \(R\) hard to choose; decomposition not always unique; ill-conditioned.</li>
        </ul>
        
        <div class="definition">
            📌 Used in chemometrics, signal processing.
        </div>
        
        <h3>(b) Tucker Decomposition</h3>
        <p>Generalization of SVD to higher orders:</p>
        <div class="equation">
            \[ \mathcal{T} \approx \mathcal{G} \times_1 A^{(1)} \times_2 A^{(2)} \cdots \times_N A^{(N)} \]
        </div>
        <ul>
            <li>\(\mathcal{G}\): <strong>Core tensor</strong> (size \(r_1 \times r_2 \times \cdots \times r_N\))</li>
            <li>\(A^{(n)}\): Factor matrices (\(d_n \times r_n\))</li>
        </ul>
        <ul>
            <li><strong>Storage</strong>: \(\prod r_n + \sum d_n r_n\)</li>
            <li><strong>Pros</strong>: Flexible; captures multi-linear structure.</li>
            <li><strong>Cons</strong>: Core tensor still exponential in \(N\) if all \(r_n\) large → <strong>curse of dimensionality</strong>.</li>
        </ul>
        
        <div class="definition">
            📌 Basis for <strong>Higher-Order SVD (HOSVD)</strong>.
        </div>
        
        <h3>(c) Tensor Train (TT) / Matrix Product State (MPS)</h3>
        <p><strong>Decomposes tensor into a chain of 3D cores</strong>:</p>
        <div class="equation">
            \[ \mathcal{T}_{i_1 i_2 \dots i_N} = \sum_{\alpha_1,\dots,\alpha_{N-1}} 
            G^{(1)}_{i_1,\alpha_1} 
            G^{(2)}_{\alpha_1,i_2,\alpha_2} 
            \cdots 
            G^{(N)}_{\alpha_{N-1},i_N} \]
        </div>
        <ul>
            <li><strong>Bond dimensions</strong> \(\{\alpha_k\}\) control approximation rank.</li>
            <li><strong>Storage</strong>: \(O(N d r^2)\) for uniform physical dimension \(d\) and bond dimension \(r\).</li>
            <li><strong>Avoids exponential scaling</strong> if \(r\) is small (e.g., for weakly entangled states).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Key insight</strong>: TT/MPS is <strong>exact</strong> for any tensor (with \(r\) up to \(d^{N/2}\)), but <strong>efficient</strong> only when \(r \ll d^{N/2}\).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Matrix Product States (MPS) Representation</h2>
    <div class="content">
        <h3>Origin</h3>
        <p>MPS is the <strong>tensor network form of a quantum many-body wavefunction</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle = \sum_{s_1,\dots,s_N} c_{s_1 \dots s_N} |s_1 s_2 \dots s_N\rangle \]
        </div>
        <p>where \(c_{s_1 \dots s_N}\) is represented as a <strong>Tensor Train</strong>.</p>
        
        <h3>MPS Structure</h3>
        <p>Each site \(k\) has a <strong>core tensor</strong> \(A^{[k]}_{s_k}\) of shape \((\chi_{k-1}, d, \chi_k)\), where:</p>
        <ul>
            <li>\(d\): local Hilbert space dimension (e.g., \(d=2\) for qubits)</li>
            <li>\(\chi_k\): <strong>bond dimension</strong> (entanglement capacity)</li>
        </ul>
        <p><strong>Boundary conditions</strong>: \(\chi_0 = \chi_N = 1\) (open boundary)</p>
        
        <h3>Gauge Freedom</h3>
        <ul>
            <li>MPS is <strong>not unique</strong>: Can insert \(X X^{-1} = I\) between cores.</li>
            <li>Common gauges:
            <ul>
                <li><strong>Left-canonical</strong>: \(\sum_{s_k} (A^{[k]}_{s_k})^\dagger A^{[k]}_{s_k} = I\)</li>
                <li><strong>Right-canonical</strong>: \(\sum_{s_k} A^{[k]}_{s_k} (A^{[k]}_{s_k})^\dagger = I\)</li>
            </ul>
            </li>
            <li>Enables efficient local operations (e.g., in DMRG).</li>
        </ul>
        
        <h3>Entanglement and Bond Dimension</h3>
        <ul>
            <li>For a bipartition at bond \(k\), <strong>entanglement entropy</strong> \(S \leq \log_2 \chi_k\).</li>
            <li><strong>Area law</strong>: Ground states of 1D gapped Hamiltonians have <strong>constant</strong> \(\chi\) → MPS is efficient.</li>
        </ul>
        
        <div class="definition">
            🌟 MPS is the foundation of <strong>Density Matrix Renormalization Group (DMRG)</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Tensor Network Diagrams and Notation</h2>
    <div class="content">
        <h3>Diagrammatic Rules</h3>
        <ul>
            <li><strong>Node</strong>: Tensor (shape indicates order).</li>
            <li><strong>Edge</strong>: Index (labeled or unlabeled).</li>
            <li><strong>Connected edges</strong>: Contracted (summed over).</li>
            <li><strong>Open edges</strong>: Free indices (output of network).</li>
        </ul>
        
        <h3>Examples</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Object</th>
                        <th>Diagram</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Vector \(v_i\)</td>
                        <td>○——</td>
                    </tr>
                    <tr>
                        <td>Matrix \(M_{ij}\)</td>
                        <td>○——○</td>
                    </tr>
                    <tr>
                        <td>Order-3 tensor \(T_{ijk}\)</td>
                        <td>○ with 3 legs</td>
                    </tr>
                    <tr>
                        <td>Matrix multiplication \(C = AB\)</td>
                        <td>○——○——○ ⇒ ○————○</td>
                    </tr>
                    <tr>
                        <td>MPS (4 sites)</td>
                        <td>○—○—○—○ (each node has one physical leg down, bonds horizontal)</td>
                    </tr>
                    <tr>
                        <td>Contraction of two tensors</td>
                        <td>Two nodes connected by an edge</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Advantages of Diagrams</h3>
        <ul>
            <li>Visualize complex contractions.</li>
            <li>Reveal computational structure (e.g., sequential vs. parallel).</li>
            <li>Simplify derivation of algorithms (e.g., DMRG, TEBD).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Contraction Algorithms and Complexity</h2>
    <div class="content">
        <h3>What is Contraction?</h3>
        <p>Evaluate a tensor network by summing over all internal (bond) indices.</p>
        <p>Goal: Compute scalar (e.g., \(\langle \psi | \phi \rangle\)) or reduced tensor (e.g., reduced density matrix).</p>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Order matters</strong>: Different contraction sequences have vastly different costs.</li>
            <li><strong>Optimal contraction is NP-hard</strong> (equivalent to graph partitioning).</li>
        </ul>
        
        <h3>Contraction Strategies</h3>
        <h4>(a) Sequential (Naive) Contraction</h4>
        <ul>
            <li>Contract one pair at a time.</li>
            <li><strong>Cost</strong>: Can be exponential if done poorly.</li>
        </ul>
        
        <h4>(b) Optimal Tree Contraction (for Trees)</h4>
        <ul>
            <li>For <strong>tree-structured networks</strong> (no cycles), optimal order is linear in size.</li>
            <li>Use dynamic programming to find min-cost order.</li>
        </ul>
        
        <h4>(c) Heuristic Methods (for General Networks)</h4>
        <ul>
            <li><strong>Greedy</strong>: At each step, contract pair minimizing cost (e.g., `opt_einsum` in Python).</li>
            <li><strong>Graph-based</strong>: Model as <strong>line graph</strong>; use graph partitioning (e.g., METIS).</li>
            <li><strong>Tree decomposition</strong>: For networks with small <strong>treewidth</strong>.</li>
        </ul>
        
        <h3>Complexity Example: MPS Overlap</h3>
        <p>Compute \(\langle \psi | \phi \rangle\) for two MPS with bond dimensions \(\chi_\psi, \chi_\phi\).</p>
        <p>Contract from left to right:</p>
        <ul>
            <li>Each step: matrix multiplication of size \(\chi \times \chi\)</li>
            <li><strong>Total cost</strong>: \(O(N d \chi^3)\)</li>
        </ul>
        
        <h3>Memory vs. Time Trade-off</h3>
        <p><strong>Intermediate tensors</strong> can be large.</p>
        <p><strong>Slicing (index splitting)</strong>: Fix some indices to reduce memory at cost of repeated contractions.</p>
        
        <div class="definition">
            💡 <strong>Rule of thumb</strong>:<br>
            Contraction cost ≈ \(O(\chi^k)\), where \(k\) = number of bonds meeting at a vertex in the contraction tree.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Idea</th>
                        <th>Complexity/Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CP Decomposition</strong></td>
                        <td>Sum of rank-1 tensors</td>
                        <td>Compact but unstable</td>
                    </tr>
                    <tr>
                        <td><strong>Tucker</strong></td>
                        <td>Core + factor matrices</td>
                        <td>Flexible; core still large</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Train (MPS)</strong></td>
                        <td>Chain of cores; bond dimensions control entanglement</td>
                        <td>\(O(N d r^2)\); ideal for 1D quantum states</td>
                    </tr>
                    <tr>
                        <td><strong>MPS Gauge</strong></td>
                        <td>Left/right canonical forms enable local updates</td>
                        <td>Essential for DMRG</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Diagrams</strong></td>
                        <td>Visual language for indices and contractions</td>
                        <td>Intuitive algorithm design</td>
                    </tr>
                    <tr>
                        <td><strong>Contraction</strong></td>
                        <td>Order drastically affects cost; optimal is NP-hard</td>
                        <td>Heuristics used in practice (e.g., `opt_einsum`)</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li>Tensor networks <strong>tame the curse of dimensionality</strong> by exploiting <strong>structure</strong> (e.g., limited entanglement).</li>
            <li><strong>MPS</strong> is the workhorse for 1D quantum systems and underlies <strong>DMRG</strong>.</li>
            <li><strong>Contraction order</strong> is critical—small networks can be evaluated exactly; large ones require smart heuristics.</li>
            <li>Diagrams are not just illustrative—they are <strong>computational tools</strong>.</li>
            <li>Applications span <strong>quantum physics</strong>, <strong>machine learning</strong> (e.g., tensor completion), and <strong>data compression</strong>.</li>
        </ul>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Quantum-Inspired Search</h2>
    <div class="content">
        <p><em>Classical Approximations and Applications of Grover-like Techniques</em></p>
        
        <h3>Classical Approximations of Grover's Algorithm</h3>
        <h4>Can Grover's Algorithm Be Simulated Classically?</h4>
        <p><strong>Yes—but not efficiently</strong> for large unstructured search spaces.</p>
        <p>The <strong>quadratic speedup</strong> (\(O(\sqrt{N})\) vs. \(O(N)\)) arises from <strong>quantum parallelism + interference</strong>, which classical systems cannot replicate <em>exactly</em> without exponential overhead.</p>
        
        <h4>Why Exact Simulation Fails at Scale</h4>
        <ul>
            <li>To simulate Grover's state evolution, a classical computer must store and update a <strong>superposition of \(N = 2^n\) amplitudes</strong>.</li>
            <li><strong>Memory and time cost</strong>: \(O(N)\)—same as brute-force search.</li>
            <li>Thus, <strong>no asymptotic speedup</strong> from exact simulation.</li>
        </ul>
        
        <h4>Approximate Classical Emulations</h4>
        <p>Instead of simulating the full quantum state, classical algorithms borrow <strong>structural ideas</strong> from Grover:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Idea</th>
                        <th>Limitation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random sampling with boosting</strong></td>
                        <td>Repeatedly sample and "boost" likelihood of promising candidates</td>
                        <td>No guaranteed speedup; heuristic</td>
                    </tr>
                    <tr>
                        <td><strong>Amplitude-inspired weighting</strong></td>
                        <td>Assign weights to items; iteratively increase weight of marked items</td>
                        <td>Requires oracle feedback per iteration</td>
                    </tr>
                    <tr>
                        <td><strong>Monte Carlo with bias</strong></td>
                        <td>Bias random walks toward marked elements using oracle hints</td>
                        <td>Still \(O(N)\) in worst case</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Key insight</strong>: Classical systems can <strong>mimic the iterative refinement</strong> of Grover, but <strong>without interference</strong>, they cannot achieve true \(O(\sqrt{N})\) scaling.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Amplitude Amplification Techniques (Classical Analogues)</h2>
    <div class="content">
        <h3>What is Amplitude Amplification? (Quantum)</h3>
        <p>Generalization of Grover's algorithm.</p>
        <p>Given a subspace of "good" states, it <strong>rotates</strong> the state vector toward that subspace using:</p>
        <ul>
            <li>An <strong>oracle</strong> \(O\) that marks good states.</li>
            <li>A <strong>diffusion operator</strong> \(D = 2|\psi\rangle\langle\psi| - I\) that inverts amplitudes about the mean.</li>
        </ul>
        
        <h3>Classical Analogues</h3>
        <p>While true amplitude amplification is quantum, classical algorithms use <strong>similar iterative boosting</strong>:</p>
        
        <h4>(a) Multiplicative Weight Updates (MWU)</h4>
        <ul>
            <li>Maintain a probability distribution over items.</li>
            <li>After querying the oracle, <strong>increase weight</strong> of marked items, <strong>decrease</strong> others.</li>
            <li>Normalize to form new distribution.</li>
            <li>After \(O(\sqrt{N})\) rounds, marked item dominates <strong>in expectation</strong>—but <strong>not guaranteed</strong>.</li>
        </ul>
        
        <h4>(b) Boosting in Machine Learning</h4>
        <ul>
            <li>Algorithms like <strong>AdaBoost</strong> iteratively adjust weights of misclassified samples.</li>
            <li><strong>Analogy</strong>: Oracle = weak learner; amplification = focusing on hard examples.</li>
            <li>Not a search algorithm per se, but shares the <strong>iterative refinement</strong> philosophy.</li>
        </ul>
        
        <h4>(c) Stochastic Search with Feedback</h4>
        <ul>
            <li>Use oracle to guide a <strong>biased random search</strong>:
            <ul>
                <li>Start with uniform distribution.</li>
                <li>After each query, update probabilities using Bayesian inference or reinforcement learning.</li>
            </ul>
            </li>
            <li>Can reduce expected queries—but worst-case remains \(O(N)\).</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Critical difference</strong>: Quantum amplitude amplification uses <strong>coherent interference</strong> to <em>cancel</em> bad paths; classical methods only <em>reweight</em>—no destructive interference.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Oracle Design for Classical Search</h2>
    <div class="content">
        <h3>What is an Oracle?</h3>
        <p>A <strong>black-box function</strong> \(f: \{0,1\}^n \to \{0,1\}\) such that \(f(x) = 1\) iff \(x\) is a solution.</p>
        <p>In quantum algorithms, the oracle is applied <strong>coherently</strong> to superpositions.</p>
        
        <h3>Classical Oracle Usage</h3>
        <p>In classical search, the oracle is simply a <strong>predicate function</strong> called on individual inputs.</p>
        <p><strong>Design considerations</strong>:</p>
        <ul>
            <li><strong>Cost per query</strong>: Should be \(O(1)\) or low for speedup to matter.</li>
            <li><strong>Side information</strong>: Can the oracle return more than yes/no? (e.g., gradient, distance)</li>
            <li><strong>Parallelizability</strong>: Can multiple queries be made simultaneously?</li>
        </ul>
        
        <h3>Quantum-Inspired Oracle Strategies</h3>
        <ol>
            <li><strong>Batch querying</strong>: Evaluate oracle on a <strong>subset</strong> of candidates in parallel (e.g., GPU).</li>
            <li><strong>Probabilistic oracles</strong>: Return approximate answers (e.g., in noisy settings).</li>
            <li><strong>Structured oracles</strong>: Exploit partial structure (e.g., locality, smoothness) to guide search—blurring line between structured and unstructured search.</li>
        </ol>
        
        <div class="definition">
            🔍 <strong>Note</strong>: True unstructured search assumes <strong>no side information</strong>—only yes/no answers. Most real-world problems have <em>some</em> structure, enabling better-than-Grover classical heuristics.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quadratic Speedup Approximations</h2>
    <div class="content">
        <h3>Can Classical Algorithms Achieve \(O(\sqrt{N})\) Queries?</h3>
        <p><strong>No—not in the worst case for unstructured search</strong> (proven by Bennett et al., 1997).</p>
        <p><strong>Lower bound</strong>: Any classical randomized algorithm requires \(\Omega(N)\) queries to find a unique marked item with constant success probability.</p>
        
        <h3>When Can We *Approximate* Quadratic Speedup?</h3>
        <p>Only under <strong>relaxed assumptions</strong>:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Approximate Speedup</th>
                        <th>Mechanism</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Multiple solutions</strong> (\(M\) marked items)</td>
                        <td>\(O(N/M)\) classically vs. \(O(\sqrt{N/M})\) quantum</td>
                        <td>Classical: random sampling finds solution in \(O(N/M)\)</td>
                    </tr>
                    <tr>
                        <td><strong>Approximate search</strong></td>
                        <td>Find <em>near</em>-solution faster</td>
                        <td>Use locality, embeddings, or hashing (e.g., LSH)</td>
                    </tr>
                    <tr>
                        <td><strong>Parallel classical queries</strong></td>
                        <td>\(O(\sqrt{N})\) time with \(O(\sqrt{N})\) processors</td>
                        <td>Not query-efficient; trades time for hardware</td>
                    </tr>
                    <tr>
                        <td><strong>Probabilistic success</strong></td>
                        <td>High probability in fewer than \(N\) steps</td>
                        <td>Still \(\Omega(N)\) expected queries</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Quantum-Inspired Heuristics with Empirical Speedup</h3>
        <ul>
            <li><strong>Grover-inspired local search</strong>: Use amplitude-like weighting in combinatorial optimization.</li>
            <li><strong>Quantum walk-inspired search</strong>: Classical random walks with memory or restarts mimic hitting-time improvements.</li>
            <li><strong>Amplitude estimation via Monte Carlo</strong>: Estimate \(M/N\) (fraction of solutions) using classical sampling—used in finance, but with \(O(1/\epsilon^2)\) vs. quantum \(O(1/\epsilon)\).</li>
        </ul>
        
        <div class="definition">
            📉 <strong>Bottom line</strong>: Classical algorithms <strong>cannot match</strong> Grover's asymptotic query complexity for unstructured search—but can <strong>approach it</strong> in practice for structured or noisy variants.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications to Unstructured Search Problems</h2>
    <div class="content">
        <p>While true unstructured search is rare, many real-world problems <strong>approximate</strong> it:</p>
        
        <h3>(a) Cryptanalysis</h3>
        <ul>
            <li><strong>Brute-force key search</strong>: Find key \(k\) such that \(\text{Decrypt}_k(c) = m\).
            <ul>
                <li>Classical: \(O(2^n)\)</li>
                <li>Grover: \(O(2^{n/2})\) → reduces AES-128 security to ~64 bits.</li>
                <li><strong>Classical approximation</strong>: Use rainbow tables, side-channel info, or partial key guesses—<strong>not unstructured</strong>.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(b) Database Search</h3>
        <ul>
            <li><strong>Idealized</strong>: Find record with property \(P\).
            <ul>
                <li>Quantum: \(O(\sqrt{N})\) with quantum RAM (QRAM)—<strong>not yet feasible</strong>.</li>
                <li>Classical: Indexing (e.g., B-trees) makes it <strong>structured</strong> → \(O(\log N)\).</li>
                <li><strong>Unindexed databases</strong>: Linear scan = \(O(N)\); no classical quadratic speedup.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(c) NP Problem Solving</h3>
        <ul>
            <li><strong>SAT, Graph Coloring, etc.</strong>: Search space is unstructured in worst case.
            <ul>
                <li>Quantum: Grover gives \(O(\sqrt{2^n}) = O(1.414^n)\) vs. classical \(O(2^n)\).</li>
                <li>Classical heuristics (DPLL, CDCL): Exploit <strong>problem structure</strong> → often much faster than \(O(2^n)\), but <strong>exponential in worst case</strong>.</li>
                <li><strong>Quantum-inspired SAT solvers</strong>: Use amplitude-like scoring to prioritize variable assignments—modest empirical gains.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(d) Machine Learning & Optimization</h3>
        <ul>
            <li><strong>Finding optimal parameters</strong> in high-dimensional space.
            <ul>
                <li>Quantum: Amplitude amplification for minimization.</li>
                <li>Classical: <strong>Simulated annealing</strong>, <strong>genetic algorithms</strong>, <strong>Bayesian optimization</strong>—use feedback to guide search.</li>
                <li><strong>Quantum-inspired</strong>: Use Grover-like iteration in <strong>combinatorial optimization</strong> (e.g., MaxSAT), but no proven speedup.</li>
            </ul>
            </li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Quantum (Grover)</th>
                        <th>Classical Approximation</th>
                        <th>Achievable Speedup?</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Query complexity</strong></td>
                        <td>\(O(\sqrt{N})\)</td>
                        <td>\(\Omega(N)\) (worst-case lower bound)</td>
                        <td>❌ No</td>
                    </tr>
                    <tr>
                        <td><strong>Amplitude amplification</strong></td>
                        <td>Coherent interference</td>
                        <td>Weighted sampling / boosting</td>
                        <td>❌ (No interference)</td>
                    </tr>
                    <tr>
                        <td><strong>Oracle usage</strong></td>
                        <td>Applied to superposition</td>
                        <td>Applied to single inputs</td>
                        <td>—</td>
                    </tr>
                    <tr>
                        <td><strong>Multiple solutions (\(M\))</strong></td>
                        <td>\(O(\sqrt{N/M})\)</td>
                        <td>\(O(N/M)\) (random sampling)</td>
                        <td>✅ Quadratic gap remains</td>
                    </tr>
                    <tr>
                        <td><strong>Real-world applicability</strong></td>
                        <td>Limited by QRAM, noise</td>
                        <td>Widely used heuristics</td>
                        <td>✅ Practical gains</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>True quadratic speedup is uniquely quantum</strong> for unstructured search—<strong>not replicable classically</strong>.</li>
            <li><strong>Quantum-inspired classical algorithms</strong> borrow <em>ideas</em> (iterative boosting, weighting) but <strong>lack interference</strong>.</li>
            <li><strong>Oracle design</strong> and <strong>problem structure</strong> dominate real-world performance—pure unstructured search is rare.</li>
            <li><strong>Applications</strong> benefit more from <strong>hybrid approaches</strong> (e.g., classical pre-processing + quantum search) than pure emulation.</li>
            <li><strong>No free lunch</strong>: Classical methods can't beat information-theoretic lower bounds—but can exploit structure Grover ignores.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Final Thought</strong>: Quantum-inspired search is valuable for <strong>algorithm design intuition</strong> and <strong>heuristic development</strong>, but it does <strong>not challenge the fundamental quantum advantage</strong> of Grover's algorithm in its native setting.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Machine Learning Fundamentals</h2>
    <div class="content">
        <h3>Supervised Learning</h3>
        <div class="definition">
            <strong>Definition</strong>: Learning a mapping from input features \( \mathbf{x} \in \mathbb{R}^d \) to output labels \( y \) using a <strong>labeled training dataset</strong> \( \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n \).
        </div>
        
        <p>Two main types:</p>
        <ul>
            <li><strong>Classification</strong>: \( y \) is <strong>categorical</strong> (e.g., spam/not spam).</li>
            <li><strong>Regression</strong>: \( y \) is <strong>continuous</strong> (e.g., house price).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Classification and Regression Principles</h2>
    <div class="content">
        <h3>Goal</h3>
        <ul>
            <li>Learn a <strong>hypothesis function</strong> \( h_\theta(\mathbf{x}) \) that approximates the true relationship \( y \approx f(\mathbf{x}) \).</li>
            <li>Minimize <strong>prediction error</strong> on unseen data (<strong>generalization</strong>).</li>
        </ul>
        
        <h3>Loss Functions</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Common Loss Function</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Regression</strong></td>
                        <td>Mean Squared Error (MSE): \( \frac{1}{n}\sum (y_i - \hat{y}_i)^2 \)</td>
                        <td>Penalizes large errors quadratically</td>
                    </tr>
                    <tr>
                        <td><strong>Classification</strong></td>
                        <td>Cross-Entropy Loss: \( -\sum y_i \log(\hat{y}_i) \)</td>
                        <td>Measures divergence between true and predicted probabilities</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Bias-Variance Tradeoff</h3>
        <ul>
            <li><strong>High bias</strong> → underfitting (model too simple).</li>
            <li><strong>High variance</strong> → overfitting (model too complex).</li>
            <li>Goal: Find model complexity that balances both.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Decision Trees and Ensemble Methods</h2>
    <div class="content">
        <h3>Decision Trees</h3>
        <ul>
            <li><strong>Structure</strong>: Tree of <strong>if-else rules</strong> based on feature thresholds.</li>
            <li><strong>Splitting criterion</strong>:
            <ul>
                <li><strong>Classification</strong>: Information gain (based on entropy) or Gini impurity.<br>
                \[ \text{Gini} = 1 - \sum_{k} p_k^2 \]</li>
                <li><strong>Regression</strong>: Minimize MSE after split.</li>
            </ul>
            </li>
            <li><strong>Pros</strong>: Interpretable, handles mixed data types, no feature scaling needed.</li>
            <li><strong>Cons</strong>: Prone to overfitting, unstable to data changes.</li>
        </ul>
        
        <h3>Ensemble Methods</h3>
        <p>Combine multiple models to improve performance and robustness.</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Key Properties</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Bagging</strong> (e.g., <strong>Random Forest</strong>)</td>
                        <td>Train many trees on <strong>bootstrap samples</strong>; average predictions (regression) or vote (classification)</td>
                        <td>Reduces variance; decorrelates trees by random feature subsets</td>
                    </tr>
                    <tr>
                        <td><strong>Boosting</strong> (e.g., <strong>AdaBoost, XGBoost</strong>)</td>
                        <td>Sequentially train weak learners; each focuses on <strong>previous errors</strong></td>
                        <td>Reduces bias; highly accurate; sensitive to noise</td>
                    </tr>
                    <tr>
                        <td><strong>Stacking</strong></td>
                        <td>Use predictions of base models as input to a <strong>meta-learner</strong></td>
                        <td>Leverages strengths of diverse models</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Random Forest</strong>: Often a strong baseline—robust, parallelizable, handles missing data.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Support Vector Machines (SVM)</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Find the <strong>optimal separating hyperplane</strong> that maximizes the <strong>margin</strong> (distance to nearest points).</p>
        
        <h3>Mathematical Formulation</h3>
        <p>For linearly separable data:</p>
        <div class="equation">
            \[ \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{subject to} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \]
        </div>
        <p><strong>Support vectors</strong>: Data points on the margin (only these affect the solution).</p>
        
        <h3>Non-linear SVM: Kernel Trick</h3>
        <ul>
            <li>Map data to higher-dimensional space via <strong>kernel function</strong> \( K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j) \).</li>
            <li>Common kernels:
            <ul>
                <li><strong>Linear</strong>: \( K = \mathbf{x}_i^\top \mathbf{x}_j \)</li>
                <li><strong>Polynomial</strong>: \( K = (\gamma \mathbf{x}_i^\top \mathbf{x}_j + r)^d \)</li>
                <li><strong>RBF (Gaussian)</strong>: \( K = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2) \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Soft Margin (for noisy data)</h3>
        <ul>
            <li>Introduce <strong>slack variables</strong> \( \xi_i \) to allow misclassifications.</li>
            <li>Controlled by <strong>regularization parameter \( C \)</strong>:
            <ul>
                <li>Large \( C \) → hard margin (low bias, high variance)</li>
                <li>Small \( C \) → soft margin (high bias, low variance)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Pros & Cons</h3>
        <ul>
            <li>✅ Effective in high dimensions, memory efficient (uses support vectors only).</li>
            <li>❌ Poor performance on large datasets; requires careful kernel and \( C \) tuning.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Neural Networks and Backpropagation</h2>
    <div class="content">
        <h3>Neural Network Structure</h3>
        <ul>
            <li><strong>Layers</strong>: Input → hidden layers → output.</li>
            <li><strong>Neuron</strong>: \( z = \mathbf{w}^\top \mathbf{x} + b \), \( a = \sigma(z) \)</li>
            <li><strong>Activation functions</strong>:
            <ul>
                <li><strong>Sigmoid</strong>: \( \sigma(z) = \frac{1}{1 + e^{-z}} \) → outputs probabilities (classification)</li>
                <li><strong>ReLU</strong>: \( \max(0, z) \) → avoids vanishing gradient; standard in hidden layers</li>
                <li><strong>Softmax</strong>: For multi-class output: \( \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}} \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Backpropagation</h3>
        <ul>
            <li><strong>Goal</strong>: Compute gradients of loss \( \mathcal{L} \) w.r.t. all weights for gradient descent.</li>
            <li><strong>Algorithm</strong>:
            <ol>
                <li><strong>Forward pass</strong>: Compute predictions \( \hat{y} \).</li>
                <li><strong>Compute loss</strong> \( \mathcal{L}(y, \hat{y}) \).</li>
                <li><strong>Backward pass</strong>: Apply chain rule from output to input:<br>
                \[ \frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \frac{\partial \mathcal{L}}{\partial a_j^{(l)}} \cdot \frac{\partial a_j^{(l)}}{\partial z_j^{(l)}} \cdot \frac{\partial z_j^{(l)}}{\partial w_{ij}^{(l)}} \]</li>
                <li>Update weights: \( w \leftarrow w - \eta \nabla_w \mathcal{L} \) (\(\eta\) = learning rate)</li>
            </ol>
            </li>
        </ul>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>Universal approximation theorem</strong>: A NN with 1 hidden layer can approximate any continuous function (given enough neurons).</li>
            <li><strong>Deep learning</strong>: Multiple hidden layers learn hierarchical features (e.g., edges → shapes → objects in images).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Model Evaluation Metrics</h2>
    <div class="content">
        <h3>For Classification</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Formula</th>
                        <th>When to Use</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Accuracy</strong></td>
                        <td>\( \frac{TP + TN}{TP + TN + FP + FN} \)</td>
                        <td>Balanced datasets</td>
                    </tr>
                    <tr>
                        <td><strong>Precision</strong></td>
                        <td>\( \frac{TP}{TP + FP} \)</td>
                        <td>Minimize false positives (e.g., spam detection)</td>
                    </tr>
                    <tr>
                        <td><strong>Recall (Sensitivity)</strong></td>
                        <td>\( \frac{TP}{TP + FN} \)</td>
                        <td>Minimize false negatives (e.g., disease screening)</td>
                    </tr>
                    <tr>
                        <td><strong>F1-Score</strong></td>
                        <td>\( 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \)</td>
                        <td>Imbalanced datasets; harmonic mean of P & R</td>
                    </tr>
                    <tr>
                        <td><strong>ROC-AUC</strong></td>
                        <td>Area under ROC curve (TPR vs. FPR)</td>
                        <td>Threshold-invariant performance</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Confusion Matrix</strong>:<br>
            <pre>               Predicted
              +       -
Actual   +   TP      FN
         -   FP      TN</pre>
        </div>
        
        <h3>For Regression</h3>
        <ul>
            <li><strong>Mean Absolute Error (MAE)</strong>: \( \frac{1}{n}\sum |y_i - \hat{y}_i| \) → robust to outliers.</li>
            <li><strong>Mean Squared Error (MSE)</strong>: \( \frac{1}{n}\sum (y_i - \hat{y}_i)^2 \) → penalizes large errors.</li>
            <li><strong>R² (Coefficient of Determination)</strong>:<br>
            \[ R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} \]
            <ul>
                <li>\( R^2 = 1 \): perfect fit; \( R^2 = 0 \): no better than mean predictor.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Cross-Validation</h3>
        <ul>
            <li><strong>k-Fold CV</strong>: Split data into \(k\) folds; train on \(k-1\), validate on 1; repeat \(k\) times.</li>
            <li>Provides <strong>unbiased estimate</strong> of generalization performance.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Supervised Learning Algorithms</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Type</th>
                        <th>Strengths</th>
                        <th>Weaknesses</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Decision Tree</strong></td>
                        <td>Both</td>
                        <td>Interpretable, fast</td>
                        <td>Overfits</td>
                        <td>Baseline, explainability</td>
                    </tr>
                    <tr>
                        <td><strong>Random Forest</strong></td>
                        <td>Both</td>
                        <td>Robust, handles noise</td>
                        <td>Less interpretable</td>
                        <td>General-purpose</td>
                    </tr>
                    <tr>
                        <td><strong>SVM</strong></td>
                        <td>Both</td>
                        <td>Effective in high-D, memory efficient</td>
                        <td>Slow on large data</td>
                        <td>Text classification, small/medium datasets</td>
                    </tr>
                    <tr>
                        <td><strong>Neural Network</strong></td>
                        <td>Both</td>
                        <td>Learns complex patterns</td>
                        <td>Data-hungry, black-box</td>
                        <td>Images, speech, large datasets</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Supervised learning</strong> requires labeled data and aims to generalize from examples.</li>
            <li><strong>No free lunch</strong>: Algorithm choice depends on data size, dimensionality, noise, and interpretability needs.</li>
            <li><strong>Evaluation metrics must match the problem</strong>: Accuracy fails on imbalanced data; use F1 or AUC instead.</li>
            <li><strong>Ensembles</strong> (e.g., Random Forest, XGBoost) often outperform single models.</li>
            <li><strong>Neural networks</strong> excel with large data but require careful tuning and validation.</li>
        </ul>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Unsupervised Learning</h2>
    <div class="content">
        <p><em>Learning patterns from unlabeled data</em></p>
        
        <div class="definition">
            <strong>Definition</strong>: Discover hidden structures, patterns, or relationships in data <strong>without labeled outputs</strong>.<br>
            <strong>Goal</strong>: Summarize, compress, or reveal intrinsic properties of the data.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Clustering Algorithms</h2>
    <div class="content">
        <p>Clustering groups similar data points together based on feature similarity.</p>
        
        <h3>(a) K-Means Clustering</h3>
        <ul>
            <li><strong>Objective</strong>: Partition \(n\) points into \(k\) clusters to minimize <strong>within-cluster sum of squares (WCSS)</strong>:<br>
            \[ \min_{C_1,\dots,C_k} \sum_{i=1}^k \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2 \]
            where \(\boldsymbol{\mu}_i\) = centroid of cluster \(i\).</li>
            
            <li><strong>Algorithm</strong> (Lloyd's):
            <ol>
                <li>Initialize \(k\) centroids randomly.</li>
                <li><strong>Assign</strong>: Assign each point to nearest centroid.</li>
                <li><strong>Update</strong>: Recompute centroids as mean of assigned points.</li>
                <li>Repeat until convergence.</li>
            </ol>
            </li>
            
            <li><strong>Pros</strong>: Simple, fast, scalable.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li>Requires pre-specifying \(k\).</li>
                <li>Sensitive to initialization and outliers.</li>
                <li>Assumes spherical, equally sized clusters.</li>
            </ul>
            </li>
            
            <li><strong>Choosing \(k\)</strong>: Use <strong>elbow method</strong> (plot WCSS vs. \(k\)) or <strong>silhouette score</strong>.</li>
        </ul>
        
        <h3>(b) Hierarchical Clustering</h3>
        <ul>
            <li>Builds a <strong>tree of clusters</strong> (dendrogram).</li>
            <li>Two approaches:
            <ul>
                <li><strong>Agglomerative</strong> (bottom-up): Start with each point as a cluster; merge closest pairs.</li>
                <li><strong>Divisive</strong> (top-down): Start with one cluster; recursively split.</li>
            </ul>
            </li>
            
            <li><strong>Linkage criteria</strong> (define distance between clusters):
            <ul>
                <li><strong>Single</strong>: min distance → chaining effect.</li>
                <li><strong>Complete</strong>: max distance → compact clusters.</li>
                <li><strong>Average</strong>: mean distance → balanced.</li>
                <li><strong>Ward</strong>: minimizes WCSS → similar to K-means.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>: No need to specify \(k\); dendrogram gives full hierarchy.</li>
            <li><strong>Cons</strong>: \(O(n^3)\) time; not scalable to large datasets.</li>
        </ul>
        
        <h3>(c) DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h3>
        <ul>
            <li><strong>Core idea</strong>: Cluster points in <strong>dense regions</strong>, mark sparse points as <strong>noise</strong>.</li>
            <li><strong>Parameters</strong>:
            <ul>
                <li>\(\varepsilon\): Radius of neighborhood.</li>
                <li>MinPts: Minimum points in \(\varepsilon\)-neighborhood to be a <strong>core point</strong>.</li>
            </ul>
            </li>
            
            <li><strong>Point types</strong>:
            <ul>
                <li><strong>Core</strong>: ≥ MinPts in \(\varepsilon\)-neighborhood.</li>
                <li><strong>Border</strong>: In neighborhood of core, but not core itself.</li>
                <li><strong>Noise</strong>: Neither core nor border.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>:
            <ul>
                <li>Finds arbitrarily shaped clusters.</li>
                <li>Robust to outliers.</li>
                <li>No need to specify \(k\).</li>
            </ul>
            </li>
            <li><strong>Cons</strong>: Struggles with varying densities; sensitive to \(\varepsilon\) and MinPts.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Spatial data, anomaly detection, irregular cluster shapes.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Dimensionality Reduction</h2>
    <div class="content">
        <p>Reduce number of features while preserving essential structure.</p>
        
        <h3>(a) Principal Component Analysis (PCA)</h3>
        <ul>
            <li><strong>Goal</strong>: Project data onto orthogonal axes (<strong>principal components</strong>) that capture <strong>maximum variance</strong>.</li>
            <li><strong>Steps</strong>:
            <ol>
                <li>Standardize data (zero mean, unit variance).</li>
                <li>Compute covariance matrix.</li>
                <li>Eigendecomposition: Find eigenvectors (PCs) and eigenvalues (variance explained).</li>
                <li>Project data onto top \(k\) PCs.</li>
            </ol>
            </li>
            
            <li><strong>Mathematically</strong>:<br>
            \[ \mathbf{Z} = \mathbf{X} \mathbf{W}_k \]
            where \(\mathbf{W}_k\) = top \(k\) eigenvectors.</li>
            
            <li><strong>Pros</strong>: Linear, fast, removes multicollinearity.</li>
            <li><strong>Cons</strong>: Only captures linear relationships; global structure only.</li>
            <li><strong>Use cases</strong>: Visualization (2D/3D), noise reduction, preprocessing for ML.</li>
        </ul>
        
        <h3>(b) t-SNE (t-Distributed Stochastic Neighbor Embedding)</h3>
        <ul>
            <li><strong>Goal</strong>: Preserve <strong>local similarities</strong> in low-dimensional embedding (typically 2D/3D).</li>
            <li><strong>Key idea</strong>:
            <ul>
                <li>Convert high-D Euclidean distances to <strong>probabilities</strong> (similar points → high probability).</li>
                <li>Minimize <strong>KL divergence</strong> between high-D and low-D probability distributions.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>: Excellent for <strong>visualization</strong>; reveals clusters and manifolds.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li>Not linear; <strong>not for preprocessing</strong> (distorts global structure).</li>
                <li>Computationally expensive (\(O(n^2)\)).</li>
                <li>Results vary with perplexity (key hyperparameter).</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            📌 <strong>Rule</strong>: Use <strong>PCA first</strong> (to reduce to 50D), then <strong>t-SNE</strong> for visualization.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Anomaly Detection Methods</h2>
    <div class="content">
        <p>Identify rare items, events, or observations that differ significantly from the majority.</p>
        
        <h3>Common Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Statistical</strong></td>
                        <td>Assume data follows distribution (e.g., Gaussian); flag points with low likelihood</td>
                        <td>Univariate data, known distribution</td>
                    </tr>
                    <tr>
                        <td><strong>Distance-based</strong></td>
                        <td>Points far from neighbors are anomalies (e.g., k-NN distance)</td>
                        <td>Multivariate, no distribution assumption</td>
                    </tr>
                    <tr>
                        <td><strong>Density-based</strong></td>
                        <td>Low-density regions = anomalies (e.g., LOF: Local Outlier Factor)</td>
                        <td>Varying densities</td>
                    </tr>
                    <tr>
                        <td><strong>Clustering-based</strong></td>
                        <td>Points not in any cluster (or in small clusters) are anomalies</td>
                        <td>Works with DBSCAN, K-means</td>
                    </tr>
                    <tr>
                        <td><strong>Reconstruction-based</strong></td>
                        <td>Use autoencoders (see below); high reconstruction error = anomaly</td>
                        <td>High-dimensional, complex data</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Evaluation</h3>
        <ul>
            <li>Often <strong>semi-supervised</strong>: Assume most data is normal.</li>
            <li>Metrics: Precision@K, ROC-AUC (if labels available).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Autoencoders and Their Variants</h2>
    <div class="content">
        <p>Neural networks that learn efficient data <strong>encodings</strong> in an unsupervised manner.</p>
        
        <h3>Basic Autoencoder</h3>
        <ul>
            <li><strong>Architecture</strong>:<br>
            <strong>Encoder</strong>: \( \mathbf{z} = f(\mathbf{x}) = \sigma(\mathbf{W}_e \mathbf{x} + \mathbf{b}_e) \)<br>
            <strong>Decoder</strong>: \( \hat{\mathbf{x}} = g(\mathbf{z}) = \sigma(\mathbf{W}_d \mathbf{z} + \mathbf{b}_d) \)</li>
            <li><strong>Loss</strong>: Reconstruction error (e.g., MSE): \( \mathcal{L} = \|\mathbf{x} - \hat{\mathbf{x}}\|^2 \)</li>
            <li><strong>Bottleneck</strong>: Latent dimension \( < \) input dimension → forces compression.</li>
        </ul>
        
        <h3>Key Variants</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Variant</th>
                        <th>Modification</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Denoising Autoencoder (DAE)</strong></td>
                        <td>Input: corrupted \(\tilde{\mathbf{x}}\); target: clean \(\mathbf{x}\)</td>
                        <td>Learn robust features; prevent identity mapping</td>
                    </tr>
                    <tr>
                        <td><strong>Sparse Autoencoder</strong></td>
                        <td>Add sparsity penalty on latent code (e.g., KL divergence on activations)</td>
                        <td>Learn part-based representations</td>
                    </tr>
                    <tr>
                        <td><strong>Variational Autoencoder (VAE)</strong></td>
                        <td>Latent code sampled from learned distribution \(q(z|x)\); regularized via KL divergence to prior \(p(z)\)</td>
                        <td>Generative model; smooth latent space</td>
                    </tr>
                    <tr>
                        <td><strong>Convolutional Autoencoder</strong></td>
                        <td>Use CNN layers in encoder/decoder</td>
                        <td>Handle image data; preserve spatial structure</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Applications</h3>
        <ul>
            <li>Dimensionality reduction (non-linear alternative to PCA)</li>
            <li>Anomaly detection (high reconstruction error = anomaly)</li>
            <li>Denoising, image inpainting</li>
            <li>Pretraining for deep networks</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Association Rule Learning</h2>
    <div class="content">
        <p>Discover <strong>interesting relationships</strong> between variables in large datasets (e.g., market basket analysis).</p>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>Itemset</strong>: Collection of items (e.g., {milk, bread}).</li>
            <li><strong>Transaction</strong>: A single record (e.g., customer purchase).</li>
            <li><strong>Rule</strong>: \( X \Rightarrow Y \) (e.g., {milk} ⇒ {bread})</li>
        </ul>
        
        <h3>Metrics</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Formula</th>
                        <th>Interpretation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Support</strong></td>
                        <td>\( \frac{\text{# transactions containing } X \cup Y}{\text{total transactions}} \)</td>
                        <td>How frequent is the rule?</td>
                    </tr>
                    <tr>
                        <td><strong>Confidence</strong></td>
                        <td>\( \frac{\text{support}(X \cup Y)}{\text{support}(X)} \)</td>
                        <td>How often does Y occur when X occurs?</td>
                    </tr>
                    <tr>
                        <td><strong>Lift</strong></td>
                        <td>\( \frac{\text{confidence}(X \Rightarrow Y)}{\text{support}(Y)} \)</td>
                        <td>>1: positive correlation; =1: independent; <1: negative</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Apriori Algorithm</h3>
        <ul>
            <li><strong>Principle</strong>: If an itemset is frequent, all its subsets are frequent (<strong>anti-monotonicity</strong>).</li>
            <li><strong>Steps</strong>:
            <ol>
                <li>Find all frequent 1-itemsets (support ≥ min_support).</li>
                <li>Generate candidate \(k\)-itemsets from frequent \((k-1)\)-itemsets.</li>
                <li>Prune candidates with infrequent subsets.</li>
                <li>Repeat until no more frequent itemsets.</li>
            </ol>
            </li>
            <li><strong>Pros</strong>: Simple, guarantees completeness.</li>
            <li><strong>Cons</strong>: Computationally expensive for large itemsets.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Modern alternatives</strong>: FP-Growth (uses frequent pattern tree; faster, no candidate generation).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Type</th>
                        <th>Key Strength</th>
                        <th>Limitation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>K-Means</strong></td>
                        <td>Clustering</td>
                        <td>Fast, simple</td>
                        <td>Spherical clusters only</td>
                    </tr>
                    <tr>
                        <td><strong>DBSCAN</strong></td>
                        <td>Clustering</td>
                        <td>Finds arbitrary shapes, handles noise</td>
                        <td>Struggles with density variation</td>
                    </tr>
                    <tr>
                        <td><strong>PCA</strong></td>
                        <td>Dim. Reduction</td>
                        <td>Linear, fast, interpretable</td>
                        <td>Misses non-linear structure</td>
                    </tr>
                    <tr>
                        <td><strong>t-SNE</strong></td>
                        <td>Dim. Reduction</td>
                        <td>Excellent visualization</td>
                        <td>Not for preprocessing; slow</td>
                    </tr>
                    <tr>
                        <td><strong>Autoencoder</strong></td>
                        <td>Representation</td>
                        <td>Non-linear, flexible</td>
                        <td>Requires tuning; black-box</td>
                    </tr>
                    <tr>
                        <td><strong>Association Rules</strong></td>
                        <td>Pattern Mining</td>
                        <td>Interpretable business rules</td>
                        <td>Combinatorial explosion</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Clustering</strong> reveals group structure; choose algorithm based on shape, noise, and scalability.</li>
            <li><strong>Dimensionality reduction</strong>: Use <strong>PCA</strong> for linear/global structure, <strong>t-SNE</strong> for visualization.</li>
            <li><strong>Anomaly detection</strong> is critical in fraud, security, and quality control—method depends on data assumptions.</li>
            <li><strong>Autoencoders</strong> provide powerful non-linear compression and are foundational for deep generative models.</li>
            <li><strong>Association rules</strong> uncover actionable insights in transactional data—but require careful thresholding.</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Remember</strong>: Unsupervised learning is <strong>exploratory</strong>—results must be validated with domain knowledge!
        </div>
    </div>
</section>






        <div class="references">
            <div class="ref-title">References:</div>
            <div class="ref-item">[1] Rudin, W. (1976). Principles of Mathematical Analysis. McGraw-Hill.</div>
            <div class="ref-item">[2] Apostol, T. M. (1967). Calculus, Vol. I. John Wiley & Sons.</div>
            <div class="ref-item">[3] Tao, T. (2006). Analysis I. Hindustan Book Agency.</div>
            <div class="ref-item">[4] Kolmogorov, A. N., & Fomin, S. V. (1957). Elements of the Theory of Functions and Functional Analysis. Dover Publications.</div>
        </div>

        <footer>
            <p>© 2023 Mathematical Analysis Research Group | University of Excellence</p>
            <p>For academic use only | This document is optimized for printing</p>
        </footer>
        

    </div>

    <script>
        // This script is included for demonstration purposes
        // In a real implementation, you would add content dynamically here
        document.addEventListener('DOMContentLoaded', function() {
            // Example of how you might add content dynamically
            // This would be replaced with your content insertion logic
            console.log('Document ready for content insertion');
        });
    </script>
</body>
</html>