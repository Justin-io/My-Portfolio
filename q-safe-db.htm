<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Academic Document</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
            counter-reset: section;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
            padding: 40px;
        }

        header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #e0e0e0;
        }

        h1 {
            font-size: 2.5rem;
            color: #2c3e50;
            margin-bottom: 10px;
        }

        .subtitle {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 15px;
        }

        .author-info {
            display: flex;
            justify-content: space-between;
            margin-top: 15px;
            font-size: 0.9rem;
            color: #7f8c8d;
        }

        .section {
            margin-bottom: 30px;
            padding: 20px 0;
            border-bottom: 1px solid #eee;
        }

        .section-title {
            font-size: 1.8rem;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 1px solid #eee;
            color: #2c3e50;
            position: relative;
            counter-increment: section;
        }

        .section-title::before {
            content: counter(section) ". ";
            color: #3498db;
        }

        .content {
            padding: 10px 0;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .equation {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background-color: #f9f9f9;
            border-left: 4px solid #3498db;
            font-size: 1.2rem;
        }

        .table-container {
            overflow-x: auto;
            margin: 20px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 0.9rem;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        .highlight {
            background-color: #e3f2fd;
            padding: 2px 4px;
            border-radius: 3px;
        }

        .definition {
            border-left: 4px solid #27ae60;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #f8fff8;
        }

        .theorem {
            border-left: 4px solid #e74c3c;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #fff8f8;
        }

        .proof {
            border-left: 4px solid #9b59b6;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #f8f8ff;
            font-style: italic;
        }

        .references {
            margin-top: 30px;
        }

        .ref-title {
            font-weight: bold;
            margin-bottom: 10px;
        }

        .ref-item {
            margin-bottom: 8px;
            padding-left: 20px;
            text-indent: -20px;
        }

        footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            color: #7f8c8d;
            font-size: 0.9rem;
        }

        @media print {
            body {
                background-color: white;
                padding: 0;
            }
            
            .container {
                box-shadow: none;
                padding: 20px;
                max-width: 100%;
            }
            
            button {
                display: none;
            }
        }

        .print-btn {
            display: block;
            margin: 20px auto;
            padding: 12px 30px;
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background-color 0.3s;
        }

        .print-btn:hover {
            background-color: #2980b9;
        }

        .print-btn i {
            margin-right: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
    <h1>Quantum Information and Computing</h1>
    <div class="subtitle">A Comprehensive Study of Fundamental Concepts</div>
    <div class="author-info">
        <div>Harinandan K</div>
        <div>Theoretical Physics Researcher & Quantum Computing Enthusiast</div>
        <div>Kerala, India</div>
    </div>
</header>

        <section class="section">
    <h2 class="section-title">Quantum Bits (Qubits)</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing Fundamentals</em></p>
        
        <h3>Definition and Properties of Qubits</h3>
        <h4>What is a Qubit?</h4>
        <p>A <strong>qubit</strong> (quantum bit) is the fundamental unit of quantum information, analogous to the classical bit in classical computing.</p>
        <p>Unlike a classical bit, which can only be in state <strong>0</strong> or <strong>1</strong>, a qubit can exist in a <strong>superposition</strong> of both states simultaneously.</p>
        
        <h4>Key Properties of Qubits</h4>
        <ol>
            <li><strong>Superposition</strong>: Can be in a linear combination of |0⟩ and |1⟩.</li>
            <li><strong>Entanglement</strong>: Qubits can be correlated in ways that classical bits cannot (though this is more relevant for multi-qubit systems).</li>
            <li><strong>Measurement Collapse</strong>: When measured, the qubit collapses probabilistically to either |0⟩ or |1⟩.</li>
            <li><strong>No-Cloning Theorem</strong>: An unknown quantum state cannot be perfectly copied.</li>
            <li><strong>Unitary Evolution</strong>: Qubit states evolve via reversible, unitary operations (quantum gates).</li>
        </ol>
        
        <h4>Physical Realizations</h4>
        <p>Qubits can be implemented using:</p>
        <ul>
            <li>Electron or nuclear spin (e.g., in NMR or quantum dots)</li>
            <li>Photon polarization</li>
            <li>Superconducting circuits (e.g., transmon qubits)</li>
            <li>Trapped ions</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Superposition Principle and Its Mathematical Representation</h2>
    <div class="content">
        <h3>Superposition Principle</h3>
        <p>A qubit can be in a state that is a <strong>linear combination</strong> of the basis states |0⟩ and |1⟩:</p>
        <div class="equation">
            \[ |\psi\rangle = \alpha |0\rangle + \beta |1\rangle \]
        </div>
        <p>where:</p>
        <ul>
            <li>\(\alpha, \beta \in \mathbb{C}\) (complex numbers),</li>
            <li>\(|\alpha|^2 + |\beta|^2 = 1\) (normalization condition).</li>
        </ul>
        
        <h3>Interpretation</h3>
        <ul>
            <li>\(|\alpha|^2\) = probability of measuring the qubit as <strong>0</strong>.</li>
            <li>\(|\beta|^2\) = probability of measuring the qubit as <strong>1</strong>.</li>
            <li>The relative <strong>phase</strong> between \(\alpha\) and \(\beta\) is physically significant (e.g., for interference).</li>
        </ul>
        
        <h3>Example</h3>
        <p>Equal superposition state:</p>
        <div class="equation">
            \[ |+\rangle = \frac{1}{\sqrt{2}}|0\rangle + \frac{1}{\sqrt{2}}|1\rangle \]
        </div>
        <p>→ 50% chance of measuring 0 or 1.</p>
        
        <p>Another example:</p>
        <div class="equation">
            \[ |\psi\rangle = \frac{\sqrt{3}}{2}|0\rangle + \frac{1}{2}|1\rangle \]
        </div>
        <p>→ Probability of 0: \( \left|\frac{\sqrt{3}}{2}\right|^2 = \frac{3}{4} \),<br>
        Probability of 1: \( \left|\frac{1}{2}\right|^2 = \frac{1}{4} \).</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Bloch Sphere Visualization of Qubit States</h2>
    <div class="content">
        <h3>Why the Bloch Sphere?</h3>
        <p>Any single-qubit pure state can be represented as a point on the surface of a unit sphere called the <strong>Bloch sphere</strong>.</p>
        <p>Provides an intuitive geometric representation of qubit states.</p>
        
        <h3>Mathematical Mapping</h3>
        <p>A general qubit state can be written as:</p>
        <div class="equation">
            \[ |\psi\rangle = \cos\left(\frac{\theta}{2}\right)|0\rangle + e^{i\phi}\sin\left(\frac{\theta}{2}\right)|1\rangle \]
        </div>
        <p>where:</p>
        <ul>
            <li>\(0 \leq \theta \leq \pi\)</li>
            <li>\(0 \leq \phi < 2\pi\)</li>
        </ul>
        
        <h3>Coordinates on the Bloch Sphere</h3>
        <ul>
            <li><strong>North pole</strong>: |0⟩</li>
            <li><strong>South pole</strong>: |1⟩</li>
            <li><strong>Equator</strong>: Superposition states with equal probabilities (e.g., |+⟩, |−⟩, |i⟩, |−i⟩)</li>
            <li><strong>θ (theta)</strong>: Polar angle from z-axis → determines probability amplitudes.</li>
            <li><strong>φ (phi)</strong>: Azimuthal angle in xy-plane → determines relative phase.</li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: Global phase (e.g., multiplying the entire state by \(e^{i\gamma}\)) has no physical effect and is ignored.
        </div>
        
        <h3>Visualization</h3>
        <p>X, Y, Z axes correspond to measurement bases:</p>
        <ul>
            <li>Z-basis: {|0⟩, |1⟩}</li>
            <li>X-basis: {|+⟩, |−⟩}</li>
            <li>Y-basis: {|i⟩, |−i⟩}</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Measurement and Collapse of Quantum States</h2>
    <div class="content">
        <h3>Quantum Measurement Postulate</h3>
        <p>When a qubit in state \(|\psi\rangle = \alpha|0\rangle + \beta|1\rangle\) is measured in the computational (Z) basis:</p>
        <ul>
            <li>Outcome is <strong>0</strong> with probability \(|\alpha|^2\)</li>
            <li>Outcome is <strong>1</strong> with probability \(|\beta|^2\)</li>
        </ul>
        <p><strong>Immediately after measurement</strong>, the state <strong>collapses</strong> to the observed basis state:</p>
        <ul>
            <li>If result is 0 → state becomes |0⟩</li>
            <li>If result is 1 → state becomes |1⟩</li>
        </ul>
        
        <h3>Irreversibility</h3>
        <p>Measurement is <strong>non-unitary</strong> and <strong>irreversible</strong> information about the original superposition is lost.</p>
        
        <h3>Basis Dependence</h3>
        <p>Measurement can be performed in any orthonormal basis (e.g., X-basis).</p>
        <p>Example: Measuring |+⟩ in X-basis always yields "+", but in Z-basis yields 0 or 1 with 50% probability each.</p>
        
        <h3>Expectation Value</h3>
        <p>For an observable (e.g., Pauli-Z operator), the expectation value is:</p>
        <div class="equation">
            \[ \langle Z \rangle = \langle \psi | Z | \psi \rangle = |\alpha|^2 - |\beta|^2 \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Comparison with Classical Bits</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Classical Bit</th>
                        <th>Qubit</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>State</td>
                        <td>0 <strong>or</strong> 1</td>
                        <td>\(\alpha|0\rangle + \beta|1\rangle\) (superposition)</td>
                    </tr>
                    <tr>
                        <td>State Space</td>
                        <td>Discrete (2 states)</td>
                        <td>Continuous (infinite states on Bloch sphere)</td>
                    </tr>
                    <tr>
                        <td>Measurement</td>
                        <td>Reveals existing value (non-destructive)</td>
                        <td>Probabilistic; <strong>collapses</strong> the state</td>
                    </tr>
                    <tr>
                        <td>Copying</td>
                        <td>Can be copied perfectly</td>
                        <td><strong>Cannot</strong> be copied (No-Cloning Theorem)</td>
                    </tr>
                    <tr>
                        <td>Information Capacity</td>
                        <td>1 bit per bit</td>
                        <td>Infinite info in state, but only <strong>1 bit</strong> extractable per measurement</td>
                    </tr>
                    <tr>
                        <td>Operations</td>
                        <td>Logic gates (AND, OR, NOT)</td>
                        <td>Reversible <strong>unitary gates</strong> (e.g., Hadamard, Pauli)</td>
                    </tr>
                    <tr>
                        <td>Parallelism</td>
                        <td>No inherent parallelism</td>
                        <td><strong>Quantum parallelism</strong>: operations on all states in superposition</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Key Insight</strong>: While a single qubit holds more <em>potential</em> information than a classical bit, <strong>only one classical bit</strong> can be extracted per measurement due to collapse. The power of quantum computing arises from <strong>interference</strong> and <strong>entanglement</strong> across many qubits not from storing more data per qubit.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <ul>
            <li>A <strong>qubit</strong> generalizes the classical bit using quantum mechanics.</li>
            <li>It leverages <strong>superposition</strong> to process multiple states at once.</li>
            <li>The <strong>Bloch sphere</strong> offers a geometric view of all possible single-qubit states.</li>
            <li><strong>Measurement</strong> is probabilistic and destructive to superposition.</li>
            <li>Qubits are <strong>not just probabilistic bits</strong> phase and interference are crucial for quantum advantage.</li>
        </ul>
    </div>
</section>
<section class="section">
    <h2 class="section-title">Quantum Entanglement</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Information and Computing</em></p>
        
        <h3>Definition and Properties of Entangled States</h3>
        <h4>What is Quantum Entanglement?</h4>
        <p><strong>Entanglement</strong> is a uniquely quantum phenomenon where two or more particles become <strong>correlated</strong> in such a way that the quantum state of each particle <strong>cannot be described independently</strong> of the others even when separated by large distances.</p>
        <p>The combined system is described by a <strong>single quantum state</strong>, but individual subsystems <strong>do not have definite states</strong> on their own.</p>
        
        <h4>Formal Definition</h4>
        <p>A pure bipartite state \(|\psi\rangle_{AB}\) is <strong>entangled</strong> if it <strong>cannot</strong> be written as a <strong>product state</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle_{AB} \neq |\phi\rangle_A \otimes |\chi\rangle_B \]
        </div>
        <p>If such a factorization is possible, the state is <strong>separable</strong> (not entangled).</p>
        
        <div class="definition">
            <strong>Example of entangled state</strong>:<br>
            \(|\Phi^+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)\) → cannot be split into individual qubit states.
        </div>
        
        <h4>Key Properties of Entangled States</h4>
        <ol>
            <li><strong>Non-separability</strong>: The whole system's state is global; parts lack individual pure states.</li>
            <li><strong>Non-locality</strong>: Measurement outcomes on one subsystem <strong>instantaneously affect</strong> the other (though no faster-than-light communication is possible).</li>
            <li><strong>Monogamy</strong>: If qubit A is maximally entangled with B, it <strong>cannot</strong> be entangled with C.</li>
            <li><strong>Conservation under local unitary operations</strong>: Entanglement is invariant under local operations (but can be changed by measurements or noise).</li>
            <li><strong>Fragility</strong>: Entanglement is easily destroyed by <strong>decoherence</strong> (interaction with environment).</li>
        </ol>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Bell States and Their Significance</h2>
    <div class="content">
        <h3>What are Bell States?</h3>
        <p>The <strong>Bell states</strong> (or <strong>EPR pairs</strong>) are four specific <strong>maximally entangled</strong> two-qubit states that form an orthonormal basis for the 2-qubit Hilbert space (\(\mathbb{C}^2 \otimes \mathbb{C}^2\)).</p>
        
        <h3>The Four Bell States</h3>
        <div class="equation">
            \[ \begin{aligned}
            |\Phi^+\rangle &= \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle) \\
            |\Phi^-\rangle &= \frac{1}{\sqrt{2}}(|00\rangle - |11\rangle) \\
            |\Psi^+\rangle &= \frac{1}{\sqrt{2}}(|01\rangle + |10\rangle) \\
            |\Psi^-\rangle &= \frac{1}{\sqrt{2}}(|01\rangle - |10\rangle)
            \end{aligned} \]
        </div>
        
        <h3>Significance</h3>
        <ul>
            <li><strong>Maximal entanglement</strong>: Each Bell state has <strong>1 ebit</strong> (entanglement bit) of entanglement.</li>
            <li><strong>Basis for protocols</strong>: Used in quantum teleportation, superdense coding, and entanglement swapping.</li>
            <li><strong>Test of quantum non-locality</strong>: Violate <strong>Bell inequalities</strong>, distinguishing quantum mechanics from local hidden-variable theories.</li>
            <li><strong>Resource states</strong>: Serve as fundamental resources in quantum communication and computation.</li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: \(|\Psi^-\rangle\) is also called the <strong>singlet state</strong> antisymmetric under particle exchange.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Non-locality and Quantum Correlations</h2>
    <div class="content">
        <h3>What is Non-locality?</h3>
        <p><strong>Quantum non-locality</strong> refers to correlations between distant particles that <strong>cannot be explained</strong> by any <strong>local realistic theory</strong> (i.e., theories assuming:
        <ul>
            <li><strong>Locality</strong>: No influence faster than light.</li>
            <li><strong>Realism</strong>: Physical properties exist prior to measurement.)</li>
        </ul>
        
        <h3>Bell's Theorem (1964)</h3>
        <p>John Bell showed that <strong>any local hidden-variable theory</strong> must obey certain statistical constraints (<strong>Bell inequalities</strong>).</p>
        <p>Quantum mechanics <strong>violates</strong> these inequalities.</p>
        <p>Experiments (e.g., by Aspect, 1982; loophole-free tests, 2015) confirm these violations → <strong>local realism is false</strong>.</p>
        
        <h3>Example: CHSH Inequality</h3>
        <p>For observables \(A_1, A_2\) on qubit A and \(B_1, B_2\) on qubit B:</p>
        <div class="equation">
            \[ S = \langle A_1B_1 \rangle + \langle A_1B_2 \rangle + \langle A_2B_1 \rangle - \langle A_2B_2 \rangle \]
        </div>
        <ul>
            <li><strong>Local hidden-variable theories</strong>: \(|S| \leq 2\)</li>
            <li><strong>Quantum mechanics</strong>: \(|S| \leq 2\sqrt{2} \approx 2.828\) (Tsirelson's bound)</li>
            <li>Achieved using entangled states (e.g., \(|\Phi^+\rangle\)) and specific measurement bases.</li>
        </ul>
        
        <h3>Important Clarifications</h3>
        <ul>
            <li><strong>No faster-than-light communication</strong>: Outcomes are random; only <strong>correlations</strong> are non-local. Cannot transmit information instantaneously (<strong>no-signaling theorem</strong>).</li>
            <li><strong>Correlation ≠ Causation</strong>: Measurement on A doesn't "cause" B's state it reveals pre-existing (but non-classical) correlation.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Mathematical Representation of Entanglement</h2>
    <div class="content">
        <h3>Pure Bipartite States</h3>
        <p>A state \(|\psi\rangle_{AB}\) is <strong>entangled</strong> iff its <strong>Schmidt rank > 1</strong>.</p>
        <p><strong>Schmidt decomposition</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle_{AB} = \sum_i \lambda_i |u_i\rangle_A \otimes |v_i\rangle_B \]
        </div>
        <p>where \(\lambda_i \geq 0\), \(\sum_i \lambda_i^2 = 1\).</p>
        <p><strong>Entangled</strong> ⇔ more than one non-zero \(\lambda_i\).</p>
        
        <h3>Mixed States and Entanglement Criteria</h3>
        <p>For mixed states (\(\rho_{AB}\)), entanglement is harder to detect:</p>
        <ul>
            <li><strong>Separable state</strong>: \(\rho_{AB} = \sum_k p_k \, \rho_A^{(k)} \otimes \rho_B^{(k)}\), with \(p_k \geq 0\), \(\sum_k p_k = 1\).</li>
            <li><strong>Entangled</strong> ⇔ not separable.</li>
        </ul>
        
        <h4>Entanglement Detection Tools</h4>
        <ol>
            <li><strong>Positive Partial Transpose (PPT) Criterion</strong> (Peres-Horodecki):
            <ul>
                <li>If \(\rho^{T_B}\) (partial transpose over B) has <strong>negative eigenvalues</strong> → state is entangled.</li>
                <li><strong>Necessary and sufficient</strong> for 2×2 and 2×3 systems.</li>
            </ul>
            </li>
            <li><strong>Entanglement Witnesses</strong>: Hermitian operators \(W\) such that \(\text{Tr}(W\sigma) \geq 0\) for all separable \(\sigma\), but \(\text{Tr}(W\rho) < 0\) for some entangled \(\rho\).</li>
        </ol>
        
        <h3>Quantifying Entanglement (Pure States)</h3>
        <ul>
            <li><strong>Entanglement entropy</strong>: \(S(\rho_A) = -\text{Tr}(\rho_A \log_2 \rho_A)\), where \(\rho_A = \text{Tr}_B(|\psi\rangle\langle\psi|)\).
            <ul>
                <li>\(S = 0\) → separable</li>
                <li>\(S = 1\) → maximally entangled (e.g., Bell states)</li>
            </ul>
            </li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications in Quantum Information Processing</h2>
    <div class="content">
        <h3>1. Quantum Teleportation</h3>
        <ul>
            <li><strong>Goal</strong>: Transmit an unknown quantum state using classical communication and shared entanglement.</li>
            <li><strong>Resource</strong>: One shared Bell pair (e.g., \(|\Phi^+\rangle\)) + 2 classical bits.</li>
            <li><strong>Process</strong>:
                <ol>
                    <li>Alice performs Bell measurement on her qubit (state to send) and her half of Bell pair.</li>
                    <li>Sends 2 classical bits to Bob.</li>
                    <li>Bob applies correction based on bits → recovers original state.</li>
                </ol>
            </li>
            <li><strong>Key point</strong>: No cloning; original state is destroyed.</li>
        </ul>
        
        <h3>2. Superdense Coding</h3>
        <ul>
            <li><strong>Goal</strong>: Send <strong>two classical bits</strong> by transmitting <strong>one qubit</strong>.</li>
            <li><strong>Resource</strong>: One shared Bell pair.</li>
            <li><strong>Process</strong>:
                <ul>
                    <li>Alice applies one of four unitary operations to her qubit (encoding 00, 01, 10, 11).</li>
                    <li>Sends her qubit to Bob.</li>
                    <li>Bob performs Bell measurement → decodes 2 bits.</li>
                </ul>
            </li>
            <li><strong>Demonstrates</strong>: Entanglement enhances classical communication capacity.</li>
        </ul>
        
        <h3>3. Quantum Key Distribution (QKD)</h3>
        <ul>
            <li><strong>Example</strong>: E91 protocol (Ekert, 1991)
            <ul>
                <li>Uses entangled photon pairs.</li>
                <li>Security based on <strong>violation of Bell inequalities</strong> eavesdropping disturbs entanglement and is detectable.</li>
            </ul>
            </li>
        </ul>
        
        <h3>4. Quantum Computing</h3>
        <ul>
            <li><strong>Entangling gates</strong> (e.g., CNOT) create entanglement between qubits.</li>
            <li>Essential for quantum speedup (e.g., in Shor's algorithm, Grover's search).</li>
            <li><strong>Measurement-based quantum computing</strong> (MBQC): Computation driven by measurements on a highly entangled "cluster state".</li>
        </ul>
        
        <h3>5. Quantum Metrology & Sensing</h3>
        <p>Entangled states (e.g., NOON states) enable <strong>Heisenberg-limited precision</strong>, surpassing classical limits in interferometry and imaging.</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Takeaway</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Entanglement</td>
                        <td>Non-separable quantum correlations with no classical analog.</td>
                    </tr>
                    <tr>
                        <td>Bell States</td>
                        <td>Maximally entangled 2-qubit states; foundational for protocols.</td>
                    </tr>
                    <tr>
                        <td>Non-locality</td>
                        <td>Proven via Bell inequality violations; rules out local hidden variables.</td>
                    </tr>
                    <tr>
                        <td>Math Representation</td>
                        <td>Schmidt decomposition (pure), PPT criterion (mixed), entanglement entropy.</td>
                    </tr>
                    <tr>
                        <td>Applications</td>
                        <td>Teleportation, superdense coding, QKD, quantum computing, enhanced sensing.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Remember</strong>: Entanglement is a <strong>resource</strong>, not just a curiosity it enables tasks impossible classically.
        </div>
    </div>
</section>

        <section class="section">
    <h2 class="section-title">Quantum Gates and Circuits</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing</em></p>
        
        <h3>Single-Qubit Gates</h3>
        <p>Single-qubit gates are <strong>unitary operators</strong> acting on a single qubit. They correspond to rotations on the <strong>Bloch sphere</strong>.</p>
        
        <h4>Pauli Gates</h4>
        <p>These are fundamental and form the Pauli group.</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Gate</th>
                        <th>Matrix</th>
                        <th>Action on Basis States</th>
                        <th>Bloch Sphere Rotation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>X (NOT)</strong></td>
                        <td>\(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\)</td>
                        <td>\(X|0\rangle = |1\rangle\), \(X|1\rangle = |0\rangle\)</td>
                        <td>180° about <strong>x-axis</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Y</strong></td>
                        <td>\(\begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix}\)</td>
                        <td>\(Y|0\rangle = i|1\rangle\), \(Y|1\rangle = -i|0\rangle\)</td>
                        <td>180° about <strong>y-axis</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Z (Phase-flip)</strong></td>
                        <td>\(\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}\)</td>
                        <td>\(Z|0\rangle = |0\rangle\), \(Z|1\rangle = -|1\rangle\)</td>
                        <td>180° about <strong>z-axis</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Note</strong>: \(X, Y, Z\) are <strong>Hermitian</strong> and <strong>unitary</strong>: \(X^\dagger = X\), \(X^2 = I\), etc.
        </div>
        
        <h4>Hadamard Gate (H)</h4>
        <p>Creates superposition from computational basis:</p>
        <div class="equation">
            \[ H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \]
        </div>
        <p>Action:</p>
        <div class="equation">
            \[ H|0\rangle = |+\rangle = \frac{|0\rangle + |1\rangle}{\sqrt{2}}, \quad
            H|1\rangle = |-\rangle = \frac{|0\rangle - |1\rangle}{\sqrt{2}} \]
        </div>
        <ul>
            <li>Also maps \(|+\rangle \to |0\rangle\), \(|-\rangle \to |1\rangle\) → <strong>basis change</strong> between Z and X.</li>
            <li>Bloch sphere: Rotation by 180° about axis \((\hat{x} + \hat{z})/\sqrt{2}\).</li>
        </ul>
        
        <h4>Phase Gate (S) and T Gate</h4>
        <p><strong>Phase (S) gate</strong>:</p>
        <div class="equation">
            \[ S = \begin{pmatrix} 1 & 0 \\ 0 & i \end{pmatrix}, \quad S^2 = Z \]
        </div>
        <p>Adds a <strong>+90° phase</strong> to |1⟩: \(S|1\rangle = i|1\rangle\)</p>
        
        <p><strong>T gate (π/8 gate)</strong>:</p>
        <div class="equation">
            \[ T = \begin{pmatrix} 1 & 0 \\ 0 & e^{i\pi/4} \end{pmatrix}, \quad T^2 = S \]
        </div>
        <p>Adds <strong>+45° phase</strong> to |1⟩.</p>
        
        <div class="definition">
            These are <strong>non-Clifford</strong> gates (T is especially important for universality).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Multi-Qubit Gates</h2>
    <div class="content">
        <p>Multi-qubit gates enable <strong>entanglement</strong> and <strong>conditional operations</strong>.</p>
        
        <h3>CNOT (Controlled-NOT) Gate</h3>
        <ul>
            <li><strong>2-qubit gate</strong>: flips target qubit if control is |1⟩.</li>
            <li>Matrix (in basis |00⟩, |01⟩, |10⟩, |11⟩):</li>
        </ul>
        <div class="equation">
            \[ \text{CNOT} = 
            \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 1 & 0
            \end{pmatrix} \]
        </div>
        <ul>
            <li>Action:</li>
        </ul>
        <div class="equation">
            \[ \text{CNOT}|a,b\rangle = |a, a \oplus b\rangle \]
        </div>
        <ul>
            <li><strong>Creates entanglement</strong>:<br>
            \( \text{CNOT}(H \otimes I)|00\rangle = \text{CNOT}|+\!0\rangle = \frac{|00\rangle + |11\rangle}{\sqrt{2}} = |\Phi^+\rangle \)</li>
        </ul>
        
        <h3>Toffoli Gate (CCNOT)</h3>
        <ul>
            <li><strong>3-qubit gate</strong>: flips target if <strong>both controls</strong> are |1⟩.</li>
            <li>Universal for <strong>classical reversible computing</strong>.</li>
            <li>Can implement AND, OR, etc.</li>
            <li>Quantum universality: With single-qubit gates, Toffoli enables universal quantum computation.</li>
            <li>Not natively available on all hardware; often <strong>decomposed</strong> into CNOTs and T gates.</li>
        </ul>
        
        <h3>SWAP Gate</h3>
        <p>Exchanges states of two qubits:</p>
        <div class="equation">
            \[ \text{SWAP}|a,b\rangle = |b,a\rangle \]
        </div>
        <p>Matrix:</p>
        <div class="equation">
            \[ \text{SWAP} =
            \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1
            \end{pmatrix} \]
        </div>
        <p>Can be built from <strong>3 CNOTs</strong>:</p>
        <div class="equation">
            \[ \text{SWAP} = \text{CNOT}_{12} \cdot \text{CNOT}_{21} \cdot \text{CNOT}_{12} \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Universal Gate Sets</h2>
    <div class="content">
        <p>A set of quantum gates is <strong>universal</strong> if it can approximate <strong>any unitary operation</strong> on \(n\) qubits to arbitrary precision.</p>
        
        <h3>Common Universal Sets</h3>
        <ol>
            <li><strong>{CNOT, H, T}</strong>
            <ul>
                <li>Most widely used in fault-tolerant quantum computing.</li>
                <li>T is non-Clifford; enables approximation of any rotation via <strong>Solovay-Kitaev theorem</strong>.</li>
            </ul>
            </li>
            <li><strong>{CNOT, single-qubit rotations}</strong>
            <ul>
                <li>Any single-qubit unitary can be decomposed into rotations: \(R_x(\theta), R_y(\phi), R_z(\lambda)\).</li>
            </ul>
            </li>
            <li><strong>{Toffoli, Hadamard}</strong>
            <ul>
                <li>Sufficient for universal quantum computation (though less efficient).</li>
            </ul>
            </li>
        </ol>
        
        <div class="definition">
            <strong>Key Insight</strong>: You need <strong>at least one non-Clifford gate</strong> (like T) to achieve universality. The Clifford group (generated by H, S, CNOT) alone is <strong>not universal</strong> (Gottesman-Knill theorem).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Circuit Diagrams and Notation</h2>
    <div class="content">
        <p>Quantum circuits provide a <strong>visual representation</strong> of quantum algorithms.</p>
        
        <h3>Basic Conventions</h3>
        <ul>
            <li><strong>Horizontal lines</strong>: Qubits (top to bottom = qubit 0 to qubit \(n-1\)).</li>
            <li><strong>Time flows left to right</strong>.</li>
            <li><strong>Gates</strong> are boxes or symbols applied to qubit lines.</li>
            <li><strong>Multi-qubit gates</strong> connect control and target with dots and ⊕.</li>
        </ul>
        
        <h3>Examples</h3>
        <ul>
            <li><strong>Hadamard on qubit 0</strong>:
            <pre>q0: ──H──</pre>
            </li>
            <li><strong>CNOT (q0 control, q1 target)</strong>:
            <pre>q0: ──●──
      │
q1: ──⊕──</pre>
            </li>
            <li><strong>Toffoli (q0,q1 control; q2 target)</strong>:
            <pre>q0: ──●──
      │
q1: ──●──
      │
q2: ──⊕──</pre>
            </li>
            <li><strong>Measurement</strong>:
            <pre>q0: ──M──</pre>
            </li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: Classical bits (from measurement) are often shown as double lines.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Circuit Depth and Complexity</h2>
    <div class="content">
        <h3>Circuit Depth</h3>
        <ul>
            <li><strong>Definition</strong>: The <strong>longest path</strong> (in time steps) from input to output, considering gates that can be applied <strong>in parallel</strong>.</li>
            <li><strong>Why it matters</strong>: Depth correlates with <strong>execution time</strong> and <strong>susceptibility to noise</strong> (decoherence).</li>
            <li>Example:
            <pre>q0: ──H──●──────
         │
q1: ─────⊕──H──</pre>
            - Depth = 2 (H and CNOT cannot be parallelized; second H on q1 comes after CNOT).</li>
        </ul>
        
        <h3>Circuit Size</h3>
        <p>Total number of gates (often counts CNOTs separately due to higher error rates).</p>
        
        <h3>Complexity Classes</h3>
        <ul>
            <li><strong>BQP (Bounded-error Quantum Polynomial time)</strong>: Problems solvable by quantum circuits of <strong>polynomial size and depth</strong> with bounded error.</li>
            <li><strong>Quantum advantage</strong> often hinges on <strong>exponentially smaller depth/size</strong> vs. classical counterparts (e.g., Shor's algorithm).</li>
        </ul>
        
        <h3>Optimization Goals</h3>
        <ul>
            <li>Minimize <strong>CNOT count</strong> (most error-prone gate on NISQ devices).</li>
            <li>Reduce <strong>depth</strong> to stay within coherence time.</li>
            <li>Use <strong>gate decomposition</strong> to map to hardware-native gates.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Key Quantum Gates</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Gate</th>
                        <th>Type</th>
                        <th>Matrix / Action</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>X, Y, Z</strong></td>
                        <td>Single-qubit</td>
                        <td>Pauli matrices</td>
                        <td>Bit/phase flips, rotations</td>
                    </tr>
                    <tr>
                        <td><strong>H</strong></td>
                        <td>Single-qubit</td>
                        <td>\(\frac{1}{\sqrt{2}}\begin{bmatrix}1&1\\1&-1\end{bmatrix}\)</td>
                        <td>Superposition, basis change</td>
                    </tr>
                    <tr>
                        <td><strong>S, T</strong></td>
                        <td>Single-qubit</td>
                        <td>Phase rotations</td>
                        <td>Add complex phases; T enables universality</td>
                    </tr>
                    <tr>
                        <td><strong>CNOT</strong></td>
                        <td>2-qubit</td>
                        <td>\(|a,b\rangle \to |a, a\oplus b\rangle\)</td>
                        <td>Entanglement, conditional logic</td>
                    </tr>
                    <tr>
                        <td><strong>Toffoli</strong></td>
                        <td>3-qubit</td>
                        <td>CCNOT</td>
                        <td>Classical logic, universality</td>
                    </tr>
                    <tr>
                        <td><strong>SWAP</strong></td>
                        <td>2-qubit</td>
                        <td>\(|a,b\rangle \to |b,a\rangle\)</td>
                        <td>Qubit exchange</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li>Single-qubit gates manipulate state on the Bloch sphere.</li>
            <li>Multi-qubit gates (especially CNOT) <strong>generate entanglement</strong>.</li>
            <li><strong>{H, T, CNOT}</strong> is a standard universal gate set.</li>
            <li>Circuit <strong>depth</strong> is critical for near-term hardware.</li>
            <li>Quantum circuits are the <strong>"assembly language"</strong> of quantum algorithms.</li>
        </ul>
    </div>
</section>

       <section class="section">
    <h2 class="section-title">Quantum Algorithms and Complexity</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing</em></p>
        
        <h3>Grover's Search Algorithm</h3>
        <h4>Problem Statement</h4>
        <p>Given an unstructured database of \(N = 2^n\) items and a <strong>black-box oracle</strong> \(f(x)\) such that:</p>
        <div class="equation">
            \[ f(x) = 
            \begin{cases}
            1 & \text{if } x = x_0 \text{ (target item)} \\
            0 & \text{otherwise}
            \end{cases} \]
        </div>
        <p>Goal: Find \(x_0\) with as few queries to \(f\) as possible.</p>
        
        <h4>Classical vs. Quantum</h4>
        <ul>
            <li><strong>Classical</strong>: Requires \(O(N)\) queries (on average \(N/2\)).</li>
            <li><strong>Grover's algorithm</strong>: Requires only \(O(\sqrt{N})\) queries → <strong>quadratic speedup</strong>.</li>
        </ul>
        
        <h4>Key Principles</h4>
        <ol>
            <li><strong>Amplitude Amplification</strong>:
            <ul>
                <li>Start with uniform superposition:<br>
                \[ |\psi\rangle = H^{\otimes n}|0\rangle^{\otimes n} = \frac{1}{\sqrt{N}} \sum_{x=0}^{N-1} |x\rangle \]
                </li>
                <li>Repeatedly apply the <strong>Grover iteration</strong> \(G = (2|\psi\rangle\langle\psi| - I) \cdot O_f\), where:
                <ul>
                    <li>\(O_f\) = oracle (flips sign of target: \(|x_0\rangle \to -|x_0\rangle\))</li>
                    <li>\(2|\psi\rangle\langle\psi| - I\) = <strong>diffusion operator</strong> (inverts amplitudes about mean)</li>
                </ul>
                </li>
            </ul>
            </li>
            <li><strong>Geometric Interpretation</strong>:
            <ul>
                <li>State vector rotates in 2D plane spanned by \(|x_0\rangle\) and \(|\psi_{\perp}\rangle\) (uniform superposition of non-targets).</li>
                <li>Each Grover iteration rotates by angle \(2\theta\), where \(\sin\theta = 1/\sqrt{N}\).</li>
                <li>Optimal number of iterations: \(R \approx \frac{\pi}{4} \sqrt{N}\)</li>
            </ul>
            </li>
        </ol>
        
        <h4>Applications</h4>
        <ul>
            <li><strong>Unstructured search</strong>: Database search, collision detection.</li>
            <li><strong>Optimization</strong>: Speeds up brute-force search in NP problems (e.g., SAT solvers).</li>
            <li><strong>Amplitude estimation</strong>: Core subroutine in quantum Monte Carlo methods.</li>
            <li><strong>Cryptanalysis</strong>: Reduces effective key length (e.g., AES-128 → ~64-bit security).</li>
        </ul>
        
        <div class="definition">
            <strong>Limitation</strong>: Only provides <strong>quadratic speedup</strong> not exponential.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Fourier Transform (QFT)</h2>
    <div class="content">
        <h3>Definition</h3>
        <p>The <strong>QFT</strong> is the quantum analog of the classical Discrete Fourier Transform (DFT).</p>
        <p>Maps computational basis state \(|j\rangle\) to superposition:</p>
        <div class="equation">
            \[ \text{QFT}|j\rangle = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} e^{2\pi i j k / N} |k\rangle, \quad N = 2^n \]
        </div>
        
        <h3>Circuit Implementation</h3>
        <ul>
            <li>Efficiently implemented using <strong>Hadamard</strong> and <strong>controlled-phase</strong> gates.</li>
            <li><strong>Circuit depth</strong>: \(O(n^2)\) (can be reduced to \(O(n \log n)\) with approximations).</li>
            <li><strong>Structure</strong>:
            <ul>
                <li>Apply Hadamard to qubit 0.</li>
                <li>Apply controlled-\(R_k\) gates (phase rotations) from higher qubits.</li>
                <li>Repeat for each qubit.</li>
                <li>Reverse qubit order at the end (swap gates).</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Properties</h3>
        <ul>
            <li><strong>Unitary</strong>: QFT\(^{-1}\) = QFT\(^{\dagger}\)</li>
            <li><strong>Exponentially faster</strong> than classical FFT for preparing Fourier state (but <strong>output is not directly readable</strong> due to measurement collapse).</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Core subroutine</strong> in:
            <ul>
                <li><strong>Shor's algorithm</strong> (period finding)</li>
                <li><strong>Phase estimation</strong></li>
                <li><strong>Hidden subgroup problems</strong></li>
                <li><strong>Quantum simulations</strong> (e.g., solving linear systems via HHL algorithm)</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: QFT does <strong>not</strong> provide exponential speedup for classical FFT tasks because extracting all Fourier coefficients requires exponential measurements.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Shor's Algorithm for Integer Factorization</h2>
    <div class="content">
        <h3>Problem</h3>
        <p>Given composite integer \(N\), find non-trivial factors.</p>
        <ul>
            <li><strong>Classically</strong>: Best known algorithm (GNFS) runs in <strong>sub-exponential</strong> time → infeasible for large \(N\) (basis of RSA cryptography).</li>
            <li><strong>Shor's algorithm</strong>: Solves in <strong>polynomial time</strong> → breaks RSA.</li>
        </ul>
        
        <h3>Algorithm Overview</h3>
        <ol>
            <li><strong>Classical reduction</strong>:
            <ul>
                <li>Pick random \(a < N\), compute \(\gcd(a, N)\). If >1, done.</li>
                <li>Else, find <strong>order \(r\)</strong> of \(a \mod N\): smallest \(r\) such that \(a^r \equiv 1 \mod N\).</li>
            </ul>
            </li>
            <li><strong>Quantum subroutine: Period Finding</strong>
            <ul>
                <li>Use <strong>quantum phase estimation</strong> or <strong>QFT-based period finding</strong>:
                <ul>
                    <li>Prepare state: \(\frac{1}{\sqrt{Q}} \sum_{x=0}^{Q-1} |x\rangle |a^x \mod N\rangle\)</li>
                    <li>Measure second register → collapses first to periodic superposition with period \(r\).</li>
                    <li>Apply <strong>QFT</strong> to first register → peaks at multiples of \(Q/r\).</li>
                    <li>Classical post-processing (continued fractions) extracts \(r\).</li>
                </ul>
                </li>
            </ul>
            </li>
            <li><strong>Classical post-processing</strong>:
            <ul>
                <li>If \(r\) even and \(a^{r/2} \not\equiv -1 \mod N\), then<br>
                \(\gcd(a^{r/2} \pm 1, N)\) yields non-trivial factors.</li>
            </ul>
            </li>
        </ol>
        
        <h3>Complexity</h3>
        <ul>
            <li><strong>Time</strong>: \(O((\log N)^3)\) → <strong>exponential speedup</strong> over classical.</li>
            <li><strong>Qubits</strong>: \(O(\log N)\)</li>
        </ul>
        
        <h3>Significance</h3>
        <ul>
            <li>First <strong>exponentially faster</strong> quantum algorithm for a practical problem.</li>
            <li>Motivated global investment in <strong>post-quantum cryptography</strong>.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Walks</h2>
    <div class="content">
        <h3>What is a Quantum Walk?</h3>
        <p>Quantum analog of classical random walks.</p>
        <p>Two main types:</p>
        <ol>
            <li><strong>Discrete-time quantum walk (DTQW)</strong></li>
            <li><strong>Continuous-time quantum walk (CTQW)</strong></li>
        </ol>
        
        <h3>Discrete-Time Quantum Walk (DTQW)</h3>
        <ul>
            <li>Requires <strong>coin</strong> and <strong>position</strong> registers.</li>
            <li>Evolution:<br>
            \(|\psi(t+1)\rangle = S \cdot (C \otimes I) |\psi(t)\rangle\)
            <ul>
                <li>\(C\): <strong>Coin operator</strong> (e.g., Hadamard) → creates superposition of directions.</li>
                <li>\(S\): <strong>Shift operator</strong> → moves particle based on coin state.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Properties</h3>
        <ul>
            <li><strong>Quadratic speedup</strong> in hitting time vs. classical walks (e.g., on a line: spreads as \(t\) vs. \(\sqrt{t}\)).</li>
            <li><strong>Interference and entanglement</strong> between coin and position.</li>
            <li><strong>Universal for quantum computation</strong>: Any quantum circuit can be simulated by a quantum walk.</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Search algorithms</strong>: Spatial search on graphs (e.g., finding marked node on hypercube).</li>
            <li><strong>Graph isomorphism</strong>, <strong>element distinctness</strong>.</li>
            <li><strong>Quantum simulation</strong> of physical processes (e.g., energy transport).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Complexity Classes</h2>
    <div class="content">
        <h3>BQP (Bounded-error Quantum Polynomial Time)</h3>
        <ul>
            <li><strong>Definition</strong>: Class of decision problems solvable by a quantum computer in <strong>polynomial time</strong>, with error probability ≤ 1/3.</li>
            <li><strong>Formally</strong>:<br>
            \(L \in \text{BQP}\) if ∃ uniform family of quantum circuits \(\{C_n\}\) such that:
            <ul>
                <li>Size of \(C_n\) = poly(n)</li>
                <li>For \(x \in L\): \(\Pr[C_n(x) = 1] \geq 2/3\)</li>
                <li>For \(x \notin L\): \(\Pr[C_n(x) = 0] \geq 2/3\)</li>
            </ul>
            </li>
            <li><strong>Contains</strong>: P, BPP</li>
            <li><strong>Contained in</strong>: PSPACE</li>
            <li><strong>Believed not to contain</strong>: NP-complete problems (but not proven)</li>
            <li><strong>Key problems in BQP</strong>:
            <ul>
                <li>Factoring (Shor)</li>
                <li>Discrete log</li>
                <li>Simulation of quantum systems</li>
            </ul>
            </li>
        </ul>
        
        <h3>QMA (Quantum Merlin-Arthur)</h3>
        <ul>
            <li><strong>Quantum analog of NP</strong>.</li>
            <li><strong>Definition</strong>: A problem is in QMA if, for every "yes" instance, there exists a <strong>polynomial-size quantum witness</strong> \(|\psi\rangle\) that a <strong>polynomial-time quantum verifier</strong> accepts with high probability; for "no" instances, no such witness exists.</li>
            <li><strong>Formally</strong>:<br>
            \(L \in \text{QMA}\) if ∃ quantum verifier \(V\) (poly-time) such that:
            <ul>
                <li>If \(x \in L\): ∃ \(|\psi\rangle\) with \(\Pr[V(x, |\psi\rangle) = 1] \geq 2/3\)</li>
                <li>If \(x \notin L\): ∀ \(|\psi\rangle\), \(\Pr[V(x, |\psi\rangle) = 1] \leq 1/3\)</li>
            </ul>
            </li>
            <li><strong>Complete problem</strong>: <strong>k-local Hamiltonian problem</strong> (quantum analog of SAT).</li>
            <li><strong>Contains</strong>: BQP, NP</li>
            <li><strong>Contained in</strong>: PP</li>
        </ul>
        
        <div class="definition">
            <strong>Analogy</strong>:<br>
            - <strong>NP</strong>: Merlin sends classical proof → Arthur verifies classically.<br>
            - <strong>QMA</strong>: Merlin sends <strong>quantum proof</strong> → Arthur verifies <strong>quantumly</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm/Concept</th>
                        <th>Speedup</th>
                        <th>Key Idea</th>
                        <th>Application</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Grover's Search</strong></td>
                        <td>Quadratic (\(O(\sqrt{N})\))</td>
                        <td>Amplitude amplification</td>
                        <td>Unstructured search, optimization</td>
                    </tr>
                    <tr>
                        <td><strong>QFT</strong></td>
                        <td>Exponential state prep</td>
                        <td>Quantum Fourier basis</td>
                        <td>Shor's, phase estimation</td>
                    </tr>
                    <tr>
                        <td><strong>Shor's Algorithm</strong></td>
                        <td>Exponential</td>
                        <td>Period finding via QFT</td>
                        <td>Integer factorization, RSA break</td>
                    </tr>
                    <tr>
                        <td><strong>Quantum Walks</strong></td>
                        <td>Quadratic (hitting time)</td>
                        <td>Coherent walk with interference</td>
                        <td>Graph search, simulation</td>
                    </tr>
                    <tr>
                        <td><strong>BQP</strong></td>
                        <td> </td>
                        <td>Efficient quantum decision problems</td>
                        <td>Captures power of quantum computers</td>
                    </tr>
                    <tr>
                        <td><strong>QMA</strong></td>
                        <td> </td>
                        <td>Quantum proofs + verification</td>
                        <td>Quantum satisfiability, physics</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Grover</strong>: Quadratic speedup for search broadly applicable but not exponential.</li>
            <li><strong>Shor + QFT</strong>: Exponential speedup for structured problems (factoring, period finding).</li>
            <li><strong>Quantum walks</strong>: Offer alternative algorithmic framework with speedups on graphs.</li>
            <li><strong>BQP ≠ NP</strong>: Quantum computers likely <strong>cannot solve NP-complete problems efficiently</strong>.</li>
            <li><strong>QMA</strong>: Natural class for quantum many-body problems and verification.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum-Inspired Algorithms</h2>
    <div class="content">
        <p><em>Classical Simulation of Quantum Phenomena</em></p>
        
        <h3>Simulating Quantum Phenomena: Overview</h3>
        <h4>What Are Quantum-Inspired Algorithms?</h4>
        <p><strong>Definition</strong>: Classical algorithms that borrow ideas from quantum mechanics (e.g., superposition, interference, entanglement) to solve problems more efficiently <strong>without requiring a quantum computer</strong>.</p>
        <p><strong>Goal</strong>: Leverage quantum-like structures to gain speedups or new insights on classical hardware.</p>
        <p><strong>Important distinction</strong>:</p>
        <ul>
            <li><strong>Quantum simulation</strong>: Simulating a quantum system <em>on a classical computer</em>.</li>
            <li><strong>Quantum-inspired</strong>: Using quantum concepts to design <em>better classical algorithms</em>.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Note</strong>: True quantum advantage (e.g., exponential speedup) generally <strong>cannot</strong> be replicated classically for arbitrary quantum systems due to the exponential cost of representing quantum states.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Classical Simulation of Superposition</h2>
    <div class="content">
        <h3>How Superposition Is Represented Classically</h3>
        <p>A single qubit in superposition:</p>
        <div class="equation">
            \[ |\psi\rangle = \alpha|0\rangle + \beta|1\rangle, \quad |\alpha|^2 + |\beta|^2 = 1 \]
        </div>
        <p><strong>Classical representation</strong>: Store complex amplitudes \(\alpha, \beta\) as floating-point numbers → <strong>2 complex numbers</strong> per qubit.</p>
        
        <h3>Scaling Challenge</h3>
        <ul>
            <li>For \(n\) qubits, the state vector has \(2^n\) complex amplitudes.</li>
            <li><strong>Memory requirement</strong>: \(O(2^n)\) → becomes infeasible beyond ~45–50 qubits on supercomputers.</li>
        </ul>
        
        <h3>Simulation Techniques</h3>
        <ol>
            <li><strong>State-vector simulation</strong>:
            <ul>
                <li>Exact simulation using full \(2^n\)-dimensional vector.</li>
                <li>Used in simulators like Qiskit Aer, QuTiP.</li>
            </ul>
            </li>
            <li><strong>Sparse state representation</strong>:
            <ul>
                <li>If the state has few non-zero amplitudes (e.g., after few gates), store only non-zero entries.</li>
            </ul>
            </li>
            <li><strong>Tensor network methods</strong> (see Section 5): Avoid full state vector when entanglement is limited.</li>
        </ol>
        
        <div class="definition">
            ✅ <strong>Feasible for</strong>: Small circuits, educational purposes, verification of quantum hardware.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Simulating Entanglement on Classical Hardware</h2>
    <div class="content">
        <h3>The Core Challenge</h3>
        <ul>
            <li>Entanglement implies <strong>non-separability</strong>: The full \(2^n\)-dimensional state cannot be factored.</li>
            <li>Classical simulation must track <strong>global correlations</strong>, which grow exponentially with qubit count.</li>
        </ul>
        
        <h3>Approaches to Simulate Entanglement</h3>
        <h4>(a) Full State-Vector Simulation</h4>
        <ul>
            <li>Stores entire wavefunction → captures all entanglement exactly.</li>
            <li><strong>Cost</strong>: \(O(2^n)\) memory and \(O(2^n)\) time per gate (for dense gates like CNOT).</li>
        </ul>
        
        <h4>(b) Stabilizer Formalism (Gottesman-Knill Theorem)</h4>
        <ul>
            <li>Efficiently simulates circuits using only <strong>Clifford gates</strong> (H, S, CNOT).</li>
            <li>Represents state via <strong>stabilizer generators</strong> (not amplitudes).</li>
            <li><strong>Complexity</strong>: \(O(n^2)\) per gate for \(n\) qubits.</li>
            <li><strong>Limitation</strong>: Cannot simulate non-Clifford gates (e.g., T gate) efficiently → not universal.</li>
        </ul>
        
        <h4>(c) Matrix Product States (MPS) / Tensor Networks</h4>
        <p>Represents state as a chain of tensors:</p>
        <div class="equation">
            \[ |\psi\rangle = \sum_{i_1,\dots,i_n} A^{[1]}_{i_1} A^{[2]}_{i_2} \cdots A^{[n]}_{i_n} |i_1 i_2 \dots i_n\rangle \]
        </div>
        <ul>
            <li><strong>Efficient when entanglement is low</strong> (e.g., 1D systems with area law).</li>
            <li><strong>Bond dimension \(\chi\)</strong> controls accuracy and cost: memory = \(O(n \chi^2)\).</li>
            <li>Used in DMRG (Density Matrix Renormalization Group) for condensed matter physics.</li>
        </ul>
        
        <div class="definition">
            🔑 <strong>Key Insight</strong>: Entanglement entropy limits classical simulability. High entanglement → classical simulation becomes intractable.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Interference Simulation Techniques</h2>
    <div class="content">
        <h3>What Is Quantum Interference?</h3>
        <p>Amplitudes (complex numbers) can <strong>constructively or destructively interfere</strong>.</p>
        <p>Critical for quantum speedups (e.g., in Grover's or Shor's algorithms).</p>
        
        <h3>Classical Simulation Methods</h3>
        <ol>
            <li><strong>Path Integral / Sum-over-Paths</strong>:
            <ul>
                <li>Compute final amplitude as sum over all computational paths.</li>
                <li>Each path contributes a complex phase.</li>
                <li><strong>Problem</strong>: \(2^{\text{#gates}}\) paths → exponential in circuit depth.</li>
            </ul>
            </li>
            <li><strong>Feynman's Path Simulator</strong>:
            <ul>
                <li>For a circuit with \(m\) gates, simulate by summing over all possible intermediate measurement outcomes.</li>
                <li>Time complexity: \(O(4^m)\) → only feasible for shallow circuits.</li>
            </ul>
            </li>
            <li><strong>Monte Carlo with Sign Problem</strong>:
            <ul>
                <li>Sample paths probabilistically.</li>
                <li><strong>Obstacle</strong>: Negative/complex weights → <strong>sign problem</strong> → high variance, inefficient.</li>
            </ul>
            </li>
            <li><strong>Stabilizer Rank Decomposition</strong> (for non-Clifford circuits):
            <ul>
                <li>Approximate a state as sum of \(R\) stabilizer states:<br>
                \[ |\psi\rangle \approx \sum_{k=1}^R c_k |\phi_k\rangle \]
                </li>
                <li>Simulate each \(|\phi_k\rangle\) efficiently (via Gottesman-Knill), then combine.</li>
                <li>Cost scales with <strong>stabilizer rank \(R\)</strong> (e.g., \(R = 2^t\) for \(t\) T-gates).</li>
            </ul>
            </li>
        </ol>
        
        <div class="definition">
            🌊 <strong>Interference is hard to simulate classically</strong> because it requires tracking <strong>global phase relationships</strong> across exponentially many states.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Approximation Methods for Quantum States</h2>
    <div class="content">
        <p>Exact simulation is often impossible → use <strong>approximations</strong>.</p>
        
        <h3>(a) Tensor Networks</h3>
        <ul>
            <li><strong>MPS (1D)</strong>, <strong>PEPS (2D)</strong>, <strong>MERA</strong>: Represent states with limited entanglement.</li>
            <li><strong>Accuracy controlled by bond dimension</strong>.</li>
            <li>Widely used in quantum chemistry and condensed matter.</li>
        </ul>
        
        <h3>(b) Variational Methods</h3>
        <ul>
            <li><strong>Quantum-inspired classical ansatz</strong>: Use parameterized classical models (e.g., neural networks) to approximate quantum states.
            <ul>
                <li>Example: <strong>Restricted Boltzmann Machines (RBMs)</strong> to represent wavefunctions.</li>
            </ul>
            </li>
            <li>Train via energy minimization (e.g., for ground states).</li>
        </ul>
        
        <h3>(c) Low-Rank Approximations</h3>
        <ul>
            <li>Assume density matrix \(\rho\) has low rank → store via SVD.</li>
            <li>Useful for <strong>mixed states</strong> with limited entanglement.</li>
        </ul>
        
        <h3>(d) Clifford+T Approximation</h3>
        <ul>
            <li>Decompose arbitrary gates into Clifford + T.</li>
            <li>Simulate using <strong>stabilizer decomposition</strong> (as above).</li>
            <li>Error controlled by number of T gates.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Computational Trade-offs in Simulation</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Memory</th>
                        <th>Time</th>
                        <th>Accuracy</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>State-vector</strong></td>
                        <td>\(O(2^n)\)</td>
                        <td>\(O(2^n)\)/gate</td>
                        <td>Exact</td>
                        <td>Small \(n\) (< 45 qubits)</td>
                    </tr>
                    <tr>
                        <td><strong>Stabilizer (Clifford)</strong></td>
                        <td>\(O(n^2)\)</td>
                        <td>\(O(n^2)\)/gate</td>
                        <td>Exact (Clifford only)</td>
                        <td>Clifford circuits</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Networks (MPS)</strong></td>
                        <td>\(O(n \chi^2)\)</td>
                        <td>\(O(n \chi^3)\)</td>
                        <td>Approximate</td>
                        <td>Low-entanglement 1D systems</td>
                    </tr>
                    <tr>
                        <td><strong>Path Integral</strong></td>
                        <td>\(O(1)\)</td>
                        <td>\(O(4^m)\)</td>
                        <td>Exact</td>
                        <td>Shallow circuits (small \(m\))</td>
                    </tr>
                    <tr>
                        <td><strong>Stabilizer Rank</strong></td>
                        <td>\(O(R n^2)\)</td>
                        <td>\(O(R n^2)\)</td>
                        <td>Approximate</td>
                        <td>Circuits with few T gates</td>
                    </tr>
                    <tr>
                        <td><strong>Neural Quantum States</strong></td>
                        <td>\(O(\text{params})\)</td>
                        <td>Training cost</td>
                        <td>Approximate</td>
                        <td>Ground states, optimization</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Trade-offs</h3>
        <ul>
            <li><strong>Accuracy vs. Scalability</strong>: Exact methods don't scale; approximations introduce error.</li>
            <li><strong>Entanglement vs. Efficiency</strong>: High entanglement → tensor networks fail; state-vector needed.</li>
            <li><strong>Circuit Depth vs. Path Methods</strong>: Deep circuits → path integral infeasible.</li>
            <li><strong>Hardware Limits</strong>: Even with compression, simulating 50+ qubits with high entanglement requires exascale computing.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Rule of Thumb</strong>:<br>
            If a quantum circuit can be simulated efficiently classically, it likely <strong>does not provide quantum advantage</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <ul>
            <li><strong>Superposition</strong> and <strong>interference</strong> require tracking complex amplitudes → exponential classical cost.</li>
            <li><strong>Entanglement</strong> is the main barrier to efficient classical simulation.</li>
            <li><strong>Special cases</strong> (Clifford circuits, low entanglement) can be simulated efficiently.</li>
            <li><strong>Approximation methods</strong> (tensor networks, stabilizer decompositions) enable simulation of larger systems at the cost of accuracy.</li>
            <li><strong>Quantum-inspired algorithms</strong> may offer practical speedups for specific problems (e.g., linear algebra, optimization), but <strong>do not replicate exponential quantum advantage</strong>.</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Reality Check</strong>: Classical simulation is crucial for <strong>verifying quantum hardware</strong>, <strong>developing algorithms</strong>, and <strong>exploring quantum-classical boundaries</strong> but it cannot replace scalable, fault-tolerant quantum computers for truly hard problems.
        </div>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Tensor Networks</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Simulation and High-Dimensional Data Representation</em></p>
        
        <h3>Tensor Algebra Fundamentals</h3>
        <h4>What is a Tensor?</h4>
        <p>A <strong>tensor</strong> is a multi-dimensional array of numbers that generalizes scalars (0D), vectors (1D), and matrices (2D).</p>
        <ul>
            <li><strong>Order (or rank)</strong>: Number of indices (e.g., a 3D tensor \(T_{ijk}\) has order 3).</li>
            <li><strong>Dimensions (modes)</strong>: Size along each index (e.g., \(T \in \mathbb{C}^{d_1 \times d_2 \times \cdots \times d_n}\)).</li>
        </ul>
        
        <h4>Key Operations</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Operation</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Tensor product</strong> (\(\otimes\))</td>
                        <td>Combines tensors: \((A \otimes B)_{i,j,k,l} = A_{i,j} B_{k,l}\)</td>
                    </tr>
                    <tr>
                        <td><strong>Contraction</strong></td>
                        <td>Sum over shared indices (generalizes matrix multiplication).<br>Example: \(C_{ik} = \sum_j A_{ij} B_{jk}\)</td>
                    </tr>
                    <tr>
                        <td><strong>Reshaping</strong></td>
                        <td>Reinterpret tensor as different shape (e.g., vector ↔ matrix) without changing data.</td>
                    </tr>
                    <tr>
                        <td><strong>Trace</strong></td>
                        <td>Contraction of a tensor with itself over one or more pairs of indices.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h4>Einstein Notation (Implicit Summation)</h4>
        <p>Repeated indices are summed over:<br>
        \(C_{ik} = A_{ij} B_{jk}\) means \(\sum_j A_{ij} B_{jk}\).</p>
        <p><strong>Crucial for tensor network diagrams</strong>.</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Tensor Decomposition Methods</h2>
    <div class="content">
        <p>Decomposing high-order tensors into simpler components reduces storage and enables efficient computation.</p>
        
        <h3>(a) CANDECOMP/PARAFAC (CP) Decomposition</h3>
        <p>Expresses a tensor as a <strong>sum of rank-1 tensors</strong>:</p>
        <div class="equation">
            \[ \mathcal{T} \approx \sum_{r=1}^R \lambda_r \, a^{(1)}_r \circ a^{(2)}_r \circ \cdots \circ a^{(N)}_r \]
        </div>
        <p>where \(\circ\) denotes outer product.</p>
        <ul>
            <li><strong>Parameters</strong>: \(R \cdot \sum_{n=1}^N d_n\) (vs. \(\prod d_n\) for full tensor).</li>
            <li><strong>Pros</strong>: Compact, interpretable.</li>
            <li><strong>Cons</strong>: Rank \(R\) hard to choose; decomposition not always unique; ill-conditioned.</li>
        </ul>
        
        <div class="definition">
            📌 Used in chemometrics, signal processing.
        </div>
        
        <h3>(b) Tucker Decomposition</h3>
        <p>Generalization of SVD to higher orders:</p>
        <div class="equation">
            \[ \mathcal{T} \approx \mathcal{G} \times_1 A^{(1)} \times_2 A^{(2)} \cdots \times_N A^{(N)} \]
        </div>
        <ul>
            <li>\(\mathcal{G}\): <strong>Core tensor</strong> (size \(r_1 \times r_2 \times \cdots \times r_N\))</li>
            <li>\(A^{(n)}\): Factor matrices (\(d_n \times r_n\))</li>
        </ul>
        <ul>
            <li><strong>Storage</strong>: \(\prod r_n + \sum d_n r_n\)</li>
            <li><strong>Pros</strong>: Flexible; captures multi-linear structure.</li>
            <li><strong>Cons</strong>: Core tensor still exponential in \(N\) if all \(r_n\) large → <strong>curse of dimensionality</strong>.</li>
        </ul>
        
        <div class="definition">
            📌 Basis for <strong>Higher-Order SVD (HOSVD)</strong>.
        </div>
        
        <h3>(c) Tensor Train (TT) / Matrix Product State (MPS)</h3>
        <p><strong>Decomposes tensor into a chain of 3D cores</strong>:</p>
        <div class="equation">
            \[ \mathcal{T}_{i_1 i_2 \dots i_N} = \sum_{\alpha_1,\dots,\alpha_{N-1}} 
            G^{(1)}_{i_1,\alpha_1} 
            G^{(2)}_{\alpha_1,i_2,\alpha_2} 
            \cdots 
            G^{(N)}_{\alpha_{N-1},i_N} \]
        </div>
        <ul>
            <li><strong>Bond dimensions</strong> \(\{\alpha_k\}\) control approximation rank.</li>
            <li><strong>Storage</strong>: \(O(N d r^2)\) for uniform physical dimension \(d\) and bond dimension \(r\).</li>
            <li><strong>Avoids exponential scaling</strong> if \(r\) is small (e.g., for weakly entangled states).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Key insight</strong>: TT/MPS is <strong>exact</strong> for any tensor (with \(r\) up to \(d^{N/2}\)), but <strong>efficient</strong> only when \(r \ll d^{N/2}\).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Matrix Product States (MPS) Representation</h2>
    <div class="content">
        <h3>Origin</h3>
        <p>MPS is the <strong>tensor network form of a quantum many-body wavefunction</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle = \sum_{s_1,\dots,s_N} c_{s_1 \dots s_N} |s_1 s_2 \dots s_N\rangle \]
        </div>
        <p>where \(c_{s_1 \dots s_N}\) is represented as a <strong>Tensor Train</strong>.</p>
        
        <h3>MPS Structure</h3>
        <p>Each site \(k\) has a <strong>core tensor</strong> \(A^{[k]}_{s_k}\) of shape \((\chi_{k-1}, d, \chi_k)\), where:</p>
        <ul>
            <li>\(d\): local Hilbert space dimension (e.g., \(d=2\) for qubits)</li>
            <li>\(\chi_k\): <strong>bond dimension</strong> (entanglement capacity)</li>
        </ul>
        <p><strong>Boundary conditions</strong>: \(\chi_0 = \chi_N = 1\) (open boundary)</p>
        
        <h3>Gauge Freedom</h3>
        <ul>
            <li>MPS is <strong>not unique</strong>: Can insert \(X X^{-1} = I\) between cores.</li>
            <li>Common gauges:
            <ul>
                <li><strong>Left-canonical</strong>: \(\sum_{s_k} (A^{[k]}_{s_k})^\dagger A^{[k]}_{s_k} = I\)</li>
                <li><strong>Right-canonical</strong>: \(\sum_{s_k} A^{[k]}_{s_k} (A^{[k]}_{s_k})^\dagger = I\)</li>
            </ul>
            </li>
            <li>Enables efficient local operations (e.g., in DMRG).</li>
        </ul>
        
        <h3>Entanglement and Bond Dimension</h3>
        <ul>
            <li>For a bipartition at bond \(k\), <strong>entanglement entropy</strong> \(S \leq \log_2 \chi_k\).</li>
            <li><strong>Area law</strong>: Ground states of 1D gapped Hamiltonians have <strong>constant</strong> \(\chi\) → MPS is efficient.</li>
        </ul>
        
        <div class="definition">
            🌟 MPS is the foundation of <strong>Density Matrix Renormalization Group (DMRG)</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Tensor Network Diagrams and Notation</h2>
    <div class="content">
        <h3>Diagrammatic Rules</h3>
        <ul>
            <li><strong>Node</strong>: Tensor (shape indicates order).</li>
            <li><strong>Edge</strong>: Index (labeled or unlabeled).</li>
            <li><strong>Connected edges</strong>: Contracted (summed over).</li>
            <li><strong>Open edges</strong>: Free indices (output of network).</li>
        </ul>
        
        <h3>Examples</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Object</th>
                        <th>Diagram</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Vector \(v_i\)</td>
                        <td>○——</td>
                    </tr>
                    <tr>
                        <td>Matrix \(M_{ij}\)</td>
                        <td>○——○</td>
                    </tr>
                    <tr>
                        <td>Order-3 tensor \(T_{ijk}\)</td>
                        <td>○ with 3 legs</td>
                    </tr>
                    <tr>
                        <td>Matrix multiplication \(C = AB\)</td>
                        <td>○——○——○ ⇒ ○————○</td>
                    </tr>
                    <tr>
                        <td>MPS (4 sites)</td>
                        <td>○—○—○—○ (each node has one physical leg down, bonds horizontal)</td>
                    </tr>
                    <tr>
                        <td>Contraction of two tensors</td>
                        <td>Two nodes connected by an edge</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Advantages of Diagrams</h3>
        <ul>
            <li>Visualize complex contractions.</li>
            <li>Reveal computational structure (e.g., sequential vs. parallel).</li>
            <li>Simplify derivation of algorithms (e.g., DMRG, TEBD).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Contraction Algorithms and Complexity</h2>
    <div class="content">
        <h3>What is Contraction?</h3>
        <p>Evaluate a tensor network by summing over all internal (bond) indices.</p>
        <p>Goal: Compute scalar (e.g., \(\langle \psi | \phi \rangle\)) or reduced tensor (e.g., reduced density matrix).</p>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Order matters</strong>: Different contraction sequences have vastly different costs.</li>
            <li><strong>Optimal contraction is NP-hard</strong> (equivalent to graph partitioning).</li>
        </ul>
        
        <h3>Contraction Strategies</h3>
        <h4>(a) Sequential (Naive) Contraction</h4>
        <ul>
            <li>Contract one pair at a time.</li>
            <li><strong>Cost</strong>: Can be exponential if done poorly.</li>
        </ul>
        
        <h4>(b) Optimal Tree Contraction (for Trees)</h4>
        <ul>
            <li>For <strong>tree-structured networks</strong> (no cycles), optimal order is linear in size.</li>
            <li>Use dynamic programming to find min-cost order.</li>
        </ul>
        
        <h4>(c) Heuristic Methods (for General Networks)</h4>
        <ul>
            <li><strong>Greedy</strong>: At each step, contract pair minimizing cost (e.g., `opt_einsum` in Python).</li>
            <li><strong>Graph-based</strong>: Model as <strong>line graph</strong>; use graph partitioning (e.g., METIS).</li>
            <li><strong>Tree decomposition</strong>: For networks with small <strong>treewidth</strong>.</li>
        </ul>
        
        <h3>Complexity Example: MPS Overlap</h3>
        <p>Compute \(\langle \psi | \phi \rangle\) for two MPS with bond dimensions \(\chi_\psi, \chi_\phi\).</p>
        <p>Contract from left to right:</p>
        <ul>
            <li>Each step: matrix multiplication of size \(\chi \times \chi\)</li>
            <li><strong>Total cost</strong>: \(O(N d \chi^3)\)</li>
        </ul>
        
        <h3>Memory vs. Time Trade-off</h3>
        <p><strong>Intermediate tensors</strong> can be large.</p>
        <p><strong>Slicing (index splitting)</strong>: Fix some indices to reduce memory at cost of repeated contractions.</p>
        
        <div class="definition">
            💡 <strong>Rule of thumb</strong>:<br>
            Contraction cost ≈ \(O(\chi^k)\), where \(k\) = number of bonds meeting at a vertex in the contraction tree.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Idea</th>
                        <th>Complexity/Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CP Decomposition</strong></td>
                        <td>Sum of rank-1 tensors</td>
                        <td>Compact but unstable</td>
                    </tr>
                    <tr>
                        <td><strong>Tucker</strong></td>
                        <td>Core + factor matrices</td>
                        <td>Flexible; core still large</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Train (MPS)</strong></td>
                        <td>Chain of cores; bond dimensions control entanglement</td>
                        <td>\(O(N d r^2)\); ideal for 1D quantum states</td>
                    </tr>
                    <tr>
                        <td><strong>MPS Gauge</strong></td>
                        <td>Left/right canonical forms enable local updates</td>
                        <td>Essential for DMRG</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Diagrams</strong></td>
                        <td>Visual language for indices and contractions</td>
                        <td>Intuitive algorithm design</td>
                    </tr>
                    <tr>
                        <td><strong>Contraction</strong></td>
                        <td>Order drastically affects cost; optimal is NP-hard</td>
                        <td>Heuristics used in practice (e.g., `opt_einsum`)</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li>Tensor networks <strong>tame the curse of dimensionality</strong> by exploiting <strong>structure</strong> (e.g., limited entanglement).</li>
            <li><strong>MPS</strong> is the workhorse for 1D quantum systems and underlies <strong>DMRG</strong>.</li>
            <li><strong>Contraction order</strong> is critical small networks can be evaluated exactly; large ones require smart heuristics.</li>
            <li>Diagrams are not just illustrative they are <strong>computational tools</strong>.</li>
            <li>Applications span <strong>quantum physics</strong>, <strong>machine learning</strong> (e.g., tensor completion), and <strong>data compression</strong>.</li>
        </ul>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Quantum-Inspired Search</h2>
    <div class="content">
        <p><em>Classical Approximations and Applications of Grover-like Techniques</em></p>
        
        <h3>Classical Approximations of Grover's Algorithm</h3>
        <h4>Can Grover's Algorithm Be Simulated Classically?</h4>
        <p><strong>Yes but not efficiently</strong> for large unstructured search spaces.</p>
        <p>The <strong>quadratic speedup</strong> (\(O(\sqrt{N})\) vs. \(O(N)\)) arises from <strong>quantum parallelism + interference</strong>, which classical systems cannot replicate <em>exactly</em> without exponential overhead.</p>
        
        <h4>Why Exact Simulation Fails at Scale</h4>
        <ul>
            <li>To simulate Grover's state evolution, a classical computer must store and update a <strong>superposition of \(N = 2^n\) amplitudes</strong>.</li>
            <li><strong>Memory and time cost</strong>: \(O(N)\) same as brute-force search.</li>
            <li>Thus, <strong>no asymptotic speedup</strong> from exact simulation.</li>
        </ul>
        
        <h4>Approximate Classical Emulations</h4>
        <p>Instead of simulating the full quantum state, classical algorithms borrow <strong>structural ideas</strong> from Grover:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Idea</th>
                        <th>Limitation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random sampling with boosting</strong></td>
                        <td>Repeatedly sample and "boost" likelihood of promising candidates</td>
                        <td>No guaranteed speedup; heuristic</td>
                    </tr>
                    <tr>
                        <td><strong>Amplitude-inspired weighting</strong></td>
                        <td>Assign weights to items; iteratively increase weight of marked items</td>
                        <td>Requires oracle feedback per iteration</td>
                    </tr>
                    <tr>
                        <td><strong>Monte Carlo with bias</strong></td>
                        <td>Bias random walks toward marked elements using oracle hints</td>
                        <td>Still \(O(N)\) in worst case</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Key insight</strong>: Classical systems can <strong>mimic the iterative refinement</strong> of Grover, but <strong>without interference</strong>, they cannot achieve true \(O(\sqrt{N})\) scaling.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Amplitude Amplification Techniques (Classical Analogues)</h2>
    <div class="content">
        <h3>What is Amplitude Amplification? (Quantum)</h3>
        <p>Generalization of Grover's algorithm.</p>
        <p>Given a subspace of "good" states, it <strong>rotates</strong> the state vector toward that subspace using:</p>
        <ul>
            <li>An <strong>oracle</strong> \(O\) that marks good states.</li>
            <li>A <strong>diffusion operator</strong> \(D = 2|\psi\rangle\langle\psi| - I\) that inverts amplitudes about the mean.</li>
        </ul>
        
        <h3>Classical Analogues</h3>
        <p>While true amplitude amplification is quantum, classical algorithms use <strong>similar iterative boosting</strong>:</p>
        
        <h4>(a) Multiplicative Weight Updates (MWU)</h4>
        <ul>
            <li>Maintain a probability distribution over items.</li>
            <li>After querying the oracle, <strong>increase weight</strong> of marked items, <strong>decrease</strong> others.</li>
            <li>Normalize to form new distribution.</li>
            <li>After \(O(\sqrt{N})\) rounds, marked item dominates <strong>in expectation</strong> but <strong>not guaranteed</strong>.</li>
        </ul>
        
        <h4>(b) Boosting in Machine Learning</h4>
        <ul>
            <li>Algorithms like <strong>AdaBoost</strong> iteratively adjust weights of misclassified samples.</li>
            <li><strong>Analogy</strong>: Oracle = weak learner; amplification = focusing on hard examples.</li>
            <li>Not a search algorithm per se, but shares the <strong>iterative refinement</strong> philosophy.</li>
        </ul>
        
        <h4>(c) Stochastic Search with Feedback</h4>
        <ul>
            <li>Use oracle to guide a <strong>biased random search</strong>:
            <ul>
                <li>Start with uniform distribution.</li>
                <li>After each query, update probabilities using Bayesian inference or reinforcement learning.</li>
            </ul>
            </li>
            <li>Can reduce expected queries but worst-case remains \(O(N)\).</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Critical difference</strong>: Quantum amplitude amplification uses <strong>coherent interference</strong> to <em>cancel</em> bad paths; classical methods only <em>reweight</em> no destructive interference.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Oracle Design for Classical Search</h2>
    <div class="content">
        <h3>What is an Oracle?</h3>
        <p>A <strong>black-box function</strong> \(f: \{0,1\}^n \to \{0,1\}\) such that \(f(x) = 1\) iff \(x\) is a solution.</p>
        <p>In quantum algorithms, the oracle is applied <strong>coherently</strong> to superpositions.</p>
        
        <h3>Classical Oracle Usage</h3>
        <p>In classical search, the oracle is simply a <strong>predicate function</strong> called on individual inputs.</p>
        <p><strong>Design considerations</strong>:</p>
        <ul>
            <li><strong>Cost per query</strong>: Should be \(O(1)\) or low for speedup to matter.</li>
            <li><strong>Side information</strong>: Can the oracle return more than yes/no? (e.g., gradient, distance)</li>
            <li><strong>Parallelizability</strong>: Can multiple queries be made simultaneously?</li>
        </ul>
        
        <h3>Quantum-Inspired Oracle Strategies</h3>
        <ol>
            <li><strong>Batch querying</strong>: Evaluate oracle on a <strong>subset</strong> of candidates in parallel (e.g., GPU).</li>
            <li><strong>Probabilistic oracles</strong>: Return approximate answers (e.g., in noisy settings).</li>
            <li><strong>Structured oracles</strong>: Exploit partial structure (e.g., locality, smoothness) to guide search blurring line between structured and unstructured search.</li>
        </ol>
        
        <div class="definition">
            🔍 <strong>Note</strong>: True unstructured search assumes <strong>no side information</strong> only yes/no answers. Most real-world problems have <em>some</em> structure, enabling better-than-Grover classical heuristics.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quadratic Speedup Approximations</h2>
    <div class="content">
        <h3>Can Classical Algorithms Achieve \(O(\sqrt{N})\) Queries?</h3>
        <p><strong>No not in the worst case for unstructured search</strong> (proven by Bennett et al., 1997).</p>
        <p><strong>Lower bound</strong>: Any classical randomized algorithm requires \(\Omega(N)\) queries to find a unique marked item with constant success probability.</p>
        
        <h3>When Can We *Approximate* Quadratic Speedup?</h3>
        <p>Only under <strong>relaxed assumptions</strong>:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Approximate Speedup</th>
                        <th>Mechanism</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Multiple solutions</strong> (\(M\) marked items)</td>
                        <td>\(O(N/M)\) classically vs. \(O(\sqrt{N/M})\) quantum</td>
                        <td>Classical: random sampling finds solution in \(O(N/M)\)</td>
                    </tr>
                    <tr>
                        <td><strong>Approximate search</strong></td>
                        <td>Find <em>near</em>-solution faster</td>
                        <td>Use locality, embeddings, or hashing (e.g., LSH)</td>
                    </tr>
                    <tr>
                        <td><strong>Parallel classical queries</strong></td>
                        <td>\(O(\sqrt{N})\) time with \(O(\sqrt{N})\) processors</td>
                        <td>Not query-efficient; trades time for hardware</td>
                    </tr>
                    <tr>
                        <td><strong>Probabilistic success</strong></td>
                        <td>High probability in fewer than \(N\) steps</td>
                        <td>Still \(\Omega(N)\) expected queries</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Quantum-Inspired Heuristics with Empirical Speedup</h3>
        <ul>
            <li><strong>Grover-inspired local search</strong>: Use amplitude-like weighting in combinatorial optimization.</li>
            <li><strong>Quantum walk-inspired search</strong>: Classical random walks with memory or restarts mimic hitting-time improvements.</li>
            <li><strong>Amplitude estimation via Monte Carlo</strong>: Estimate \(M/N\) (fraction of solutions) using classical sampling used in finance, but with \(O(1/\epsilon^2)\) vs. quantum \(O(1/\epsilon)\).</li>
        </ul>
        
        <div class="definition">
            📉 <strong>Bottom line</strong>: Classical algorithms <strong>cannot match</strong> Grover's asymptotic query complexity for unstructured search but can <strong>approach it</strong> in practice for structured or noisy variants.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications to Unstructured Search Problems</h2>
    <div class="content">
        <p>While true unstructured search is rare, many real-world problems <strong>approximate</strong> it:</p>
        
        <h3>(a) Cryptanalysis</h3>
        <ul>
            <li><strong>Brute-force key search</strong>: Find key \(k\) such that \(\text{Decrypt}_k(c) = m\).
            <ul>
                <li>Classical: \(O(2^n)\)</li>
                <li>Grover: \(O(2^{n/2})\) → reduces AES-128 security to ~64 bits.</li>
                <li><strong>Classical approximation</strong>: Use rainbow tables, side-channel info, or partial key guesses <strong>not unstructured</strong>.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(b) Database Search</h3>
        <ul>
            <li><strong>Idealized</strong>: Find record with property \(P\).
            <ul>
                <li>Quantum: \(O(\sqrt{N})\) with quantum RAM (QRAM) <strong>not yet feasible</strong>.</li>
                <li>Classical: Indexing (e.g., B-trees) makes it <strong>structured</strong> → \(O(\log N)\).</li>
                <li><strong>Unindexed databases</strong>: Linear scan = \(O(N)\); no classical quadratic speedup.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(c) NP Problem Solving</h3>
        <ul>
            <li><strong>SAT, Graph Coloring, etc.</strong>: Search space is unstructured in worst case.
            <ul>
                <li>Quantum: Grover gives \(O(\sqrt{2^n}) = O(1.414^n)\) vs. classical \(O(2^n)\).</li>
                <li>Classical heuristics (DPLL, CDCL): Exploit <strong>problem structure</strong> → often much faster than \(O(2^n)\), but <strong>exponential in worst case</strong>.</li>
                <li><strong>Quantum-inspired SAT solvers</strong>: Use amplitude-like scoring to prioritize variable assignments modest empirical gains.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(d) Machine Learning & Optimization</h3>
        <ul>
            <li><strong>Finding optimal parameters</strong> in high-dimensional space.
            <ul>
                <li>Quantum: Amplitude amplification for minimization.</li>
                <li>Classical: <strong>Simulated annealing</strong>, <strong>genetic algorithms</strong>, <strong>Bayesian optimization</strong> use feedback to guide search.</li>
                <li><strong>Quantum-inspired</strong>: Use Grover-like iteration in <strong>combinatorial optimization</strong> (e.g., MaxSAT), but no proven speedup.</li>
            </ul>
            </li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Quantum (Grover)</th>
                        <th>Classical Approximation</th>
                        <th>Achievable Speedup?</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Query complexity</strong></td>
                        <td>\(O(\sqrt{N})\)</td>
                        <td>\(\Omega(N)\) (worst-case lower bound)</td>
                        <td>❌ No</td>
                    </tr>
                    <tr>
                        <td><strong>Amplitude amplification</strong></td>
                        <td>Coherent interference</td>
                        <td>Weighted sampling / boosting</td>
                        <td>❌ (No interference)</td>
                    </tr>
                    <tr>
                        <td><strong>Oracle usage</strong></td>
                        <td>Applied to superposition</td>
                        <td>Applied to single inputs</td>
                        <td> </td>
                    </tr>
                    <tr>
                        <td><strong>Multiple solutions (\(M\))</strong></td>
                        <td>\(O(\sqrt{N/M})\)</td>
                        <td>\(O(N/M)\) (random sampling)</td>
                        <td>✅ Quadratic gap remains</td>
                    </tr>
                    <tr>
                        <td><strong>Real-world applicability</strong></td>
                        <td>Limited by QRAM, noise</td>
                        <td>Widely used heuristics</td>
                        <td>✅ Practical gains</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>True quadratic speedup is uniquely quantum</strong> for unstructured search <strong>not replicable classically</strong>.</li>
            <li><strong>Quantum-inspired classical algorithms</strong> borrow <em>ideas</em> (iterative boosting, weighting) but <strong>lack interference</strong>.</li>
            <li><strong>Oracle design</strong> and <strong>problem structure</strong> dominate real-world performance pure unstructured search is rare.</li>
            <li><strong>Applications</strong> benefit more from <strong>hybrid approaches</strong> (e.g., classical pre-processing + quantum search) than pure emulation.</li>
            <li><strong>No free lunch</strong>: Classical methods can't beat information-theoretic lower bounds but can exploit structure Grover ignores.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Final Thought</strong>: Quantum-inspired search is valuable for <strong>algorithm design intuition</strong> and <strong>heuristic development</strong>, but it does <strong>not challenge the fundamental quantum advantage</strong> of Grover's algorithm in its native setting.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Machine Learning Fundamentals</h2>
    <div class="content">
        <h3>Supervised Learning</h3>
        <div class="definition">
            <strong>Definition</strong>: Learning a mapping from input features \( \mathbf{x} \in \mathbb{R}^d \) to output labels \( y \) using a <strong>labeled training dataset</strong> \( \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n \).
        </div>
        
        <p>Two main types:</p>
        <ul>
            <li><strong>Classification</strong>: \( y \) is <strong>categorical</strong> (e.g., spam/not spam).</li>
            <li><strong>Regression</strong>: \( y \) is <strong>continuous</strong> (e.g., house price).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Classification and Regression Principles</h2>
    <div class="content">
        <h3>Goal</h3>
        <ul>
            <li>Learn a <strong>hypothesis function</strong> \( h_\theta(\mathbf{x}) \) that approximates the true relationship \( y \approx f(\mathbf{x}) \).</li>
            <li>Minimize <strong>prediction error</strong> on unseen data (<strong>generalization</strong>).</li>
        </ul>
        
        <h3>Loss Functions</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Common Loss Function</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Regression</strong></td>
                        <td>Mean Squared Error (MSE): \( \frac{1}{n}\sum (y_i - \hat{y}_i)^2 \)</td>
                        <td>Penalizes large errors quadratically</td>
                    </tr>
                    <tr>
                        <td><strong>Classification</strong></td>
                        <td>Cross-Entropy Loss: \( -\sum y_i \log(\hat{y}_i) \)</td>
                        <td>Measures divergence between true and predicted probabilities</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Bias-Variance Tradeoff</h3>
        <ul>
            <li><strong>High bias</strong> → underfitting (model too simple).</li>
            <li><strong>High variance</strong> → overfitting (model too complex).</li>
            <li>Goal: Find model complexity that balances both.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Decision Trees and Ensemble Methods</h2>
    <div class="content">
        <h3>Decision Trees</h3>
        <ul>
            <li><strong>Structure</strong>: Tree of <strong>if-else rules</strong> based on feature thresholds.</li>
            <li><strong>Splitting criterion</strong>:
            <ul>
                <li><strong>Classification</strong>: Information gain (based on entropy) or Gini impurity.<br>
                \[ \text{Gini} = 1 - \sum_{k} p_k^2 \]</li>
                <li><strong>Regression</strong>: Minimize MSE after split.</li>
            </ul>
            </li>
            <li><strong>Pros</strong>: Interpretable, handles mixed data types, no feature scaling needed.</li>
            <li><strong>Cons</strong>: Prone to overfitting, unstable to data changes.</li>
        </ul>
        
        <h3>Ensemble Methods</h3>
        <p>Combine multiple models to improve performance and robustness.</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Key Properties</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Bagging</strong> (e.g., <strong>Random Forest</strong>)</td>
                        <td>Train many trees on <strong>bootstrap samples</strong>; average predictions (regression) or vote (classification)</td>
                        <td>Reduces variance; decorrelates trees by random feature subsets</td>
                    </tr>
                    <tr>
                        <td><strong>Boosting</strong> (e.g., <strong>AdaBoost, XGBoost</strong>)</td>
                        <td>Sequentially train weak learners; each focuses on <strong>previous errors</strong></td>
                        <td>Reduces bias; highly accurate; sensitive to noise</td>
                    </tr>
                    <tr>
                        <td><strong>Stacking</strong></td>
                        <td>Use predictions of base models as input to a <strong>meta-learner</strong></td>
                        <td>Leverages strengths of diverse models</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Random Forest</strong>: Often a strong baseline robust, parallelizable, handles missing data.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Support Vector Machines (SVM)</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Find the <strong>optimal separating hyperplane</strong> that maximizes the <strong>margin</strong> (distance to nearest points).</p>
        
        <h3>Mathematical Formulation</h3>
        <p>For linearly separable data:</p>
        <div class="equation">
            \[ \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{subject to} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \]
        </div>
        <p><strong>Support vectors</strong>: Data points on the margin (only these affect the solution).</p>
        
        <h3>Non-linear SVM: Kernel Trick</h3>
        <ul>
            <li>Map data to higher-dimensional space via <strong>kernel function</strong> \( K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j) \).</li>
            <li>Common kernels:
            <ul>
                <li><strong>Linear</strong>: \( K = \mathbf{x}_i^\top \mathbf{x}_j \)</li>
                <li><strong>Polynomial</strong>: \( K = (\gamma \mathbf{x}_i^\top \mathbf{x}_j + r)^d \)</li>
                <li><strong>RBF (Gaussian)</strong>: \( K = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2) \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Soft Margin (for noisy data)</h3>
        <ul>
            <li>Introduce <strong>slack variables</strong> \( \xi_i \) to allow misclassifications.</li>
            <li>Controlled by <strong>regularization parameter \( C \)</strong>:
            <ul>
                <li>Large \( C \) → hard margin (low bias, high variance)</li>
                <li>Small \( C \) → soft margin (high bias, low variance)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Pros & Cons</h3>
        <ul>
            <li>✅ Effective in high dimensions, memory efficient (uses support vectors only).</li>
            <li>❌ Poor performance on large datasets; requires careful kernel and \( C \) tuning.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Neural Networks and Backpropagation</h2>
    <div class="content">
        <h3>Neural Network Structure</h3>
        <ul>
            <li><strong>Layers</strong>: Input → hidden layers → output.</li>
            <li><strong>Neuron</strong>: \( z = \mathbf{w}^\top \mathbf{x} + b \), \( a = \sigma(z) \)</li>
            <li><strong>Activation functions</strong>:
            <ul>
                <li><strong>Sigmoid</strong>: \( \sigma(z) = \frac{1}{1 + e^{-z}} \) → outputs probabilities (classification)</li>
                <li><strong>ReLU</strong>: \( \max(0, z) \) → avoids vanishing gradient; standard in hidden layers</li>
                <li><strong>Softmax</strong>: For multi-class output: \( \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}} \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Backpropagation</h3>
        <ul>
            <li><strong>Goal</strong>: Compute gradients of loss \( \mathcal{L} \) w.r.t. all weights for gradient descent.</li>
            <li><strong>Algorithm</strong>:
            <ol>
                <li><strong>Forward pass</strong>: Compute predictions \( \hat{y} \).</li>
                <li><strong>Compute loss</strong> \( \mathcal{L}(y, \hat{y}) \).</li>
                <li><strong>Backward pass</strong>: Apply chain rule from output to input:<br>
                \[ \frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \frac{\partial \mathcal{L}}{\partial a_j^{(l)}} \cdot \frac{\partial a_j^{(l)}}{\partial z_j^{(l)}} \cdot \frac{\partial z_j^{(l)}}{\partial w_{ij}^{(l)}} \]</li>
                <li>Update weights: \( w \leftarrow w - \eta \nabla_w \mathcal{L} \) (\(\eta\) = learning rate)</li>
            </ol>
            </li>
        </ul>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>Universal approximation theorem</strong>: A NN with 1 hidden layer can approximate any continuous function (given enough neurons).</li>
            <li><strong>Deep learning</strong>: Multiple hidden layers learn hierarchical features (e.g., edges → shapes → objects in images).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Model Evaluation Metrics</h2>
    <div class="content">
        <h3>For Classification</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Formula</th>
                        <th>When to Use</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Accuracy</strong></td>
                        <td>\( \frac{TP + TN}{TP + TN + FP + FN} \)</td>
                        <td>Balanced datasets</td>
                    </tr>
                    <tr>
                        <td><strong>Precision</strong></td>
                        <td>\( \frac{TP}{TP + FP} \)</td>
                        <td>Minimize false positives (e.g., spam detection)</td>
                    </tr>
                    <tr>
                        <td><strong>Recall (Sensitivity)</strong></td>
                        <td>\( \frac{TP}{TP + FN} \)</td>
                        <td>Minimize false negatives (e.g., disease screening)</td>
                    </tr>
                    <tr>
                        <td><strong>F1-Score</strong></td>
                        <td>\( 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \)</td>
                        <td>Imbalanced datasets; harmonic mean of P & R</td>
                    </tr>
                    <tr>
                        <td><strong>ROC-AUC</strong></td>
                        <td>Area under ROC curve (TPR vs. FPR)</td>
                        <td>Threshold-invariant performance</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Confusion Matrix</strong>:<br>
            <pre>               Predicted
              +       -
Actual   +   TP      FN
         -   FP      TN</pre>
        </div>
        
        <h3>For Regression</h3>
        <ul>
            <li><strong>Mean Absolute Error (MAE)</strong>: \( \frac{1}{n}\sum |y_i - \hat{y}_i| \) → robust to outliers.</li>
            <li><strong>Mean Squared Error (MSE)</strong>: \( \frac{1}{n}\sum (y_i - \hat{y}_i)^2 \) → penalizes large errors.</li>
            <li><strong>R² (Coefficient of Determination)</strong>:<br>
            \[ R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} \]
            <ul>
                <li>\( R^2 = 1 \): perfect fit; \( R^2 = 0 \): no better than mean predictor.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Cross-Validation</h3>
        <ul>
            <li><strong>k-Fold CV</strong>: Split data into \(k\) folds; train on \(k-1\), validate on 1; repeat \(k\) times.</li>
            <li>Provides <strong>unbiased estimate</strong> of generalization performance.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Supervised Learning Algorithms</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Type</th>
                        <th>Strengths</th>
                        <th>Weaknesses</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Decision Tree</strong></td>
                        <td>Both</td>
                        <td>Interpretable, fast</td>
                        <td>Overfits</td>
                        <td>Baseline, explainability</td>
                    </tr>
                    <tr>
                        <td><strong>Random Forest</strong></td>
                        <td>Both</td>
                        <td>Robust, handles noise</td>
                        <td>Less interpretable</td>
                        <td>General-purpose</td>
                    </tr>
                    <tr>
                        <td><strong>SVM</strong></td>
                        <td>Both</td>
                        <td>Effective in high-D, memory efficient</td>
                        <td>Slow on large data</td>
                        <td>Text classification, small/medium datasets</td>
                    </tr>
                    <tr>
                        <td><strong>Neural Network</strong></td>
                        <td>Both</td>
                        <td>Learns complex patterns</td>
                        <td>Data-hungry, black-box</td>
                        <td>Images, speech, large datasets</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Supervised learning</strong> requires labeled data and aims to generalize from examples.</li>
            <li><strong>No free lunch</strong>: Algorithm choice depends on data size, dimensionality, noise, and interpretability needs.</li>
            <li><strong>Evaluation metrics must match the problem</strong>: Accuracy fails on imbalanced data; use F1 or AUC instead.</li>
            <li><strong>Ensembles</strong> (e.g., Random Forest, XGBoost) often outperform single models.</li>
            <li><strong>Neural networks</strong> excel with large data but require careful tuning and validation.</li>
        </ul>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Unsupervised Learning</h2>
    <div class="content">
        <p><em>Learning patterns from unlabeled data</em></p>
        
        <div class="definition">
            <strong>Definition</strong>: Discover hidden structures, patterns, or relationships in data <strong>without labeled outputs</strong>.<br>
            <strong>Goal</strong>: Summarize, compress, or reveal intrinsic properties of the data.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Clustering Algorithms</h2>
    <div class="content">
        <p>Clustering groups similar data points together based on feature similarity.</p>
        
        <h3>(a) K-Means Clustering</h3>
        <ul>
            <li><strong>Objective</strong>: Partition \(n\) points into \(k\) clusters to minimize <strong>within-cluster sum of squares (WCSS)</strong>:<br>
            \[ \min_{C_1,\dots,C_k} \sum_{i=1}^k \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2 \]
            where \(\boldsymbol{\mu}_i\) = centroid of cluster \(i\).</li>
            
            <li><strong>Algorithm</strong> (Lloyd's):
            <ol>
                <li>Initialize \(k\) centroids randomly.</li>
                <li><strong>Assign</strong>: Assign each point to nearest centroid.</li>
                <li><strong>Update</strong>: Recompute centroids as mean of assigned points.</li>
                <li>Repeat until convergence.</li>
            </ol>
            </li>
            
            <li><strong>Pros</strong>: Simple, fast, scalable.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li>Requires pre-specifying \(k\).</li>
                <li>Sensitive to initialization and outliers.</li>
                <li>Assumes spherical, equally sized clusters.</li>
            </ul>
            </li>
            
            <li><strong>Choosing \(k\)</strong>: Use <strong>elbow method</strong> (plot WCSS vs. \(k\)) or <strong>silhouette score</strong>.</li>
        </ul>
        
        <h3>(b) Hierarchical Clustering</h3>
        <ul>
            <li>Builds a <strong>tree of clusters</strong> (dendrogram).</li>
            <li>Two approaches:
            <ul>
                <li><strong>Agglomerative</strong> (bottom-up): Start with each point as a cluster; merge closest pairs.</li>
                <li><strong>Divisive</strong> (top-down): Start with one cluster; recursively split.</li>
            </ul>
            </li>
            
            <li><strong>Linkage criteria</strong> (define distance between clusters):
            <ul>
                <li><strong>Single</strong>: min distance → chaining effect.</li>
                <li><strong>Complete</strong>: max distance → compact clusters.</li>
                <li><strong>Average</strong>: mean distance → balanced.</li>
                <li><strong>Ward</strong>: minimizes WCSS → similar to K-means.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>: No need to specify \(k\); dendrogram gives full hierarchy.</li>
            <li><strong>Cons</strong>: \(O(n^3)\) time; not scalable to large datasets.</li>
        </ul>
        
        <h3>(c) DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h3>
        <ul>
            <li><strong>Core idea</strong>: Cluster points in <strong>dense regions</strong>, mark sparse points as <strong>noise</strong>.</li>
            <li><strong>Parameters</strong>:
            <ul>
                <li>\(\varepsilon\): Radius of neighborhood.</li>
                <li>MinPts: Minimum points in \(\varepsilon\)-neighborhood to be a <strong>core point</strong>.</li>
            </ul>
            </li>
            
            <li><strong>Point types</strong>:
            <ul>
                <li><strong>Core</strong>: ≥ MinPts in \(\varepsilon\)-neighborhood.</li>
                <li><strong>Border</strong>: In neighborhood of core, but not core itself.</li>
                <li><strong>Noise</strong>: Neither core nor border.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>:
            <ul>
                <li>Finds arbitrarily shaped clusters.</li>
                <li>Robust to outliers.</li>
                <li>No need to specify \(k\).</li>
            </ul>
            </li>
            <li><strong>Cons</strong>: Struggles with varying densities; sensitive to \(\varepsilon\) and MinPts.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Spatial data, anomaly detection, irregular cluster shapes.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Dimensionality Reduction</h2>
    <div class="content">
        <p>Reduce number of features while preserving essential structure.</p>
        
        <h3>(a) Principal Component Analysis (PCA)</h3>
        <ul>
            <li><strong>Goal</strong>: Project data onto orthogonal axes (<strong>principal components</strong>) that capture <strong>maximum variance</strong>.</li>
            <li><strong>Steps</strong>:
            <ol>
                <li>Standardize data (zero mean, unit variance).</li>
                <li>Compute covariance matrix.</li>
                <li>Eigendecomposition: Find eigenvectors (PCs) and eigenvalues (variance explained).</li>
                <li>Project data onto top \(k\) PCs.</li>
            </ol>
            </li>
            
            <li><strong>Mathematically</strong>:<br>
            \[ \mathbf{Z} = \mathbf{X} \mathbf{W}_k \]
            where \(\mathbf{W}_k\) = top \(k\) eigenvectors.</li>
            
            <li><strong>Pros</strong>: Linear, fast, removes multicollinearity.</li>
            <li><strong>Cons</strong>: Only captures linear relationships; global structure only.</li>
            <li><strong>Use cases</strong>: Visualization (2D/3D), noise reduction, preprocessing for ML.</li>
        </ul>
        
        <h3>(b) t-SNE (t-Distributed Stochastic Neighbor Embedding)</h3>
        <ul>
            <li><strong>Goal</strong>: Preserve <strong>local similarities</strong> in low-dimensional embedding (typically 2D/3D).</li>
            <li><strong>Key idea</strong>:
            <ul>
                <li>Convert high-D Euclidean distances to <strong>probabilities</strong> (similar points → high probability).</li>
                <li>Minimize <strong>KL divergence</strong> between high-D and low-D probability distributions.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>: Excellent for <strong>visualization</strong>; reveals clusters and manifolds.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li>Not linear; <strong>not for preprocessing</strong> (distorts global structure).</li>
                <li>Computationally expensive (\(O(n^2)\)).</li>
                <li>Results vary with perplexity (key hyperparameter).</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            📌 <strong>Rule</strong>: Use <strong>PCA first</strong> (to reduce to 50D), then <strong>t-SNE</strong> for visualization.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Anomaly Detection Methods</h2>
    <div class="content">
        <p>Identify rare items, events, or observations that differ significantly from the majority.</p>
        
        <h3>Common Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Statistical</strong></td>
                        <td>Assume data follows distribution (e.g., Gaussian); flag points with low likelihood</td>
                        <td>Univariate data, known distribution</td>
                    </tr>
                    <tr>
                        <td><strong>Distance-based</strong></td>
                        <td>Points far from neighbors are anomalies (e.g., k-NN distance)</td>
                        <td>Multivariate, no distribution assumption</td>
                    </tr>
                    <tr>
                        <td><strong>Density-based</strong></td>
                        <td>Low-density regions = anomalies (e.g., LOF: Local Outlier Factor)</td>
                        <td>Varying densities</td>
                    </tr>
                    <tr>
                        <td><strong>Clustering-based</strong></td>
                        <td>Points not in any cluster (or in small clusters) are anomalies</td>
                        <td>Works with DBSCAN, K-means</td>
                    </tr>
                    <tr>
                        <td><strong>Reconstruction-based</strong></td>
                        <td>Use autoencoders (see below); high reconstruction error = anomaly</td>
                        <td>High-dimensional, complex data</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Evaluation</h3>
        <ul>
            <li>Often <strong>semi-supervised</strong>: Assume most data is normal.</li>
            <li>Metrics: Precision@K, ROC-AUC (if labels available).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Autoencoders and Their Variants</h2>
    <div class="content">
        <p>Neural networks that learn efficient data <strong>encodings</strong> in an unsupervised manner.</p>
        
        <h3>Basic Autoencoder</h3>
        <ul>
            <li><strong>Architecture</strong>:<br>
            <strong>Encoder</strong>: \( \mathbf{z} = f(\mathbf{x}) = \sigma(\mathbf{W}_e \mathbf{x} + \mathbf{b}_e) \)<br>
            <strong>Decoder</strong>: \( \hat{\mathbf{x}} = g(\mathbf{z}) = \sigma(\mathbf{W}_d \mathbf{z} + \mathbf{b}_d) \)</li>
            <li><strong>Loss</strong>: Reconstruction error (e.g., MSE): \( \mathcal{L} = \|\mathbf{x} - \hat{\mathbf{x}}\|^2 \)</li>
            <li><strong>Bottleneck</strong>: Latent dimension \( < \) input dimension → forces compression.</li>
        </ul>
        
        <h3>Key Variants</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Variant</th>
                        <th>Modification</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Denoising Autoencoder (DAE)</strong></td>
                        <td>Input: corrupted \(\tilde{\mathbf{x}}\); target: clean \(\mathbf{x}\)</td>
                        <td>Learn robust features; prevent identity mapping</td>
                    </tr>
                    <tr>
                        <td><strong>Sparse Autoencoder</strong></td>
                        <td>Add sparsity penalty on latent code (e.g., KL divergence on activations)</td>
                        <td>Learn part-based representations</td>
                    </tr>
                    <tr>
                        <td><strong>Variational Autoencoder (VAE)</strong></td>
                        <td>Latent code sampled from learned distribution \(q(z|x)\); regularized via KL divergence to prior \(p(z)\)</td>
                        <td>Generative model; smooth latent space</td>
                    </tr>
                    <tr>
                        <td><strong>Convolutional Autoencoder</strong></td>
                        <td>Use CNN layers in encoder/decoder</td>
                        <td>Handle image data; preserve spatial structure</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Applications</h3>
        <ul>
            <li>Dimensionality reduction (non-linear alternative to PCA)</li>
            <li>Anomaly detection (high reconstruction error = anomaly)</li>
            <li>Denoising, image inpainting</li>
            <li>Pretraining for deep networks</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Association Rule Learning</h2>
    <div class="content">
        <p>Discover <strong>interesting relationships</strong> between variables in large datasets (e.g., market basket analysis).</p>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>Itemset</strong>: Collection of items (e.g., {milk, bread}).</li>
            <li><strong>Transaction</strong>: A single record (e.g., customer purchase).</li>
            <li><strong>Rule</strong>: \( X \Rightarrow Y \) (e.g., {milk} ⇒ {bread})</li>
        </ul>
        
        <h3>Metrics</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Formula</th>
                        <th>Interpretation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Support</strong></td>
                        <td>\( \frac{\text{# transactions containing } X \cup Y}{\text{total transactions}} \)</td>
                        <td>How frequent is the rule?</td>
                    </tr>
                    <tr>
                        <td><strong>Confidence</strong></td>
                        <td>\( \frac{\text{support}(X \cup Y)}{\text{support}(X)} \)</td>
                        <td>How often does Y occur when X occurs?</td>
                    </tr>
                    <tr>
                        <td><strong>Lift</strong></td>
                        <td>\( \frac{\text{confidence}(X \Rightarrow Y)}{\text{support}(Y)} \)</td>
                        <td>>1: positive correlation; =1: independent; <1: negative</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Apriori Algorithm</h3>
        <ul>
            <li><strong>Principle</strong>: If an itemset is frequent, all its subsets are frequent (<strong>anti-monotonicity</strong>).</li>
            <li><strong>Steps</strong>:
            <ol>
                <li>Find all frequent 1-itemsets (support ≥ min_support).</li>
                <li>Generate candidate \(k\)-itemsets from frequent \((k-1)\)-itemsets.</li>
                <li>Prune candidates with infrequent subsets.</li>
                <li>Repeat until no more frequent itemsets.</li>
            </ol>
            </li>
            <li><strong>Pros</strong>: Simple, guarantees completeness.</li>
            <li><strong>Cons</strong>: Computationally expensive for large itemsets.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Modern alternatives</strong>: FP-Growth (uses frequent pattern tree; faster, no candidate generation).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Type</th>
                        <th>Key Strength</th>
                        <th>Limitation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>K-Means</strong></td>
                        <td>Clustering</td>
                        <td>Fast, simple</td>
                        <td>Spherical clusters only</td>
                    </tr>
                    <tr>
                        <td><strong>DBSCAN</strong></td>
                        <td>Clustering</td>
                        <td>Finds arbitrary shapes, handles noise</td>
                        <td>Struggles with density variation</td>
                    </tr>
                    <tr>
                        <td><strong>PCA</strong></td>
                        <td>Dim. Reduction</td>
                        <td>Linear, fast, interpretable</td>
                        <td>Misses non-linear structure</td>
                    </tr>
                    <tr>
                        <td><strong>t-SNE</strong></td>
                        <td>Dim. Reduction</td>
                        <td>Excellent visualization</td>
                        <td>Not for preprocessing; slow</td>
                    </tr>
                    <tr>
                        <td><strong>Autoencoder</strong></td>
                        <td>Representation</td>
                        <td>Non-linear, flexible</td>
                        <td>Requires tuning; black-box</td>
                    </tr>
                    <tr>
                        <td><strong>Association Rules</strong></td>
                        <td>Pattern Mining</td>
                        <td>Interpretable business rules</td>
                        <td>Combinatorial explosion</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Clustering</strong> reveals group structure; choose algorithm based on shape, noise, and scalability.</li>
            <li><strong>Dimensionality reduction</strong>: Use <strong>PCA</strong> for linear/global structure, <strong>t-SNE</strong> for visualization.</li>
            <li><strong>Anomaly detection</strong> is critical in fraud, security, and quality control method depends on data assumptions.</li>
            <li><strong>Autoencoders</strong> provide powerful non-linear compression and are foundational for deep generative models.</li>
            <li><strong>Association rules</strong> uncover actionable insights in transactional data but require careful thresholding.</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Remember</strong>: Unsupervised learning is <strong>exploratory</strong> results must be validated with domain knowledge!
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Deep Learning</h2>
    <div class="content">
        <p><em>Advanced Neural Network Architectures and Training Methods</em></p>
        
        <h3>Convolutional Neural Networks (CNNs)</h3>
        <h4>Core Idea</h4>
        <ul>
            <li>Designed for <strong>grid-like data</strong> (e.g., images, audio spectrograms).</li>
            <li>Exploit <strong>spatial locality</strong> and <strong>translation invariance</strong> via <strong>convolutional layers</strong>.</li>
        </ul>
        
        <h4>Key Components</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Function</th>
                        <th>Parameters</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Convolutional</strong></td>
                        <td>Apply learnable filters (kernels) to detect local features (edges, textures)</td>
                        <td>Filter size (e.g., 3×3), #filters, stride, padding</td>
                    </tr>
                    <tr>
                        <td><strong>Activation</strong></td>
                        <td>Introduce non-linearity (e.g., ReLU: \( \max(0, x) \))</td>
                        <td>None</td>
                    </tr>
                    <tr>
                        <td><strong>Pooling</strong></td>
                        <td>Downsample feature maps (reduce spatial size, retain key info)</td>
                        <td>Max-pooling (most common), average-pooling</td>
                    </tr>
                    <tr>
                        <td><strong>Fully Connected (FC)</strong></td>
                        <td>Final layers for classification/regression</td>
                        <td>Standard dense layers</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h4>Why CNNs Work</h4>
        <ul>
            <li><strong>Parameter sharing</strong>: Same filter applied across entire input → fewer parameters.</li>
            <li><strong>Hierarchical feature learning</strong>:<br>
            Early layers → edges → mid layers → shapes → late layers → objects.</li>
            <li><strong>Translation equivariance</strong>: Shift in input → shift in feature map (not output class).</li>
        </ul>
        
        <h4>Common Architectures</h4>
        <ul>
            <li><strong>LeNet-5</strong> (1998): First CNN for digit recognition.</li>
            <li><strong>AlexNet</strong> (2012): Deep CNN with ReLU, dropout; won ImageNet.</li>
            <li><strong>VGG</strong>: Uniform 3×3 convolutions; deeper = better.</li>
            <li><strong>ResNet</strong>: Uses <strong>skip connections</strong> to solve vanishing gradients → trains 1000+ layers.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Image classification, object detection, segmentation, medical imaging.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Recurrent Neural Networks (RNNs) and LSTMs</h2>
    <div class="content">
        <h3>RNNs: Processing Sequences</h3>
        <ul>
            <li><strong>Idea</strong>: Maintain a <strong>hidden state</strong> \( h_t \) that captures information from past inputs.</li>
            <li><strong>Update rule</strong>:<br>
            \[ h_t = \sigma(W_h h_{t-1} + W_x x_t + b) \]
            \( y_t = W_y h_t + b_y \)</li>
            
            <li><strong>Pros</strong>: Handles variable-length sequences; shares parameters across time.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li><strong>Vanishing/exploding gradients</strong> → struggles with long-term dependencies.</li>
                <li>Sequential computation → not parallelizable.</li>
            </ul>
            </li>
        </ul>
        
        <h3>LSTM (Long Short-Term Memory)</h3>
        <ul>
            <li>Solves vanishing gradient with <strong>gating mechanisms</strong>:
            <ul>
                <li><strong>Forget gate</strong> \( f_t \): Decide what to discard from cell state.</li>
                <li><strong>Input gate</strong> \( i_t \): Decide what new info to store.</li>
                <li><strong>Output gate</strong> \( o_t \): Decide what to output.</li>
            </ul>
            </li>
            
            <li><strong>Cell state update</strong>:<br>
            \[ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t, \quad h_t = o_t \odot \tanh(C_t) \]</li>
            
            <li><strong>GRU (Gated Recurrent Unit)</strong>: Simpler variant (2 gates); often comparable to LSTM.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Time series forecasting, speech recognition, machine translation (pre-Transformer).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Transformer Architectures and Attention Mechanisms</h2>
    <div class="content">
        <h3>Limitations of RNNs/CNNs for Sequences</h3>
        <ul>
            <li>RNNs: Not parallelizable; slow training.</li>
            <li>CNNs: Fixed receptive field; need stacking for long-range dependencies.</li>
        </ul>
        
        <h3>Attention Mechanism (Core Idea)</h3>
        <ul>
            <li>Compute <strong>weighted sum</strong> of values, where weights are based on <strong>compatibility</strong> between query and keys.</li>
            <li><strong>Scaled Dot-Product Attention</strong>:<br>
            \[ \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V \]
            <ul>
                <li>\( Q \): queries, \( K \): keys, \( V \): values</li>
                <li>\( d_k \): dimension of keys (scaling prevents softmax saturation)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Transformer Architecture (Vaswani et al., 2017)</h3>
        <ul>
            <li><strong>Encoder-Decoder</strong> structure (for seq2seq tasks like translation).</li>
            <li><strong>Each block contains</strong>:
            <ul>
                <li><strong>Multi-Head Attention</strong>: Concatenate attention from multiple subspaces → captures different relationships.</li>
                <li><strong>Position-wise Feed-Forward Network</strong>: Two linear layers with ReLU.</li>
                <li><strong>Residual connections + LayerNorm</strong>: Stabilize training.</li>
            </ul>
            </li>
            
            <li><strong>Positional Encoding</strong>: Inject sequence order info (since no recurrence):<br>
            \[ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad
            PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right) \]</li>
        </ul>
        
        <h3>Impact</h3>
        <ul>
            <li><strong>Parallelizable</strong>: All tokens processed simultaneously → faster training.</li>
            <li><strong>State-of-the-art</strong>: Foundation of BERT, GPT, T5, and most modern NLP models.</li>
            <li><strong>Beyond NLP</strong>: Vision Transformers (ViT), speech, protein folding (AlphaFold).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: NLP, multimodal learning, any task with long-range dependencies.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Deep Learning Optimization Techniques</h2>
    <div class="content">
        <p>Training deep networks is challenging due to non-convexity, saddle points, and ill-conditioning.</p>
        
        <h3>Gradient Descent Variants</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Optimizer</th>
                        <th>Update Rule</th>
                        <th>Advantages</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>SGD</strong></td>
                        <td>\( \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L} \)</td>
                        <td>Simple, but oscillates</td>
                    </tr>
                    <tr>
                        <td><strong>SGD + Momentum</strong></td>
                        <td>\( v \leftarrow \beta v + \nabla \mathcal{L},\ \theta \leftarrow \theta - \eta v \)</td>
                        <td>Smoother convergence</td>
                    </tr>
                    <tr>
                        <td><strong>RMSProp</strong></td>
                        <td>\( s \leftarrow \beta s + (1-\beta)(\nabla \mathcal{L})^2,\ \theta \leftarrow \theta - \eta \frac{\nabla \mathcal{L}}{\sqrt{s} + \epsilon} \)</td>
                        <td>Adapts per-parameter learning rate</td>
                    </tr>
                    <tr>
                        <td><strong>Adam</strong></td>
                        <td>Combines momentum + RMSProp</td>
                        <td>Most popular; robust default</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Learning Rate Scheduling</h3>
        <ul>
            <li><strong>Step decay</strong>: Reduce LR at fixed epochs.</li>
            <li><strong>Cosine annealing</strong>: Smoothly decrease LR to 0.</li>
            <li><strong>Warmup</strong>: Gradually increase LR at start (critical for Transformers).</li>
        </ul>
        
        <h3>Gradient Clipping</h3>
        <p>Cap gradient norm to prevent exploding gradients (essential for RNNs).</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Regularization Methods</h2>
    <div class="content">
        <p>Prevent overfitting in high-capacity deep networks.</p>
        
        <h3>(a) Dropout</h3>
        <ul>
            <li><strong>Idea</strong>: Randomly set a fraction \( p \) of activations to zero during training.</li>
            <li><strong>Effect</strong>: Forces network to not rely on specific neurons → ensemble-like behavior.</li>
            <li><strong>At test time</strong>: Scale activations by \( (1 - p) \) (or use inverted dropout).</li>
            <li><strong>Typical rates</strong>: 0.2–0.5 in hidden layers; not used in RNN hidden states (use <strong>variational dropout</strong> instead).</li>
        </ul>
        
        <h3>(b) Batch Normalization (BatchNorm)</h3>
        <ul>
            <li><strong>Idea</strong>: Normalize layer inputs to have zero mean and unit variance <strong>per batch</strong>:<br>
            \[ \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta \]
            where \( \gamma, \beta \) are learnable scale/shift parameters.</li>
            
            <li><strong>Benefits</strong>:
            <ul>
                <li>Reduces internal covariate shift.</li>
                <li>Allows higher learning rates.</li>
                <li>Acts as a regularizer (due to batch noise).</li>
            </ul>
            </li>
            
            <li><strong>Placement</strong>: After linear layer, before activation (common in CNNs).</li>
            <li><strong>Alternatives</strong>:
            <ul>
                <li><strong>LayerNorm</strong>: Normalize across features (used in Transformers).</li>
                <li><strong>GroupNorm</strong>: Between BatchNorm and LayerNorm; works well for small batches.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Other Regularization Techniques</h3>
        <ul>
            <li><strong>Weight decay (L2 regularization)</strong>: Penalize large weights.</li>
            <li><strong>Data augmentation</strong>: Artificially expand training set (e.g., rotate/flipping images).</li>
            <li><strong>Early stopping</strong>: Halt training when validation loss stops improving.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Key Innovation</th>
                        <th>Primary Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CNN</strong></td>
                        <td>Local receptive fields, weight sharing</td>
                        <td>Images, spatial data</td>
                    </tr>
                    <tr>
                        <td><strong>RNN/LSTM</strong></td>
                        <td>Hidden state for memory</td>
                        <td>Sequential data (time series, text)</td>
                    </tr>
                    <tr>
                        <td><strong>Transformer</strong></td>
                        <td>Self-attention, parallelization</td>
                        <td>NLP, long-range dependencies</td>
                    </tr>
                    <tr>
                        <td><strong>Adam</strong></td>
                        <td>Adaptive per-parameter learning rates</td>
                        <td>Default optimizer for most tasks</td>
                    </tr>
                    <tr>
                        <td><strong>Dropout</strong></td>
                        <td>Random neuron deactivation</td>
                        <td>General-purpose regularization</td>
                    </tr>
                    <tr>
                        <td><strong>BatchNorm</strong></td>
                        <td>Normalize per batch</td>
                        <td>Stabilize and accelerate CNN training</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>CNNs</strong> dominate <strong>spatial data</strong>; <strong>Transformers</strong> dominate <strong>sequential data</strong> (replacing RNNs).</li>
            <li><strong>Attention</strong> is a universal mechanism now used in vision, audio, and biology.</li>
            <li><strong>Optimization</strong> is as important as architecture: Adam + learning rate scheduling is standard.</li>
            <li><strong>Regularization</strong> is essential: Combine dropout, BatchNorm, and data augmentation.</li>
            <li><strong>Modern deep learning</strong> = <strong>modular design</strong>: Mix and match components (e.g., CNN + Transformer for video).</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>: For new projects, start with a <strong>Transformer</strong> (for sequences) or <strong>ResNet/ViT</strong> (for images) they are strong baselines.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Neural Networks</h2>
    <div class="content">
        <h3>Graph Representation Learning</h3>
        <div class="definition">
            <strong>Goal</strong>: Learn low-dimensional, continuous vector representations (<strong>embeddings</strong>) of nodes, edges, or entire graphs that preserve structural and feature information for downstream tasks (e.g., classification, link prediction, clustering).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Theory Fundamentals</h2>
    <div class="content">
        <h3>Basic Definitions</h3>
        <ul>
            <li><strong>Graph</strong> \( G = (V, E) \):
            <ul>
                <li>\( V \): Set of <strong>nodes</strong> (vertices), \( |V| = n \)</li>
                <li>\( E \subseteq V \times V \): Set of <strong>edges</strong> (links)</li>
            </ul>
            </li>
            <li><strong>Types</strong>:
            <ul>
                <li><strong>Undirected</strong>: \( (u,v) \in E \Rightarrow (v,u) \in E \)</li>
                <li><strong>Directed</strong>: Edges have direction (e.g., web graphs)</li>
                <li><strong>Weighted</strong>: Edges have weights \( w_{uv} \)</li>
                <li><strong>Attributed</strong>: Nodes/edges have features \( \mathbf{x}_v \in \mathbb{R}^d \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Concepts</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Term</th>
                        <th>Definition</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Adjacency matrix</strong> \( A \)</td>
                        <td>\( A_{ij} = 1 \) if edge between \( i,j \); 0 otherwise</td>
                    </tr>
                    <tr>
                        <td><strong>Degree matrix</strong> \( D \)</td>
                        <td>Diagonal matrix: \( D_{ii} = \sum_j A_{ij} \)</td>
                    </tr>
                    <tr>
                        <td><strong>Laplacian</strong> \( L \)</td>
                        <td>\( L = D - A \) (captures smoothness on graphs)</td>
                    </tr>
                    <tr>
                        <td><strong>Path / Walk</strong></td>
                        <td>Sequence of connected nodes</td>
                    </tr>
                    <tr>
                        <td><strong>Neighborhood</strong> \( \mathcal{N}(v) \)</td>
                        <td>Set of nodes adjacent to \( v \)</td>
                    </tr>
                    <tr>
                        <td><strong>Connected component</strong></td>
                        <td>Maximal subgraph where all nodes are reachable</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Why Graphs Matter</h3>
        <ul>
            <li>Model relational data: social networks, molecules, knowledge graphs, traffic systems.</li>
            <li>Structure encodes semantics: “You are who your friends are” (homophily).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Embeddings and Node Representations</h2>
    <div class="content">
        <h3>What is a Graph Embedding?</h3>
        <p>Map each node \( v \) to a vector \( \mathbf{z}_v \in \mathbb{R}^k \) (\( k \ll n \)) such that <strong>graph proximity ≈ vector similarity</strong>.</p>
        
        <h3>Desired Properties</h3>
        <ol>
            <li><strong>Structural equivalence</strong>: Nodes with similar roles (e.g., hubs) have similar embeddings.</li>
            <li><strong>Homophily</strong>: Connected nodes have similar embeddings.</li>
            <li><strong>Scalability</strong>: Efficient for large graphs.</li>
            <li><strong>Inductive capability</strong>: Generalize to unseen nodes.</li>
        </ol>
        
        <h3>Categories of Methods</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Approach</th>
                        <th>Examples</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Matrix factorization</strong></td>
                        <td>Factorize graph matrices (e.g., \( A \), \( L \))</td>
                        <td>Laplacian Eigenmaps, GraRep</td>
                    </tr>
                    <tr>
                        <td><strong>Random walk-based</strong></td>
                        <td>Use walk statistics as context</td>
                        <td>DeepWalk, node2vec</td>
                    </tr>
                    <tr>
                        <td><strong>Neural network-based</strong></td>
                        <td>Learn via message passing</td>
                        <td>GCN, GAT, GraphSAGE</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Random Walk Techniques on Graphs</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Treat node neighborhoods as “sentences” and use <strong>language modeling</strong> (e.g., Word2Vec) to learn embeddings.</p>
        
        <h3>(a) DeepWalk (2014)</h3>
        <ul>
            <li><strong>Steps</strong>:
            <ol>
                <li>Generate <strong>random walks</strong> (length \( l \)) from each node.</li>
                <li>Treat each walk as a “sentence” of nodes.</li>
                <li>Apply <strong>Skip-gram</strong> to predict context nodes given center node.</li>
            </ol>
            </li>
            <li><strong>Optimization</strong>:<br>
            \[ \max_{\mathbf{Z}} \sum_{v \in V} \sum_{u \in \mathcal{N}_R(v)} \log P(u | v; \mathbf{Z}) \]
            where \( \mathcal{N}_R(v) \) = nodes in random walks starting at \( v \).</li>
        </ul>
        
        <h3>(b) node2vec (2016)</h3>
        <ul>
            <li><strong>Improvement</strong>: Biased random walks controlled by two parameters:
            <ul>
                <li><strong>Return parameter \( p \)</strong>: Likelihood to return to previous node.</li>
                <li><strong>In-out parameter \( q \)</strong>: Explore locally (\( q < 1 \)) vs. globally (\( q > 1 \)).</li>
            </ul>
            </li>
            <li><strong>Flexible</strong>: Interpolates between <strong>BFS</strong> (structural equivalence) and <strong>DFS</strong> (homophily).</li>
        </ul>
        
        <h3>Pros & Cons</h3>
        <ul>
            <li>✅ Simple, scalable, no node features needed.</li>
            <li>❌ Transductive only (can't embed new nodes without retraining).</li>
            <li>❌ Ignores edge weights/direction in basic form.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Kernels and Similarity Measures</h2>
    <div class="content">
        <h3>What is a Graph Kernel?</h3>
        <p>A function \( \kappa(G_1, G_2) \) that measures similarity between two <strong>entire graphs</strong> by comparing substructures.</p>
        
        <h3>Common Graph Kernels</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Kernel</th>
                        <th>Substructure Compared</th>
                        <th>Complexity</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random Walk Kernel</strong></td>
                        <td>Counts matching walks</td>
                        <td>\( O(n^3) \); may diverge</td>
                    </tr>
                    <tr>
                        <td><strong>Shortest-Path Kernel</strong></td>
                        <td>Distribution of shortest-path lengths</td>
                        <td>\( O(n^2) \)</td>
                    </tr>
                    <tr>
                        <td><strong>Graphlet Kernel</strong></td>
                        <td>Counts of small connected subgraphs (e.g., triangles)</td>
                        <td>Exponential in graphlet size</td>
                    </tr>
                    <tr>
                        <td><strong>Weisfeiler-Lehman (WL) Kernel</strong></td>
                        <td>Refines node labels via neighborhood aggregation</td>
                        <td>\( O(h n) \), \( h \)=iterations</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>WL Kernel (Most Practical)</h3>
        <ul>
            <li>Simulates <strong>WL graph isomorphism test</strong>:
            <ol>
                <li>Initialize node labels (e.g., degree).</li>
                <li>Iteratively update label of \( v \):<br>
                \( l_v^{(k)} = \text{hash}(l_v^{(k-1)}, \{l_u^{(k-1)} : u \in \mathcal{N}(v)\}) \)</li>
                <li>Compare label distributions between graphs.</li>
            </ol>
            </li>
        </ul>
        
        <div class="definition">
            🔑 <strong>Connection to GNNs</strong>: WL test ≈ 1-layer GNN; expressive power of GNNs bounded by WL test.
        </div>
        
        <h3>Limitations</h3>
        <ul>
            <li>Kernels are <strong>computationally expensive</strong> for large graphs.</li>
            <li>Not end-to-end trainable with neural networks (unlike GNNs).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications of Graph Representation Learning</h2>
    <div class="content">
        <h3>(a) Node-Level Tasks</h3>
        <ul>
            <li><strong>Node classification</strong>: Predict labels (e.g., user interests in social networks).
            <ul>
                <li><em>Methods</em>: GCN, GAT, GraphSAGE.</li>
            </ul>
            </li>
            <li><strong>Link prediction</strong>: Predict missing/future edges (e.g., friend recommendation).
            <ul>
                <li><em>Approach</em>: Embed nodes → score pairs via dot product or MLP.</li>
            </ul>
            </li>
            <li><strong>Anomaly detection</strong>: Identify unusual nodes (e.g., fraud in transaction graphs).</li>
        </ul>
        
        <h3>(b) Graph-Level Tasks</h3>
        <ul>
            <li><strong>Graph classification</strong>: Predict property of entire graph (e.g., molecule toxicity).
            <ul>
                <li><em>Methods</em>: Graph kernels, GNNs with <strong>readout</strong> (e.g., sum/mean pooling).</li>
            </ul>
            </li>
            <li><strong>Graph similarity</strong>: Measure similarity between graphs (e.g., chemical compounds).</li>
        </ul>
        
        <h3>(c) Real-World Domains</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Domain</th>
                        <th>Application</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Social Networks</strong></td>
                        <td>Community detection, recommendation</td>
                        <td>Facebook friend suggestions</td>
                    </tr>
                    <tr>
                        <td><strong>Bioinformatics</strong></td>
                        <td>Protein function prediction, drug discovery</td>
                        <td>Predicting protein-protein interactions</td>
                    </tr>
                    <tr>
                        <td><strong>Knowledge Graphs</strong></td>
                        <td>Entity linking, question answering</td>
                        <td>Google Knowledge Graph</td>
                    </tr>
                    <tr>
                        <td><strong>Computer Vision</strong></td>
                        <td>Scene graph generation</td>
                        <td>Understanding object relationships in images</td>
                    </tr>
                    <tr>
                        <td><strong>Traffic/Transport</strong></td>
                        <td>Traffic forecasting</td>
                        <td>Predicting congestion using road networks</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>(d) Emerging Trends</h3>
        <ul>
            <li><strong>Heterogeneous graphs</strong>: Multiple node/edge types (e.g., academic graphs: authors, papers, venues).</li>
            <li><strong>Dynamic graphs</strong>: Evolving structure over time (e.g., Twitter retweets).</li>
            <li><strong>Large-scale GNNs</strong>: Scaling to billion-edge graphs (e.g., PinSage at Pinterest).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Idea</th>
                        <th>Strengths</th>
                        <th>Limitations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random Walks (DeepWalk/node2vec)</strong></td>
                        <td>Use walk context + Skip-gram</td>
                        <td>Scalable, no features needed</td>
                        <td>Transductive, ignores features</td>
                    </tr>
                    <tr>
                        <td><strong>Graph Kernels</strong></td>
                        <td>Compare substructures via kernel function</td>
                        <td>Theoretically grounded</td>
                        <td>Not differentiable, slow</td>
                    </tr>
                    <tr>
                        <td><strong>GNNs (e.g., GCN)</strong></td>
                        <td>Message passing + neural networks</td>
                        <td>Inductive, uses features, end-to-end</td>
                        <td>Over-smoothing, scalability</td>
                    </tr>
                    <tr>
                        <td><strong>WL Kernel</strong></td>
                        <td>Simulate graph isomorphism test</td>
                        <td>Strong theoretical link to GNNs</td>
                        <td>Fixed representation</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Graph representation learning</strong> bridges graph theory and deep learning.</li>
            <li><strong>Random walks</strong> enable NLP-inspired embedding methods for graphs.</li>
            <li><strong>Graph kernels</strong> provide powerful similarity measures but lack trainability.</li>
            <li><strong>GNNs</strong> unify structure and features in an end-to-end framework dominant in modern applications.</li>
            <li><strong>Applications span</strong> social science, biology, recommendation, and AI reasoning.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Rule of Thumb</strong>:<br>
            - Use <strong>node2vec</strong> for quick, feature-free embeddings.<br>
            - Use <strong>GNNs</strong> when node features exist or inductive learning is needed.<br>
            - Use <strong>graph kernels</strong> for small graphs with strong theoretical guarantees.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Neural Network Architectures</h2>
    <div class="content">
        <p><em>Message-Passing Frameworks for Relational Data</em></p>
        
        <div class="definition">
            <strong>Core Paradigm</strong>: <strong>Message Passing</strong><br>
            Each node aggregates information from its neighbors to update its representation:<br>
            \[ \mathbf{h}_v^{(k)} = \text{UPDATE}^{(k)} \left( \mathbf{h}_v^{(k-1)}, \, \text{AGGREGATE}^{(k)} \left( \{ \mathbf{h}_u^{(k-1)} : u \in \mathcal{N}(v) \} \right) \right) \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Convolutional Networks (GCNs)</h2>
    <div class="content">
        <p><em>(Kipf & Welling, ICLR 2017)</em></p>
        
        <h3>Key Idea</h3>
        <p>Generalize <strong>convolution</strong> from grids (images) to <strong>arbitrary graphs</strong> using <strong>spectral graph theory</strong> (simplified to spatial domain).</p>
        
        <h3>Layer-wise Propagation Rule</h3>
        <div class="equation">
            \[ \mathbf{H}^{(l+1)} = \sigma \left( \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1/2} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right) \]
        </div>
        <ul>
            <li>\( \tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I} \): Adjacency matrix with self-loops</li>
            <li>\( \tilde{\mathbf{D}} \): Degree matrix of \( \tilde{\mathbf{A}} \)</li>
            <li>\( \mathbf{W}^{(l)} \): Learnable weight matrix</li>
            <li>\( \sigma \): Non-linearity (e.g., ReLU)</li>
        </ul>
        
        <h3>Intuition</h3>
        <ul>
            <li><strong>Normalized adjacency</strong>: \( \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1/2} \) ensures stable aggregation (avoids numerical instability).</li>
            <li>Each node takes a <strong>weighted average</strong> of its own and neighbors' features.</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
            <li>Simple, effective, end-to-end trainable.</li>
            <li>Works well on <strong>homophilic graphs</strong> (connected nodes are similar).</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Transductive</strong>: Requires full graph at training (can't generalize to unseen nodes).</li>
            <li><strong>Fixed aggregation</strong>: All neighbors weighted equally (no attention).</li>
            <li><strong>Over-smoothing</strong>: Node representations become indistinguishable after many layers.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Semi-supervised node classification on citation networks (e.g., Cora, PubMed).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Attention Networks (GATs)</h2>
    <div class="content">
        <p><em>(Veličković et al., ICLR 2018)</em></p>
        
        <h3>Key Idea</h3>
        <p>Replace fixed aggregation in GCN with <strong>attention mechanisms</strong> → learn <strong>importance weights</strong> for neighbors.</p>
        
        <h3>Attention Mechanism</h3>
        <ol>
            <li><strong>Compute attention coefficients</strong> between node \(i\) and neighbor \(j\):<br>
            \[ e_{ij} = \mathbf{a}^\top [\mathbf{W} \mathbf{h}_i \, \| \, \mathbf{W} \mathbf{h}_j] \]
            where \( \| \) = concatenation, \( \mathbf{a} \) = attention vector.</li>
            
            <li><strong>Apply softmax</strong> over neighbors:<br>
            \[ \alpha_{ij} = \text{softmax}_j(e_{ij}) = \frac{\exp(\text{LeakyReLU}(e_{ij}))}{\sum_{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(e_{ik}))} \]</li>
            
            <li><strong>Aggregate</strong>:<br>
            \[ \mathbf{h}_i' = \sigma \left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W} \mathbf{h}_j \right) \]</li>
        </ol>
        
        <h3>Multi-Head Attention</h3>
        <p>Use \( K \) independent attention heads → concatenate or average outputs:<br>
        \[ \mathbf{h}_i' = \underset{k=1}{\overset{K}{\|}} \sigma \left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k \mathbf{h}_j \right) \]</p>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>Inductive</strong>: Can generalize to unseen graphs/nodes.</li>
            <li><strong>Interpretable</strong>: Attention weights reveal important neighbors.</li>
            <li>Handles <strong>heterophilic graphs</strong> better than GCN (not all neighbors are equal).</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li>Computationally heavier than GCN (due to attention computation).</li>
            <li>May overfit on small graphs.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Tasks requiring interpretability or handling diverse neighbor importance (e.g., recommendation, knowledge graphs).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">GraphSAGE and Inductive Learning</h2>
    <div class="content">
        <p><em>(Hamilton et al., NeurIPS 2017)</em></p>
        
        <h3>Key Idea</h3>
        <p><strong>Inductive framework</strong>: Learn <strong>aggregator functions</strong> that generalize to <strong>unseen nodes</strong> (no need for full graph at training).</p>
        
        <h3>Algorithm</h3>
        <ol>
            <li>Sample fixed-size neighborhood \( \mathcal{N}(v) \) (e.g., 10 neighbors).</li>
            <li><strong>Aggregate</strong> neighbor features using a learnable function:
            <ul>
                <li><strong>Mean</strong>: \( \text{AGG} = \text{mean}(\{ \mathbf{h}_u : u \in \mathcal{N}(v) \}) \)</li>
                <li><strong>LSTM</strong>: Order-sensitive aggregation</li>
                <li><strong>Pooling</strong>: \( \text{AGG} = \max( \{ \sigma(\mathbf{W} \mathbf{h}_u + \mathbf{b}) \} ) \)</li>
            </ul>
            </li>
            <li><strong>Combine</strong> with node's own features:<br>
            \[ \mathbf{h}_v^{(k)} = \sigma \left( \mathbf{W}^{(k)} \cdot \text{CONCAT}( \mathbf{h}_v^{(k-1)}, \, \text{AGG}^{(k)} ) \right) \]</li>
        </ol>
        
        <h3>Why Inductive?</h3>
        <p>Aggregators are <strong>parameterized functions</strong>, not graph-dependent → can embed <strong>new nodes</strong> at inference.</p>
        
        <h3>Strengths</h3>
        <ul>
            <li>Scales to <strong>large graphs</strong> (via neighbor sampling).</li>
            <li>Truly <strong>inductive</strong>: Works on dynamic or evolving graphs.</li>
            <li>Flexible aggregator design.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li>Sampling introduces stochasticity → higher variance.</li>
            <li>Fixed neighborhood size may miss long-range dependencies.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Large-scale industrial applications (e.g., PinSage at Pinterest for recommendation).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Isomorphism Networks (GINs)</h2>
    <div class="content">
        <p><em>(Xu et al., ICLR 2019)</em></p>
        
        <h3>Key Idea</h3>
        <p>Design a GNN as <strong>powerful as the Weisfeiler-Lehman (WL) graph isomorphism test</strong> the theoretical upper bound for message-passing GNNs.</p>
        
        <h3>WL Test Recap</h3>
        <p>Iteratively refines node labels based on multiset of neighbor labels.</p>
        <p>If two graphs have different label distributions → not isomorphic.</p>
        
        <h3>GIN Layer</h3>
        <div class="equation">
            \[ \mathbf{h}_v^{(k)} = \text{MLP}^{(k)} \left( (1 + \epsilon^{(k)}) \cdot \mathbf{h}_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} \mathbf{h}_u^{(k-1)} \right) \]
        </div>
        <ul>
            <li>\( \epsilon^{(k)} \): Learnable scalar (or fixed)</li>
            <li><strong>Sum aggregation</strong> (not mean/max) → injective over multisets</li>
            <li><strong>MLP</strong> ensures universal approximation</li>
        </ul>
        
        <h3>Why Sum Aggregation?</h3>
        <ul>
            <li>Mean/max are <strong>not injective</strong>: Different multisets can yield same mean/max.</li>
            <li><strong>Sum</strong> preserves full multiset information → matches WL test expressivity.</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>Maximally expressive</strong> among standard GNNs.</li>
            <li>Excellent for <strong>graph-level tasks</strong> (e.g., molecular property prediction).</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li>Overkill for simple node classification.</li>
            <li>Still limited by WL test (can't distinguish some non-isomorphic graphs).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Graph classification where structural fidelity is critical (e.g., quantum chemistry, molecule property prediction).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Temporal Graph Neural Networks (TGNNs)</h2>
    <div class="content">
        <h3>Problem Setting</h3>
        <ul>
            <li>Graph evolves over time: edges/nodes appear/disappear, features change.</li>
            <li><strong>Goal</strong>: Predict future links, node states, or graph properties.</li>
        </ul>
        
        <h3>Key Challenges</h3>
        <ul>
            <li><strong>Temporal dynamics</strong>: How to model time?</li>
            <li><strong>Causality</strong>: Future cannot influence past.</li>
            <li><strong>Irregular timestamps</strong>: Events occur at arbitrary times.</li>
        </ul>
        
        <h3>Architectures</h3>
        <h4>(a) Discrete-Time TGNNs</h4>
        <ul>
            <li><strong>Assumption</strong>: Time is divided into fixed intervals (e.g., days).</li>
            <li><strong>Approach</strong>: Stack GNNs over time slices + RNNs (e.g., GRU) to propagate state:<br>
            \[ \mathbf{h}_v^{(t)} = \text{GRU} \left( \mathbf{h}_v^{(t-1)}, \, \text{GNN}(\mathbf{X}^{(t)}, \mathbf{A}^{(t)}) \right) \]</li>
            <li><strong>Examples</strong>: DySAT, EvolveGCN.</li>
        </ul>
        
        <h4>(b) Continuous-Time TGNNs</h4>
        <ul>
            <li><strong>Model events</strong> (e.g., edge creation) at exact timestamps.</li>
            <li><strong>Approach</strong>: Use <strong>temporal point processes</strong> or <strong>ODEs</strong>.</li>
            <li><strong>Key model</strong>: <strong>TGAT</strong> (Temporal Graph Attention Network):
            <ul>
                <li>Embed time via <strong>time encodings</strong> (like Transformers).</li>
                <li>Use attention over <strong>temporal neighbors</strong> (past interactions).</li>
            </ul>
            </li>
            <li><strong>Other models</strong>: CAW (Causal Anonymous Walks), TGN (Temporal Graph Network).</li>
        </ul>
        
        <h3>Temporal Embedding Techniques</h3>
        <ul>
            <li><strong>Time encoding</strong>: \( \phi(\Delta t) = [\cos(\omega_1 \Delta t), \sin(\omega_1 \Delta t), \dots] \)</li>
            <li><strong>Memory modules</strong>: Store node state in external memory (e.g., TGN).</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li>Fraud detection (transaction graphs)</li>
            <li>Epidemic forecasting</li>
            <li>Social network evolution</li>
            <li>Traffic prediction</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Dynamic systems where timing matters (e.g., financial transactions, communication logs).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Architecture</th>
                        <th>Key Innovation</th>
                        <th>Inductive?</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>GCN</strong></td>
                        <td>Spectral convolution simplified</td>
                        <td>❌ (transductive)</td>
                        <td>Homophilic node classification</td>
                    </tr>
                    <tr>
                        <td><strong>GAT</strong></td>
                        <td>Attention-based neighbor weighting</td>
                        <td>✅</td>
                        <td>Interpretable, heterophilic graphs</td>
                    </tr>
                    <tr>
                        <td><strong>GraphSAGE</strong></td>
                        <td>Neighbor sampling + learnable aggregators</td>
                        <td>✅</td>
                        <td>Large-scale, dynamic graphs</td>
                    </tr>
                    <tr>
                        <td><strong>GIN</strong></td>
                        <td>Maximally expressive (WL-equivalent)</td>
                        <td>✅</td>
                        <td>Graph classification, molecules</td>
                    </tr>
                    <tr>
                        <td><strong>Temporal GNNs</strong></td>
                        <td>Model time + structure jointly</td>
                        <td>✅</td>
                        <td>Evolving graphs, event prediction</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>GCN</strong> is the foundational spatial GNN but limited by fixed aggregation.</li>
            <li><strong>GAT</strong> adds flexibility and interpretability via attention.</li>
            <li><strong>GraphSAGE</strong> enables <strong>scalable inductive learning</strong> critical for real-world systems.</li>
            <li><strong>GIN</strong> sets the <strong>theoretical expressivity limit</strong> for message-passing GNNs.</li>
            <li><strong>Temporal GNNs</strong> extend GNNs to <strong>dynamic settings</strong> essential for real-world applications.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Design Tip</strong>:<br>
            - For <strong>static, small graphs</strong>: GCN or GAT.<br>
            - For <strong>large or evolving graphs</strong>: GraphSAGE or Temporal GNNs.<br>
            - For <strong>graph-level prediction</strong>: GIN.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Graph Learning Applications</h2>
    <div class="content">
        <p><em>From Prediction to Generation on Relational Data</em></p>
        
        <h3>Node Classification and Link Prediction</h3>
        <h4>Node Classification</h4>
        <ul>
            <li><strong>Goal</strong>: Predict labels for <strong>unlabeled nodes</strong> in a graph (e.g., user interests, protein functions).</li>
            <li><strong>Assumption</strong>: <strong>Homophily</strong>   connected nodes tend to share labels.</li>
            <li><strong>Input</strong>: Graph \( G = (V, E) \), node features \( \mathbf{X} \), labels for a subset \( V_{\text{train}} \subset V \).</li>
            <li><strong>Output</strong>: Labels for \( V \setminus V_{\text{train}} \).</li>
        </ul>
        
        <h4>Approaches</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Type</th>
                        <th>Notes</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>GNNs (GCN, GAT, GraphSAGE)</strong></td>
                        <td>End-to-end</td>
                        <td>State-of-the-art; use both structure and features</td>
                    </tr>
                    <tr>
                        <td><strong>Label Propagation</strong></td>
                        <td>Classical</td>
                        <td>Iteratively smooth labels over edges; no features needed</td>
                    </tr>
                    <tr>
                        <td><strong>Random Walks (node2vec + classifier)</strong></td>
                        <td>Embedding-based</td>
                        <td>Transductive; ignores node features</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Social networks: User profiling<br>
            - Biology: Protein function annotation<br>
            - Fraud detection: Flag suspicious accounts
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Link Prediction</h2>
    <div class="content">
        <ul>
            <li><strong>Goal</strong>: Predict <strong>missing or future edges</strong> (e.g., "Will these two users connect?").</li>
            <li><strong>Input</strong>: Observed graph \( G \); possibly node features.</li>
            <li><strong>Output</strong>: Probability of edge existence for node pairs \( (u, v) \notin E \).</li>
        </ul>
        
        <h3>Scoring Functions</h3>
        <p>Given node embeddings \( \mathbf{z}_u, \mathbf{z}_v \):</p>
        <ul>
            <li><strong>Dot product</strong>: \( s(u,v) = \mathbf{z}_u^\top \mathbf{z}_v \)</li>
            <li><strong>Cosine similarity</strong>: \( s(u,v) = \frac{\mathbf{z}_u^\top \mathbf{z}_v}{\|\mathbf{z}_u\| \|\mathbf{z}_v\|} \)</li>
            <li><strong>MLP</strong>: \( s(u,v) = \text{MLP}([\mathbf{z}_u \| \mathbf{z}_v]) \)</li>
        </ul>
        
        <h3>Methods</h3>
        <ul>
            <li><strong>GNN-based</strong>: Use GNN to get embeddings → score pairs.</li>
            <li><strong>Matrix factorization</strong>: Factorize adjacency matrix \( A \approx Z Z^\top \).</li>
            <li><strong>Heuristics</strong>: Common neighbors, Jaccard index, Adamic-Adar.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Recommendation systems (e.g., "People you may know")<br>
            - Knowledge graph completion (e.g., predicting missing facts)<br>
            - Drug-target interaction prediction
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Classification and Regression</h2>
    <div class="content">
        <h3>Graph Classification</h3>
        <ul>
            <li><strong>Goal</strong>: Assign a label to an <strong>entire graph</strong> (e.g., "Is this molecule toxic?").</li>
            <li><strong>Input</strong>: Set of graphs \( \{G_1, G_2, \dots, G_N\} \), each with node/edge features.</li>
            <li><strong>Output</strong>: Class label per graph.</li>
        </ul>
        
        <h4>Pipeline</h4>
        <ol>
            <li><strong>Node representation</strong>: Use GNN layers to get \( \mathbf{h}_v \) for all \( v \in G \).</li>
            <li><strong>Readout (graph pooling)</strong>: Aggregate node embeddings into graph-level vector \( \mathbf{h}_G \):
            <ul>
                <li><strong>Mean/sum pooling</strong>: \( \mathbf{h}_G = \sum_{v \in V} \mathbf{h}_v \)</li>
                <li><strong>Set2Set</strong>, <strong>DiffPool</strong>, <strong>SAGPool</strong>: Learnable pooling</li>
            </ul>
            </li>
            <li><strong>Classifier</strong>: MLP on \( \mathbf{h}_G \) → output label.</li>
        </ol>
        
        <h4>Key Models</h4>
        <ul>
            <li><strong>GIN</strong>: Maximally expressive for graph classification.</li>
            <li><strong>DGCNN</strong>: Sorts node embeddings before pooling.</li>
            <li><strong>Graph kernels</strong>: WL kernel for small graphs.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Chemistry: Molecular property prediction (e.g., solubility, toxicity)<br>
            - Program analysis: Detecting malware in code graphs<br>
            - Social network analysis: Identifying community types
        </div>
        
        <h3>Graph Regression</h3>
        <ul>
            <li>Same as classification, but output is <strong>continuous</strong> (e.g., molecular energy, graph diameter).</li>
            <li>Replace final classifier with <strong>regressor</strong> (e.g., linear layer).</li>
            <li>Loss: Mean Squared Error (MSE) or MAE.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Quantum chemistry: Predicting molecular energy (QM9 dataset)<br>
            - Traffic: Estimating total congestion in a road network
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Generation and Synthesis</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Learn distribution \( p(G) \) over graphs → generate <strong>new, realistic graphs</strong>.</p>
        
        <h3>Applications</h3>
        <ul>
            <li>Drug discovery: Generate novel molecules with desired properties.</li>
            <li>Circuit design: Create efficient chip layouts.</li>
            <li>Privacy: Synthesize anonymized social networks.</li>
        </ul>
        
        <h3>Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Strengths/Limitations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>VAE-based (GraphVAE)</strong></td>
                        <td>Encode graph → latent vector → decode to adjacency matrix</td>
                        <td>Handles small graphs; struggles with discreteness</td>
                    </tr>
                    <tr>
                        <td><strong>Autoregressive (GraphRNN)</strong></td>
                        <td>Generate nodes/edges sequentially using RNNs</td>
                        <td>Captures structure; slow, order-dependent</td>
                    </tr>
                    <tr>
                        <td><strong>GAN-based (MolGAN)</strong></td>
                        <td>Generator creates graphs; discriminator critiques</td>
                        <td>Mode collapse; hard to train</td>
                    </tr>
                    <tr>
                        <td><strong>Score-based / Diffusion</strong></td>
                        <td>Learn gradient of log-density → sample via Langevin dynamics</td>
                        <td>State-of-the-art quality; computationally heavy</td>
                    </tr>
                    <tr>
                        <td><strong>Rule-based (GrammarVAE)</strong></td>
                        <td>Use formal grammars (e.g., SMILES for molecules)</td>
                        <td>Valid structures guaranteed; limited expressivity</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Discrete structure</strong>: Gradients don't flow through adjacency matrices.</li>
            <li><strong>Validity</strong>: Generated graphs must obey domain constraints (e.g., chemical valency).</li>
            <li><strong>Evaluation</strong>: No standard metric; use <strong>MMD</strong> (Maximum Mean Discrepancy) on graph statistics.</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Cutting-edge</strong>: <strong>3D molecular generation</strong> (e.g., GeoDiff) combines graph + geometry.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Dynamic Graph Learning</h2>
    <div class="content">
        <h3>Problem Setting</h3>
        <ul>
            <li>Graph changes over time: nodes/edges appear/disappear, features evolve.</li>
            <li><strong>Two types</strong>:
            <ul>
                <li><strong>Discrete-time</strong>: Snapshots at fixed intervals (e.g., daily social network).</li>
                <li><strong>Continuous-time</strong>: Events at arbitrary timestamps (e.g., financial transactions).</li>
            </ul>
            </li>
        </ul>
        
        <h3>Tasks</h3>
        <ul>
            <li><strong>Temporal node classification</strong>: Predict node label at time \( t \).</li>
            <li><strong>Dynamic link prediction</strong>: Predict future edges.</li>
            <li><strong>Anomaly detection</strong>: Spot unusual activity (e.g., fraud bursts).</li>
        </ul>
        
        <h3>Key Architectures</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Mechanism</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>TGAT</strong></td>
                        <td>Temporal self-attention + time encoding</td>
                        <td>Continuous-time event prediction</td>
                    </tr>
                    <tr>
                        <td><strong>TGN</strong></td>
                        <td>Memory module + message passing</td>
                        <td>Streaming graphs</td>
                    </tr>
                    <tr>
                        <td><strong>DySAT</strong></td>
                        <td>Self-attention over time + structure</td>
                        <td>Discrete snapshots</td>
                    </tr>
                    <tr>
                        <td><strong>CAW</strong></td>
                        <td>Causal anonymous walks</td>
                        <td>Irregular timestamps</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Temporal Encoding</h3>
        <p>Inject time into embeddings:</p>
        <div class="equation">
            \[ \phi(\Delta t) = [\cos(\omega_1 \Delta t), \sin(\omega_1 \Delta t), \dots, \cos(\omega_d \Delta t), \sin(\omega_d \Delta t)] \]
        </div>
        <p>(similar to Transformer positional encoding)</p>
        
        <div class="definition">
            🌐 <strong>Use cases</strong>:<br>
            - Cybersecurity: Detecting attack patterns in network traffic<br>
            - E-commerce: Real-time recommendation on evolving user-item graphs<br>
            - Epidemiology: Modeling disease spread
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Scalability Challenges in Graph Learning</h2>
    <div class="content">
        <h3>Why Graphs Don't Scale Easily</h3>
        <ul>
            <li><strong>Irregular structure</strong>: No fixed grid → hard to batch.</li>
            <li><strong>Neighborhood explosion</strong>: \( k \)-hop neighborhood grows exponentially.</li>
            <li><strong>Memory bottleneck</strong>: Storing full adjacency matrix is \( O(n^2) \).</li>
        </ul>
        
        <h3>Key Challenges</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Challenge</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Full-batch training</strong></td>
                        <td>GCN requires full graph → infeasible for large graphs (e.g., >1M nodes)</td>
                    </tr>
                    <tr>
                        <td><strong>Neighbor explosion</strong></td>
                        <td>Aggregating 2-hop neighbors of a node may include entire graph</td>
                    </tr>
                    <tr>
                        <td><strong>Heterogeneous hardware</strong></td>
                        <td>Sparse operations don't leverage GPUs efficiently</td>
                    </tr>
                    <tr>
                        <td><strong>Dynamic graphs</strong></td>
                        <td>Continual updates require online learning</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Scalability Solutions</h3>
        <h4>(a) Sampling-Based Methods</h4>
        <ul>
            <li><strong>Neighbor sampling</strong> (GraphSAGE): Sample fixed number of neighbors per layer.</li>
            <li><strong>Layer sampling</strong> (FastGCN): Sample nodes per layer independently.</li>
            <li><strong>Subgraph sampling</strong> (Cluster-GCN): Train on graph partitions.</li>
        </ul>
        
        <h4>(b) Precomputation & Compression</h4>
        <ul>
            <li><strong>Precompute embeddings</strong>: Use unsupervised methods (e.g., node2vec) → train classifier separately.</li>
            <li><strong>Graph coarsening</strong>: Merge nodes to create hierarchy (e.g., METIS).</li>
        </ul>
        
        <h4>(c) System Optimizations</h4>
        <ul>
            <li><strong>Sparse tensor libraries</strong>: DGL, PyG optimize sparse-dense ops.</li>
            <li><strong>In-memory graph stores</strong>: Use CSR/CSC formats for fast access.</li>
            <li><strong>Distributed training</strong>: Partition graph across machines (e.g., AliGraph, Euler).</li>
        </ul>
        
        <h4>(d) Inductive Architectures</h4>
        <p>Models like <strong>GraphSAGE</strong> and <strong>GAT</strong> avoid full-graph dependency → naturally scalable.</p>
        
        <div class="definition">
            📊 <strong>Scalability Benchmarks</strong>:<br>
            - <strong>OGB-Large</strong>: Datasets with 100M+ edges.<br>
            - <strong>Twitter Graph</strong>: 1.5B nodes, 35B edges → requires distributed GNNs.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Application</th>
                        <th>Key Task</th>
                        <th>Representative Models</th>
                        <th>Real-World Use</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Node Classification</strong></td>
                        <td>Label nodes</td>
                        <td>GCN, GAT, GraphSAGE</td>
                        <td>User profiling, fraud detection</td>
                    </tr>
                    <tr>
                        <td><strong>Link Prediction</strong></td>
                        <td>Predict edges</td>
                        <td>SEAL, GNN + MLP</td>
                        <td>Recommendation, KG completion</td>
                    </tr>
                    <tr>
                        <td><strong>Graph Classification</strong></td>
                        <td>Label graphs</td>
                        <td>GIN, DiffPool</td>
                        <td>Drug discovery, program analysis</td>
                    </tr>
                    <tr>
                        <td><strong>Graph Generation</strong></td>
                        <td>Create new graphs</td>
                        <td>GraphRNN, MolGAN, Diffusion</td>
                        <td>Molecule design, synthetic data</td>
                    </tr>
                    <tr>
                        <td><strong>Dynamic Learning</strong></td>
                        <td>Model time + graph</td>
                        <td>TGAT, TGN, CAW</td>
                        <td>Financial fraud, traffic forecasting</td>
                    </tr>
                    <tr>
                        <td><strong>Scalability</strong></td>
                        <td>Handle large graphs</td>
                        <td>GraphSAGE, Cluster-GCN</td>
                        <td>Web-scale recommendation</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Node/link tasks</strong> dominate industrial applications (e.g., recommendation).</li>
            <li><strong>Graph classification</strong> is crucial in scientific domains (e.g., chemistry).</li>
            <li><strong>Graph generation</strong> is emerging but faces validity and evaluation hurdles.</li>
            <li><strong>Dynamic graphs</strong> require specialized architectures that respect causality.</li>
            <li><strong>Scalability</strong> is the #1 barrier to real-world deployment sampling and system co-design are essential.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Future Directions</strong>:<br>
            - <strong>Foundation models for graphs</strong> (e.g., pre-trained GNNs)<br>
            - <strong>3D+temporal graph learning</strong> (e.g., protein folding over time)<br>
            - <strong>Privacy-preserving graph learning</strong> (federated GNNs)
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Cybersecurity Fundamentals</h2>
    <div class="content">
        <h3>Network Security</h3>
        <div class="definition">
            <strong>Goal</strong>: Protect the <strong>confidentiality, integrity, and availability (CIA triad)</strong> of data and resources as they traverse networks.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Network Protocols and Vulnerabilities</h2>
    <div class="content">
        <h3>Key Protocols & Associated Risks</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Protocol</th>
                        <th>Purpose</th>
                        <th>Common Vulnerabilities</th>
                        <th>Mitigations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>TCP/IP</strong></td>
                        <td>Core internet protocol suite</td>
                        <td>IP spoofing, SYN flood (DoS), session hijacking</td>
                        <td>Ingress/egress filtering, SYN cookies</td>
                    </tr>
                    <tr>
                        <td><strong>HTTP</strong></td>
                        <td>Web communication</td>
                        <td>Eavesdropping, cookie theft, XSS</td>
                        <td>Use <strong>HTTPS</strong> (HTTP over TLS)</td>
                    </tr>
                    <tr>
                        <td><strong>DNS</strong></td>
                        <td>Domain name resolution</td>
                        <td>DNS spoofing, cache poisoning, DDoS on DNS servers</td>
                        <td>DNSSEC, DoH (DNS over HTTPS), Anycast DNS</td>
                    </tr>
                    <tr>
                        <td><strong>ARP</strong></td>
                        <td>Map IP → MAC address</td>
                        <td><strong>ARP spoofing</strong> (Man-in-the-Middle)</td>
                        <td>Static ARP entries, DHCP snooping, port security</td>
                    </tr>
                    <tr>
                        <td><strong>SMTP/POP3/IMAP</strong></td>
                        <td>Email transfer/retrieval</td>
                        <td>Credential sniffing, email spoofing</td>
                        <td>Enforce <strong>STARTTLS</strong>, SPF/DKIM/DMARC</td>
                    </tr>
                    <tr>
                        <td><strong>SNMP</strong></td>
                        <td>Network device monitoring</td>
                        <td>Default community strings (e.g., "public"), info leakage</td>
                        <td>Disable v1/v2; use SNMPv3 with auth/encryption</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Common Attack Vectors</h3>
        <ul>
            <li><strong>Man-in-the-Middle (MitM)</strong>: Attacker intercepts/modifies traffic (e.g., via ARP spoofing).</li>
            <li><strong>Denial-of-Service (DoS/DDoS)</strong>: Overwhelm network resources (e.g., SYN flood, UDP flood).</li>
            <li><strong>Reconnaissance</strong>: Scanning (Nmap), banner grabbing → map network for weaknesses.</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Defense Principle</strong>: <strong>Encrypt everything</strong> (TLS 1.2+), disable legacy protocols (SSLv3, Telnet), and apply least privilege.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Firewall Principles and Configurations</h2>
    <div class="content">
        <h3>What is a Firewall?</h3>
        <p>A <strong>network security device</strong> that monitors and filters incoming/outgoing traffic based on <strong>predefined rules</strong>.</p>
        
        <h3>Types of Firewalls</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>How It Works</th>
                        <th>Pros</th>
                        <th>Cons</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Packet-Filtering</strong></td>
                        <td>Filters based on IP, port, protocol</td>
                        <td>Fast, low overhead</td>
                        <td>No state awareness; easily bypassed</td>
                    </tr>
                    <tr>
                        <td><strong>Stateful Inspection</strong></td>
                        <td>Tracks active connections (state table)</td>
                        <td>Blocks unsolicited traffic; more secure</td>
                        <td>Higher resource use</td>
                    </tr>
                    <tr>
                        <td><strong>Proxy Firewall (Application-Level)</strong></td>
                        <td>Acts as intermediary; inspects Layer 7 (app data)</td>
                        <td>Deep inspection; hides internal IPs</td>
                        <td>Latency; app-specific</td>
                    </tr>
                    <tr>
                        <td><strong>Next-Generation Firewall (NGFW)</strong></td>
                        <td>Combines stateful + app awareness + IPS + identity</td>
                        <td>Granular control (user/app-based policies)</td>
                        <td>Complex; expensive</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Firewall Configuration Best Practices</h3>
        <ul>
            <li><strong>Default-deny policy</strong>: Block all traffic unless explicitly allowed.</li>
            <li><strong>Least privilege</strong>: Only open necessary ports (e.g., 443 for HTTPS, not 80).</li>
            <li><strong>Rule ordering</strong>: Place specific rules before general ones.</li>
            <li><strong>Logging & monitoring</strong>: Enable logs for denied/allowed traffic.</li>
            <li><strong>Regular audits</strong>: Remove obsolete rules ("rule bloat" increases risk).</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Example Rule</strong>:<br>
            <code>ALLOW TCP 192.168.1.0/24 → ANY on port 443</code><br>
            <code>DENY ALL</code>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Intrusion Detection/Prevention Systems (IDS/IPS)</h2>
    <div class="content">
        <h3>IDS vs. IPS</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>IDS (Intrusion Detection System)</th>
                        <th>IPS (Intrusion Prevention System)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Action</strong></td>
                        <td><strong>Detects</strong> and alerts</td>
                        <td><strong>Detects + blocks</strong> malicious traffic</td>
                    </tr>
                    <tr>
                        <td><strong>Placement</strong></td>
                        <td>Passive (SPAN/mirror port)</td>
                        <td>Inline (in traffic path)</td>
                    </tr>
                    <tr>
                        <td><strong>Risk</strong></td>
                        <td>No impact on traffic if fails</td>
                        <td>Failure = network downtime</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Monitoring, forensics</td>
                        <td>Real-time threat blocking</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Detection Methods</h3>
        <ul>
            <li><strong>Signature-Based</strong>: Matches known attack patterns (e.g., Snort rules).<br>
            → Fast, low false positives; misses zero-days.</li>
            <li><strong>Anomaly-Based</strong>: Learns "normal" behavior; flags deviations.<br>
            → Catches novel attacks; high false positives.</li>
            <li><strong>Heuristic/Behavioral</strong>: Analyzes code behavior (e.g., sandboxing).</li>
        </ul>
        
        <h3>Deployment Modes</h3>
        <ul>
            <li><strong>Network-based (NIDS/NIPS)</strong>: Monitors entire network segment.</li>
            <li><strong>Host-based (HIDS/HIPS)</strong>: Installed on endpoints (e.g., OSSEC, CrowdStrike).</li>
        </ul>
        
        <div class="definition">
            🚨 <strong>Best Practice</strong>: Use <strong>IDS + IPS in tandem</strong> IDS for visibility, IPS for enforcement.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Virtual Private Networks (VPNs)</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Create a <strong>secure, encrypted tunnel</strong> over public networks (e.g., internet) to protect data in transit.</p>
        
        <h3>Types of VPNs</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Use Case</th>
                        <th>Protocols</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Remote Access VPN</strong></td>
                        <td>Individual users → corporate network</td>
                        <td>SSL/TLS (e.g., OpenVPN), IPsec</td>
                    </tr>
                    <tr>
                        <td><strong>Site-to-Site VPN</strong></td>
                        <td>Connect two networks (e.g., branch office ↔ HQ)</td>
                        <td>IPsec (IKEv2), GRE over IPsec</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Protocols</h3>
        <ul>
            <li><strong>IPsec</strong>: Suite of protocols (AH, ESP, IKE) for secure IP communication.
            <ul>
                <li><strong>Transport mode</strong>: Encrypts payload only (host-to-host).</li>
                <li><strong>Tunnel mode</strong>: Encrypts entire IP packet (network-to-network).</li>
            </ul>
            </li>
            <li><strong>SSL/TLS VPN</strong>: Uses web browser; no client install needed (e.g., Cisco AnyConnect).</li>
            <li><strong>WireGuard</strong>: Modern, lightweight, high-performance (uses Curve25519, ChaCha20).</li>
        </ul>
        
        <h3>Security Considerations</h3>
        <ul>
            <li><strong>Strong authentication</strong>: Use certificates or MFA (not just passwords).</li>
            <li><strong>Perfect Forward Secrecy (PFS)</strong>: Ensure session keys aren't compromised if master key is.</li>
            <li><strong>Avoid PPTP/L2TP without IPsec</strong>: Broken encryption (e.g., MS-CHAPv2).</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Zero Trust Note</strong>: Traditional VPNs grant broad network access → modern trend is <strong>Zero Trust Network Access (ZTNA)</strong> (e.g., Cloudflare Access, Zscaler).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Network Segmentation and Micro-Segmentation</h2>
    <div class="content">
        <h3>Why Segment?</h3>
        <ul>
            <li>Limit lateral movement during breaches ("assume breach" model).</li>
            <li>Contain threats (e.g., ransomware).</li>
            <li>Enforce least privilege.</li>
        </ul>
        
        <h3>Network Segmentation</h3>
        <ul>
            <li><strong>Definition</strong>: Divide network into <strong>subnets/VLANs</strong> based on function (e.g., HR, Finance, IoT).</li>
            <li><strong>Tools</strong>: Routers, VLANs, firewalls.</li>
            <li><strong>Example</strong>:
            <ul>
                <li>VLAN 10: Users</li>
                <li>VLAN 20: Servers</li>
                <li>VLAN 30: Guest Wi-Fi</li>
                <li>→ Firewall rules restrict inter-VLAN traffic.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Micro-Segmentation</h3>
        <ul>
            <li><strong>Definition</strong>: Apply security policies at <strong>individual workload level</strong> (e.g., per VM, container, app).</li>
            <li><strong>Granularity</strong>: "East-west" traffic (within data center) is controlled as tightly as "north-south".</li>
            <li><strong>Implementation</strong>:
            <ul>
                <li><strong>Software-defined</strong>: VMware NSX, Illumio, AWS Security Groups.</li>
                <li><strong>Policy example</strong>: "Web server can talk to DB on port 3306, but nothing else."</li>
            </ul>
            </li>
        </ul>
        
        <h3>Benefits</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Scope</th>
                        <th>Flexibility</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Network Segmentation</strong></td>
                        <td>Subnet/VLAN level</td>
                        <td>Moderate</td>
                        <td>Traditional data centers</td>
                    </tr>
                    <tr>
                        <td><strong>Micro-Segmentation</strong></td>
                        <td>Per-workload</td>
                        <td>High</td>
                        <td>Cloud, containers, Zero Trust</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🌐 <strong>Zero Trust Alignment</strong>: Micro-segmentation enforces "never trust, always verify" for internal traffic.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Core Purpose</th>
                        <th>Key Tools/Protocols</th>
                        <th>Best Practice</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Network Protocols</strong></td>
                        <td>Enable communication</td>
                        <td>TCP/IP, DNS, HTTP</td>
                        <td>Disable insecure protocols; enforce encryption</td>
                    </tr>
                    <tr>
                        <td><strong>Firewalls</strong></td>
                        <td>Filter traffic</td>
                        <td>NGFW, iptables, Palo Alto</td>
                        <td>Default-deny; least privilege</td>
                    </tr>
                    <tr>
                        <td><strong>IDS/IPS</strong></td>
                        <td>Detect/block attacks</td>
                        <td>Snort (IDS), Suricata (IPS)</td>
                        <td>Combine signature + anomaly detection</td>
                    </tr>
                    <tr>
                        <td><strong>VPNs</strong></td>
                        <td>Secure remote access</td>
                        <td>IPsec, WireGuard, OpenVPN</td>
                        <td>Use MFA; prefer ZTNA for new deployments</td>
                    </tr>
                    <tr>
                        <td><strong>Segmentation</strong></td>
                        <td>Limit breach impact</td>
                        <td>VLANs, NSX, Security Groups</td>
                        <td>Start with coarse segmentation → refine to micro</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Defense in depth</strong>: Combine firewalls, IDS/IPS, segmentation, and encryption.</li>
            <li><strong>Assume breach</strong>: Segment networks to contain attackers.</li>
            <li><strong>Modern shift</strong>: From perimeter-based (firewall + VPN) to <strong>Zero Trust</strong> (micro-segmentation + ZTNA).</li>
            <li><strong>Visibility is critical</strong>: Log and monitor all network traffic.</li>
            <li><strong>Patch and harden</strong>: Keep network devices (routers, switches) updated.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>: Always test firewall/IPS rules in <strong>monitor mode</strong> before enforcing to avoid outages.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Threat Landscape</h2>
    <div class="content">
        <p><em>Understanding Modern Cyber Threats and Attack Vectors</em></p>
        
        <div class="definition">
            <strong>Goal</strong>: Identify, classify, and defend against evolving cyber threats that target individuals, organizations, and critical infrastructure.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Malware Types and Characteristics</h2>
    <div class="content">
        <p><strong>Malware</strong> (malicious software) is designed to disrupt, damage, or gain unauthorized access to systems.</p>
        
        <h3>Common Malware Types</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Behavior</th>
                        <th>Delivery Method</th>
                        <th>Example/Notable Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Virus</strong></td>
                        <td>Attaches to legitimate files; requires user execution</td>
                        <td>Email attachments, infected USBs</td>
                        <td>ILOVEYOU (2000)</td>
                    </tr>
                    <tr>
                        <td><strong>Worm</strong></td>
                        <td>Self-replicating; spreads without user interaction</td>
                        <td>Network vulnerabilities, email</td>
                        <td>WannaCry (used EternalBlue exploit)</td>
                    </tr>
                    <tr>
                        <td><strong>Trojan</strong></td>
                        <td>Disguised as legitimate software; creates backdoors</td>
                        <td>Fake installers, pirated software</td>
                        <td>Emotet, Zeus</td>
                    </tr>
                    <tr>
                        <td><strong>Ransomware</strong></td>
                        <td>Encrypts files/data; demands payment for decryption</td>
                        <td>Phishing, RDP brute-force</td>
                        <td>LockBit, REvil, Colonial Pipeline attack (2021)</td>
                    </tr>
                    <tr>
                        <td><strong>Spyware</strong></td>
                        <td>Secretly monitors user activity (keystrokes, browsing)</td>
                        <td>Bundled software, drive-by downloads</td>
                        <td>Pegasus (NSO Group)</td>
                    </tr>
                    <tr>
                        <td><strong>Adware</strong></td>
                        <td>Displays unwanted ads; may track behavior</td>
                        <td>Free software bundles</td>
                        <td>Not always malicious, but privacy-invasive</td>
                    </tr>
                    <tr>
                        <td><strong>Rootkit</strong></td>
                        <td>Hides presence of malware; gains privileged access</td>
                        <td>Exploits, Trojans</td>
                        <td>Sony BMG rootkit (2005)</td>
                    </tr>
                    <tr>
                        <td><strong>Fileless Malware</strong></td>
                        <td>Resides in memory (RAM); uses legitimate tools (e.g., PowerShell)</td>
                        <td>Phishing, macros</td>
                        <td>Used in APT attacks; evades disk-based AV</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Characteristics</h3>
        <ul>
            <li><strong>Persistence</strong>: Survives reboots (e.g., via registry keys, scheduled tasks).</li>
            <li><strong>Evasion</strong>: Uses packing, obfuscation, or polymorphism to avoid detection.</li>
            <li><strong>Command & Control (C2)</strong>: Communicates with attacker-controlled server for instructions.</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Defense</strong>:<br>
            - Keep systems patched<br>
            - Use EDR (Endpoint Detection & Response)<br>
            - Apply principle of least privilege<br>
            - Regular backups (for ransomware recovery)
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Advanced Persistent Threats (APTs)</h2>
    <div class="content">
        <h3>What is an APT?</h3>
        <p><strong>Sophisticated, long-term cyber campaigns</strong> conducted by <strong>well-resourced actors</strong> (nation-states, organized crime).</p>
        <p><strong>Goals</strong>: Espionage, data theft, sabotage (not quick financial gain).</p>
        
        <h3>APT Lifecycle (Kill Chain)</h3>
        <ol>
            <li><strong>Reconnaissance</strong>: Gather intel on target (employees, tech stack).</li>
            <li><strong>Initial Access</strong>: Phishing, zero-day exploit, or supply chain compromise.</li>
            <li><strong>Execution</strong>: Run malware (often fileless).</li>
            <li><strong>Persistence</strong>: Establish backdoors, create accounts.</li>
            <li><strong>Privilege Escalation</strong>: Gain admin/root access.</li>
            <li><strong>Defense Evasion</strong>: Disable logging, AV, use living-off-the-land (LOTL) techniques.</li>
            <li><strong>Lateral Movement</strong>: Move across network (e.g., via Pass-the-Hash).</li>
            <li><strong>Exfiltration</strong>: Steal data slowly to avoid detection.</li>
        </ol>
        
        <h3>Notable APT Groups</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Group</th>
                        <th>Attribution</th>
                        <th>Targets</th>
                        <th>Tactics</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>APT29 (Cozy Bear)</strong></td>
                        <td>Russia (SVR)</td>
                        <td>Government, healthcare</td>
                        <td>Spear-phishing, ZeroLogon exploit</td>
                    </tr>
                    <tr>
                        <td><strong>APT28 (Fancy Bear)</strong></td>
                        <td>Russia (GRU)</td>
                        <td>Political orgs, Olympics</td>
                        <td>Credential theft, fake domains</td>
                    </tr>
                    <tr>
                        <td><strong>Lazarus Group</strong></td>
                        <td>North Korea</td>
                        <td>Financial, crypto</td>
                        <td>SWIFT banking attacks, ransomware</td>
                    </tr>
                    <tr>
                        <td><strong>Equation Group</strong></td>
                        <td>USA (NSA-linked)</td>
                        <td>Global telecoms, hard drive firmware</td>
                        <td>Firmware-level rootkits</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🔍 <strong>Detection Challenges</strong>:<br>
            - Operate slowly ("low and slow")<br>
            - Use custom, undetected tools<br>
            - Blend with normal traffic
        </div>
        
        <div class="definition">
            🛡️ <strong>Mitigation</strong>:<br>
            - Network segmentation<br>
            - User behavior analytics (UEBA)<br>
            - Threat intelligence sharing<br>
            - Assume breach; hunt for anomalies
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Zero-Day Exploits and Mitigation</h2>
    <div class="content">
        <h3>What is a Zero-Day?</h3>
        <p>A <strong>previously unknown vulnerability</strong> with <strong>no patch available</strong>.</p>
        <p>"Zero-day" = 0 days between discovery by attacker and vendor awareness.</p>
        
        <h3>Why Dangerous?</h3>
        <ul>
            <li>No signature-based detection possible.</li>
            <li>High success rate (no defenses in place).</li>
            <li>Often sold on dark web for $100K–$2M (e.g., iOS zero-days).</li>
        </ul>
        
        <h3>Famous Examples</h3>
        <ul>
            <li><strong>Stuxnet (2010)</strong>: Used 4 zero-days to sabotage Iranian centrifuges.</li>
            <li><strong>SolarWinds (2020)</strong>: Supply chain attack via compromised update (zero-day in build system).</li>
            <li><strong>Log4Shell (2021)</strong>: Critical RCE in Log4j (CVE-2021-44228); exploited within hours.</li>
        </ul>
        
        <h3>Mitigation Strategies</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Virtual Patching</strong></td>
                        <td>Use WAF/IPS to block exploit patterns before patch is applied</td>
                    </tr>
                    <tr>
                        <td><strong>Attack Surface Reduction</strong></td>
                        <td>Disable unused features (e.g., macros, scripting)</td>
                    </tr>
                    <tr>
                        <td><strong>Memory Protections</strong></td>
                        <td>DEP, ASLR, stack canaries to prevent code execution</td>
                    </tr>
                    <tr>
                        <td><strong>Sandboxing</strong></td>
                        <td>Run untrusted code in isolated environments</td>
                    </tr>
                    <tr>
                        <td><strong>Threat Hunting</strong></td>
                        <td>Proactively search for indicators of compromise (IOCs)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            💡 <strong>Best Practice</strong>:<br>
            - Subscribe to vulnerability feeds (CISA KEV, NVD)<br>
            - Prioritize patching based on exploitability (EPSS score)
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Phishing and Social Engineering</h2>
    <div class="content">
        <h3>Phishing</h3>
        <p><strong>Definition</strong>: Fraudulent attempt to steal credentials/data via <strong>deceptive communication</strong>.</p>
        
        <h4>Types</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Method</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Email Phishing</strong></td>
                        <td>Mass emails with fake login pages</td>
                        <td>"Your account is locked click here"</td>
                    </tr>
                    <tr>
                        <td><strong>Spear Phishing</strong></td>
                        <td>Targeted at specific individual/org</td>
                        <td>Fake HR email to CFO with "invoice"</td>
                    </tr>
                    <tr>
                        <td><strong>Whaling</strong></td>
                        <td>Targets executives (CEO, CFO)</td>
                        <td>Fake legal subpoena</td>
                    </tr>
                    <tr>
                        <td><strong>Smishing/Vishing</strong></td>
                        <td>SMS or voice calls</td>
                        <td>"Your package is delayed press 1"</td>
                    </tr>
                    <tr>
                        <td><strong>Clone Phishing</strong></td>
                        <td>Copy legitimate email; replace link with malicious one</td>
                        <td>Duplicate PayPal receipt</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Social Engineering</h3>
        <p><strong>Broader manipulation</strong> of human psychology to bypass security.</p>
        <p><strong>Principles</strong>: Authority, urgency, scarcity, trust.</p>
        
        <h4>Common Tactics</h4>
        <ul>
            <li><strong>Pretexting</strong>: Create false scenario (e.g., "IT support" calling for password reset).</li>
            <li><strong>Baiting</strong>: Offer something enticing (e.g., free USB drive labeled "Salary Info").</li>
            <li><strong>Tailgating</strong>: Follow authorized person into secure facility.</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Defenses</strong>:<br>
            - <strong>Security awareness training</strong> (simulate phishing)<br>
            - <strong>Multi-factor authentication (MFA)</strong>   renders stolen passwords useless<br>
            - <strong>Email authentication</strong>: SPF, DKIM, DMARC<br>
            - <strong>Verify requests</strong> via secondary channel (e.g., call back on official number)
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Distributed Denial of Service (DDoS) Attacks</h2>
    <div class="content">
        <h3>What is a DDoS?</h3>
        <p>Overwhelm target (server, network, app) with <strong>flood of traffic</strong> from <strong>multiple sources</strong> (botnet), causing service unavailability.</p>
        
        <h3>Attack Types</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Category</th>
                        <th>Mechanism</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Volumetric</strong></td>
                        <td>Saturate bandwidth</td>
                        <td>UDP flood, DNS amplification</td>
                    </tr>
                    <tr>
                        <td><strong>Protocol</strong></td>
                        <td>Exhaust server resources</td>
                        <td>SYN flood, Ping of Death</td>
                    </tr>
                    <tr>
                        <td><strong>Application Layer</strong></td>
                        <td>Target app logic (Layer 7)</td>
                        <td>HTTP flood, Slowloris</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Botnets Used in DDoS</h3>
        <ul>
            <li><strong>Mirai</strong>: Infected IoT devices (cameras, routers); used in 2016 Dyn attack (took down Twitter, Netflix).</li>
            <li><strong>Meris</strong>: Exploited MikroTik routers; generated 2.4 Tbps attack (2021).</li>
        </ul>
        
        <h3>Mitigation Strategies</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Technique</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>On-Premise</strong></td>
                        <td>Rate limiting, blackholing, ACLs</td>
                    </tr>
                    <tr>
                        <td><strong>Cloud/ISP</strong></td>
                        <td>Scrubbing centers (e.g., Cloudflare, AWS Shield)</td>
                    </tr>
                    <tr>
                        <td><strong>Architectural</strong></td>
                        <td>Auto-scaling, CDN, load balancing</td>
                    </tr>
                    <tr>
                        <td><strong>Proactive</strong></td>
                        <td>DDoS protection services with Anycast network</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            📉 <strong>Key Metric</strong>: <strong>Attack size</strong> (Gbps/Tbps) and <strong>requests per second (RPS)</strong>.
        </div>
        
        <div class="definition">
            💡 <strong>Note</strong>: DDoS is often a <strong>smokescreen</strong> for data theft or ransomware deployment.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Threat</th>
                        <th>Primary Goal</th>
                        <th>Key Defense</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Malware</strong></td>
                        <td>Data theft, disruption</td>
                        <td>EDR, patching, backups</td>
                    </tr>
                    <tr>
                        <td><strong>APTs</strong></td>
                        <td>Long-term espionage</td>
                        <td>Network segmentation, UEBA, threat hunting</td>
                    </tr>
                    <tr>
                        <td><strong>Zero-Day</strong></td>
                        <td>Exploit unknown flaws</td>
                        <td>Virtual patching, attack surface reduction</td>
                    </tr>
                    <tr>
                        <td><strong>Phishing</strong></td>
                        <td>Credential theft</td>
                        <td>MFA, user training, email auth (DMARC)</td>
                    </tr>
                    <tr>
                        <td><strong>DDoS</strong></td>
                        <td>Service disruption</td>
                        <td>Cloud scrubbing, rate limiting, CDN</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Humans are the weakest link</strong>: Phishing and social engineering remain top initial access vectors.</li>
            <li><strong>APTs are patient</strong>: Detection requires behavioral analytics, not just signatures.</li>
            <li><strong>Zero-days are inevitable</strong>: Focus on <strong>resilience</strong> (assume breach) and <strong>rapid response</strong>.</li>
            <li><strong>DDoS is weaponized</strong>: Often used for extortion or distraction.</li>
            <li><strong>Defense is layered</strong>: No single tool stops all threats combine people, process, and technology.</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Golden Rule</strong>:<br>
            <strong>"Trust, but verify. Assume breach. Encrypt everything. Patch relentlessly."</strong>
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Security Operations</h2>
    <div class="content">
        <p><em>Detect, Analyze, Respond, and Recover from Cyber Threats</em></p>
        
        <div class="definition">
            <strong>Goal</strong>: Establish a proactive, efficient, and repeatable process to <strong>identify, contain, eradicate, and learn from security incidents</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Security Information and Event Management (SIEM)</h2>
    <div class="content">
        <h3>What is SIEM?</h3>
        <p>A centralized platform that <strong>collects, correlates, and analyzes log data</strong> from across an organization's IT infrastructure (networks, servers, endpoints, cloud, apps).</p>
        
        <h3>Core Functions</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Function</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Log Aggregation</strong></td>
                        <td>Ingest logs from diverse sources (firewalls, IDS, Windows Event Logs, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>Normalization</strong></td>
                        <td>Convert logs to common schema (e.g., CEF, LEEF)</td>
                    </tr>
                    <tr>
                        <td><strong>Correlation</strong></td>
                        <td>Link related events across sources (e.g., failed login + firewall block)</td>
                    </tr>
                    <tr>
                        <td><strong>Alerting</strong></td>
                        <td>Generate high-fidelity alerts based on rules or ML</td>
                    </tr>
                    <tr>
                        <td><strong>Dashboards & Reporting</strong></td>
                        <td>Visualize threats, compliance status (e.g., PCI DSS, HIPAA)</td>
                    </tr>
                    <tr>
                        <td><strong>Retention</strong></td>
                        <td>Store logs for forensic and compliance purposes (often 6–12 months)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Popular SIEM Solutions</h3>
        <ul>
            <li><strong>Commercial</strong>: Splunk Enterprise Security, IBM QRadar, Microsoft Sentinel, LogRhythm</li>
            <li><strong>Open-source</strong>: ELK Stack (Elasticsearch, Logstash, Kibana), Wazuh, Apache Metron</li>
        </ul>
        
        <h3>Use Cases</h3>
        <ul>
            <li>Detect brute-force attacks</li>
            <li>Identify data exfiltration patterns</li>
            <li>Monitor privileged user activity</li>
            <li>Meet regulatory audit requirements</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Challenges</strong>:<br>
            - High false-positive rates<br>
            - Log source coverage gaps<br>
            - Scalability and cost (storage/compute)
        </div>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>: Tune correlation rules regularly; integrate threat intelligence feeds.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Threat Intelligence and Analysis</h2>
    <div class="content">
        <h3>What is Threat Intelligence?</h3>
        <p><strong>Evidence-based knowledge</strong> about existing or emerging threats (actors, TTPs, IOCs) used to <strong>inform security decisions</strong>.</p>
        
        <h3>Types of Threat Intelligence</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Audience</th>
                        <th>Format</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Strategic</strong></td>
                        <td>Executives, risk managers</td>
                        <td>Reports, trends</td>
                        <td>"Nation-state A is targeting energy sector"</td>
                    </tr>
                    <tr>
                        <td><strong>Tactical</strong></td>
                        <td>SOC, threat hunters</td>
                        <td>TTPs, malware analysis</td>
                        <td>"APT29 uses PowerShell for lateral movement"</td>
                    </tr>
                    <tr>
                        <td><strong>Operational</strong></td>
                        <td>Incident responders</td>
                        <td>IOCs, attack timelines</td>
                        <td>"Malware hash: a1b2c3...; C2 IP: 185.143.x.x"</td>
                    </tr>
                    <tr>
                        <td><strong>Technical</strong></td>
                        <td>Analysts, engineers</td>
                        <td>IPs, domains, file hashes</td>
                        <td>"Block domain evil[.]com"</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Threat Intelligence Lifecycle</h3>
        <ol>
            <li><strong>Planning</strong>: Define requirements (e.g., "What threats target our industry?")</li>
            <li><strong>Collection</strong>: Gather data from open (OSINT), commercial, and internal sources</li>
            <li><strong>Processing</strong>: Normalize and enrich data (e.g., geolocate IPs, tag malware families)</li>
            <li><strong>Analysis</strong>: Contextualize turn data into actionable insights</li>
            <li><strong>Dissemination</strong>: Share via reports, APIs, or SIEM integrations</li>
            <li><strong>Feedback</strong>: Refine based on analyst input</li>
        </ol>
        
        <h3>Key Frameworks & Sources</h3>
        <ul>
            <li><strong>MITRE ATT&CK</strong>: Knowledge base of adversary TTPs (tactics, techniques, procedures)</li>
            <li><strong>STIX/TAXII</strong>: Standard formats for sharing threat intel (Structured Threat Info eXpression / Trusted Automated eXchange)</li>
            <li><strong>OSINT Sources</strong>: AlienVault OTX, VirusTotal, AbuseIPDB, CISA alerts</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Actionable Intel</strong>: Intelligence is only valuable if it leads to <strong>blocking, detection, or mitigation</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Incident Response Procedures</h2>
    <div class="content">
        <h3>Incident Response Lifecycle (NIST SP 800-61)</h3>
        <p>A structured approach to handling security breaches:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Phase</th>
                        <th>Key Activities</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>1. Preparation</strong></td>
                        <td>Develop IR plan, train team, deploy monitoring (SIEM, EDR), establish comms</td>
                    </tr>
                    <tr>
                        <td><strong>2. Identification</strong></td>
                        <td>Detect anomaly → validate as incident → classify (malware, DDoS, data breach)</td>
                    </tr>
                    <tr>
                        <td><strong>3. Containment</strong></td>
                        <td><strong>Short-term</strong>: Isolate affected systems (e.g., disable account, block IP)<br><strong>Long-term</strong>: Apply patches, remove backdoors</td>
                    </tr>
                    <tr>
                        <td><strong>4. Eradication</strong></td>
                        <td>Remove root cause (malware, attacker accounts); harden systems</td>
                    </tr>
                    <tr>
                        <td><strong>5. Recovery</strong></td>
                        <td>Restore systems from clean backups; monitor for reinfection</td>
                    </tr>
                    <tr>
                        <td><strong>6. Lessons Learned</strong></td>
                        <td>Post-mortem meeting; update IR plan, policies, and controls</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Incident Classification</h3>
        <ul>
            <li><strong>Low</strong>: Spam, port scans</li>
            <li><strong>Medium</strong>: Malware infection, policy violation</li>
            <li><strong>High</strong>: Data breach, ransomware, APT activity</li>
        </ul>
        
        <h3>Communication Plan</h3>
        <ul>
            <li><strong>Internal</strong>: Legal, PR, executives</li>
            <li><strong>External</strong>: Customers (if data breached), regulators (GDPR, CCPA), law enforcement (FBI, CISA)</li>
        </ul>
        
        <div class="definition">
            🚨 <strong>Golden Hour</strong>: First 60 minutes post-detection are critical for containment.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Digital Forensics Fundamentals</h2>
    <div class="content">
        <h3>Goal</h3>
        <p><strong>Preserve, identify, extract, and document</strong> digital evidence in a <strong>forensically sound</strong> manner (admissible in court).</p>
        
        <h3>Core Principles (ACPO Guidelines)</h3>
        <ol>
            <li><strong>Do not alter</strong> original evidence.</li>
            <li><strong>Document all actions</strong> (chain of custody).</li>
            <li><strong>Competence</strong>: Only qualified personnel handle evidence.</li>
            <li><strong>Audit trail</strong>: All processes must be verifiable.</li>
        </ol>
        
        <h3>Forensic Process</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Step</th>
                        <th>Tools & Techniques</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Acquisition</strong></td>
                        <td>Create bit-for-bit copy (disk image) using write-blockers<br>Tools: FTK Imager, dd, Guymager</td>
                    </tr>
                    <tr>
                        <td><strong>Preservation</strong></td>
                        <td>Hash verification (SHA-256/MD5) to ensure integrity</td>
                    </tr>
                    <tr>
                        <td><strong>Analysis</strong></td>
                        <td>Examine file system, registry, logs, memory dumps<br>Tools: Autopsy, Volatility (memory), Wireshark (network)</td>
                    </tr>
                    <tr>
                        <td><strong>Timeline Reconstruction</strong></td>
                        <td>Correlate file MAC times (Modified, Accessed, Created)</td>
                    </tr>
                    <tr>
                        <td><strong>Reporting</strong></td>
                        <td>Clear, factual report for legal/technical audiences</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Types of Forensics</h3>
        <ul>
            <li><strong>Disk Forensics</strong>: Analyze hard drives/SSDs</li>
            <li><strong>Memory Forensics</strong>: Extract running processes, network connections, encryption keys</li>
            <li><strong>Network Forensics</strong>: Reconstruct attacks from PCAPs</li>
            <li><strong>Mobile Forensics</strong>: Extract data from iOS/Android devices</li>
        </ul>
        
        <div class="definition">
            ⚖️ <strong>Legal Note</strong>: Evidence must follow <strong>chain of custody</strong> document who handled it, when, and why.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Security Automation and Orchestration</h2>
    <div class="content">
        <h3>Why Automate?</h3>
        <ul>
            <li>Reduce <strong>mean time to respond (MTTR)</strong></li>
            <li>Handle high-volume, repetitive tasks</li>
            <li>Free analysts for complex investigations</li>
        </ul>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>SOAR (Security Orchestration, Automation, and Response)</strong>: Platform that integrates SIEM, threat intel, and security tools to automate workflows.</li>
            <li><strong>Playbooks</strong>: Predefined response procedures (e.g., "If phishing email detected → quarantine, block sender, notify user").</li>
        </ul>
        
        <h3>Common Automation Use Cases</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Manual Time</th>
                        <th>Automated Time</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Enrich IP with threat intel</td>
                        <td>5–10 min</td>
                        <td>< 10 sec</td>
                    </tr>
                    <tr>
                        <td>Isolate infected endpoint</td>
                        <td>15 min</td>
                        <td>< 1 min</td>
                    </tr>
                    <tr>
                        <td>Reset user password after compromise</td>
                        <td>10 min</td>
                        <td>Instant</td>
                    </tr>
                    <tr>
                        <td>Block malicious domain at firewall</td>
                        <td>20 min</td>
                        <td>< 30 sec</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>SOAR Platforms</h3>
        <ul>
            <li>Palo Alto Cortex XSOAR</li>
            <li>Splunk SOAR (formerly Phantom)</li>
            <li>IBM Resilient</li>
            <li>Microsoft Sentinel (with Logic Apps)</li>
        </ul>
        
        <h3>Automation Risks & Mitigations</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Risk</th>
                        <th>Mitigation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>False positives trigger harmful actions</td>
                        <td>Use human-in-the-loop for critical actions</td>
                    </tr>
                    <tr>
                        <td>Over-automation reduces analyst skills</td>
                        <td>Balance automation with human oversight</td>
                    </tr>
                    <tr>
                        <td>Tool integration complexity</td>
                        <td>Use standardized APIs (REST, STIX/TAXII)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🤖 <strong>Future Trend</strong>: AI-driven SOAR predictive response, autonomous threat hunting.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Primary Purpose</th>
                        <th>Key Tools/Frameworks</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>SIEM</strong></td>
                        <td>Centralized log analysis & alerting</td>
                        <td>Splunk, QRadar, Sentinel</td>
                    </tr>
                    <tr>
                        <td><strong>Threat Intel</strong></td>
                        <td>Contextualize threats</td>
                        <td>MITRE ATT&CK, STIX/TAXII, OTX</td>
                    </tr>
                    <tr>
                        <td><strong>Incident Response</strong></td>
                        <td>Structured breach handling</td>
                        <td>NIST SP 800-61, IR playbooks</td>
                    </tr>
                    <tr>
                        <td><strong>Digital Forensics</strong></td>
                        <td>Preserve & analyze evidence</td>
                        <td>FTK, Autopsy, Volatility</td>
                    </tr>
                    <tr>
                        <td><strong>Automation (SOAR)</strong></td>
                        <td>Accelerate response</td>
                        <td>Cortex XSOAR, Splunk SOAR</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>SIEM is the SOC's central nervous system</strong> but requires tuning to avoid alert fatigue.</li>
            <li><strong>Threat intelligence must be operationalized</strong> not just collected.</li>
            <li><strong>Incident response is a team sport</strong>: Legal, PR, IT, and execs must coordinate.</li>
            <li><strong>Forensics = science + law</strong>: Integrity and documentation are non-negotiable.</li>
            <li><strong>Automation is force multiplier</strong> but never fully replace human judgment.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>:<br>
            Build <strong>runbooks</strong> for common incidents (phishing, ransomware, DDoS) and <strong>test them quarterly</strong> via tabletop exercises.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Adversarial Machine Learning</h2>
    <div class="content">
        <h3>Adversarial Attacks</h3>
        <div class="definition">
            <strong>Core Idea</strong>:<br>
            Slightly perturb input data in a way that is <strong>imperceptible to humans</strong> but causes a <strong>machine learning model to misclassify</strong> it with high confidence.
        </div>
        
        <div class="definition">
            <strong>Threat Model</strong>:<br>
            - <strong>Goal</strong>: Misclassification (targeted or untargeted)<br>
            - <strong>Knowledge</strong>: Varies (white-box vs. black-box)<br>
            - <strong>Constraint</strong>: Perturbation must be small (e.g., \( \|\delta\|_p \leq \epsilon \))
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">White-Box vs. Black-Box Attacks</h2>
    <div class="content">
        <h3>White-Box Attacks</h3>
        <ul>
            <li><strong>Assumption</strong>: Attacker has <strong>full knowledge</strong> of the target model:
            <ul>
                <li>Architecture</li>
                <li>Parameters (weights)</li>
                <li>Gradients</li>
            </ul>
            </li>
            <li><strong>Advantage</strong>: Can compute exact gradients → highly effective.</li>
            <li><strong>Use Case</strong>: Evaluating model robustness during development.</li>
        </ul>
        
        <h3>Black-Box Attacks</h3>
        <ul>
            <li><strong>Assumption</strong>: Attacker has <strong>no access</strong> to model internals.
            <ul>
                <li>Can only query the model (input → output).</li>
                <li>May only see final prediction (hard-label) or class probabilities (soft-label).</li>
            </ul>
            </li>
            <li><strong>Strategies</strong>:
            <ul>
                <li><strong>Transfer-based</strong>: Craft adversarial examples on a <strong>surrogate model</strong>, then attack target.</li>
                <li><strong>Query-based</strong>: Estimate gradients via finite differences (e.g., <strong>ZOO</strong>, <strong>Bandits</strong>).</li>
            </ul>
            </li>
            <li><strong>Use Case</strong>: Real-world attacks on APIs (e.g., Google Vision, AWS Rekognition).</li>
        </ul>
        
        <div class="definition">
            🔑 <strong>Key Insight</strong>:<br>
            Adversarial examples often <strong>transfer</strong> across models enabling black-box attacks without direct access.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Fast Gradient Sign Method (FGSM)</h2>
    <div class="content">
        <p><em>(Goodfellow et al., 2015)</em></p>
        
        <h3>Idea</h3>
        <p>Use <strong>gradient of loss w.r.t. input</strong> to find direction of maximal increase in loss.</p>
        <p>Take a <strong>single step</strong> in that direction with fixed step size \( \epsilon \).</p>
        
        <h3>Mathematical Formulation</h3>
        <p>For input \( \mathbf{x} \), true label \( y \), and model loss \( \mathcal{L} \):</p>
        <div class="equation">
            \[ \mathbf{x}_{\text{adv}} = \mathbf{x} + \epsilon \cdot \text{sign}\left( \nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}, y) \right) \]
        </div>
        <ul>
            <li>\( \epsilon \): Perturbation budget (e.g., 8/255 for pixel values in [0,1])</li>
            <li>\( \text{sign}(\cdot) \): Ensures perturbation is applied uniformly across pixels</li>
        </ul>
        
        <h3>Properties</h3>
        <ul>
            <li><strong>Fast</strong>: One forward + one backward pass.</li>
            <li><strong>Untargeted</strong>: Maximizes loss for true class.</li>
            <li><strong>Weak</strong>: Often outperformed by iterative methods.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use Case</strong>: Baseline attack for robustness testing.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Projected Gradient Descent (PGD)</h2>
    <div class="content">
        <p><em>(Madry et al., 2018)</em></p>
        
        <h3>Idea</h3>
        <p><strong>Iterative refinement</strong> of FGSM: Take multiple small steps, projecting back into \( \epsilon \)-ball after each step.</p>
        <p>Considered the <strong>strongest first-order white-box attack</strong>.</p>
        
        <h3>Algorithm</h3>
        <ol>
            <li>Initialize: \( \mathbf{x}_0^{\text{adv}} = \mathbf{x} + \text{random noise within } \epsilon\text{-ball} \)</li>
            <li>For \( t = 1 \) to \( T \):
            \[ \mathbf{x}_t^{\text{adv}} = \Pi_{\mathcal{B}_\epsilon(\mathbf{x})} \left( \mathbf{x}_{t-1}^{\text{adv}} + \alpha \cdot \text{sign}\left( \nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}_{t-1}^{\text{adv}}, y) \right) \right) \]
            <ul>
                <li>\( \alpha \): Step size (e.g., \( \epsilon / 4 \))</li>
                <li>\( \Pi \): Projection operator (clips to valid input range and \( \epsilon \)-ball)</li>
            </ul>
            </li>
        </ol>
        
        <h3>Why PGD is Stronger</h3>
        <ul>
            <li>Explores local maxima more thoroughly.</li>
            <li>Random initialization avoids poor local optima.</li>
            <li><strong>Gold standard</strong> for evaluating adversarial robustness.</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Defense Benchmark</strong>: A model robust to PGD is likely robust to many other attacks.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Carlini-Wagner (C&W) Attacks</h2>
    <div class="content">
        <p><em>(Carlini & Wagner, 2017)</em></p>
        
        <h3>Idea</h3>
        <p>Formulate attack as <strong>optimization problem</strong> that minimizes perturbation while ensuring misclassification.</p>
        <p>Bypasses <strong>defensive distillation</strong> and other gradient-masking defenses.</p>
        
        <h3>Optimization Objective</h3>
        <p>Minimize \( \|\delta\|_p + c \cdot f(\mathbf{x} + \delta) \)<br>
        subject to \( \mathbf{x} + \delta \in [0,1]^n \)</p>
        <ul>
            <li>\( \delta \): Perturbation</li>
            <li>\( f(\cdot) \): Custom loss function ensuring misclassification:
            <ul>
                <li><strong>Untargeted</strong>: \( f(\mathbf{z}) = \max(Z_{y} - \max_{i \neq y} Z_i, -\kappa) \)</li>
                <li><strong>Targeted</strong>: \( f(\mathbf{z}) = \max(\max_{i \ne t} Z_i - Z_t, -\kappa) \)</li>
                <li>\( Z \): Logits; \( \kappa \): Confidence parameter</li>
            </ul>
            </li>
            <li>\( c \): Trade-off constant (found via binary search)</li>
        </ul>
        
        <h3>Norms Supported</h3>
        <ul>
            <li>\( L_2 \): Most common (minimal Euclidean distortion)</li>
            <li>\( L_0 \): Minimize number of modified pixels</li>
            <li>\( L_\infty \): Bounded per-pixel change</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>High success rate</strong> even against defended models.</li>
            <li>Produces <strong>less perceptible</strong> perturbations than FGSM/PGD.</li>
            <li><strong>Adaptive</strong>: Can be tuned for any norm or confidence level.</li>
        </ul>
        
        <h3>Weaknesses</h3>
        <ul>
            <li>Computationally expensive (requires many iterations).</li>
            <li>Not as fast as PGD for large-scale evaluation.</li>
        </ul>
        
        <div class="definition">
            🎯 <strong>Use Case</strong>: Breaking state-of-the-art defenses; generating high-quality adversarial examples.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Transfer Attacks and Evasion Techniques</h2>
    <div class="content">
        <h3>Transferability Phenomenon</h3>
        <p>Adversarial examples crafted on <strong>Model A</strong> often fool <strong>Model B</strong> even if architectures differ.</p>
        <p><strong>Cause</strong>: Models learn similar decision boundaries; adversarial directions generalize.</p>
        
        <h3>Transfer Attack Workflow</h3>
        <ol>
            <li>Train or obtain a <strong>surrogate model</strong> (similar to target).</li>
            <li>Generate adversarial examples using white-box methods (FGSM, PGD).</li>
            <li>Submit to <strong>black-box target API</strong> → high success rate (~50–80% in practice).</li>
        </ol>
        
        <h3>Enhancing Transferability</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>How It Works</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Input Diversity</strong></td>
                        <td>Apply random transformations (resize, padding) during attack generation</td>
                    </tr>
                    <tr>
                        <td><strong>Momentum Iterative FGSM (MI-FGSM)</strong></td>
                        <td>Incorporate momentum into gradients to stabilize updates</td>
                    </tr>
                    <tr>
                        <td><strong>Ensemble Attacks</strong></td>
                        <td>Craft examples against <strong>multiple models</strong> simultaneously</td>
                    </tr>
                    <tr>
                        <td><strong>Feature-Level Attacks</strong></td>
                        <td>Perturb intermediate features instead of input</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Evasion in Real-World Systems</h3>
        <ul>
            <li><strong>APIs</strong>: Attack cloud ML services (e.g., fool facial recognition to bypass authentication).</li>
            <li><strong>Malware Detection</strong>: Perturb binary files to evade AV scanners (e.g., adding dead code).</li>
            <li><strong>Autonomous Vehicles</strong>: Trick object detectors with adversarial patches on road signs.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Limitation</strong>: Transferability drops with <strong>defensive ensembles</strong> or <strong>input transformations</strong> (e.g., JPEG compression).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Adversarial Attacks Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Attack</th>
                        <th>Type</th>
                        <th>Iterations</th>
                        <th>Strength</th>
                        <th>Speed</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>FGSM</strong></td>
                        <td>White-box</td>
                        <td>1</td>
                        <td>Low</td>
                        <td>⚡ Very Fast</td>
                        <td>Baseline testing</td>
                    </tr>
                    <tr>
                        <td><strong>PGD</strong></td>
                        <td>White-box</td>
                        <td>10–100</td>
                        <td>🔥 Very High</td>
                        <td>Medium</td>
                        <td>Robustness benchmark</td>
                    </tr>
                    <tr>
                        <td><strong>C&W</strong></td>
                        <td>White-box</td>
                        <td>1000+</td>
                        <td>🔥 High (stealthy)</td>
                        <td>Slow</td>
                        <td>Breaking defenses</td>
                    </tr>
                    <tr>
                        <td><strong>Transfer</strong></td>
                        <td>Black-box</td>
                        <td>Varies</td>
                        <td>Medium-High</td>
                        <td>Fast (after surrogate training)</td>
                        <td>Real-world API attacks</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>White-box attacks</strong> (PGD, C&W) are the strongest but require model access.</li>
            <li><strong>Black-box attacks</strong> rely on <strong>transferability</strong> or <strong>query-based gradient estimation</strong>.</li>
            <li><strong>FGSM</strong> is simple but weak; <strong>PGD</strong> is the gold standard for robustness evaluation.</li>
            <li><strong>C&W</strong> produces high-quality, minimal-perturbation examples ideal for research.</li>
            <li><strong>Transfer attacks</strong> make adversarial ML a real-world threat to deployed systems.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Defense Insight</strong>:<br>
            No single defense is perfect. <strong>Adversarial training</strong> (training on PGD examples) is the most effective known method but increases cost and may not generalize to unseen attacks.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Defense Mechanisms in Adversarial Machine Learning</h2>
    <div class="content">
        <div class="definition">
            <strong>Goal</strong>: Improve model robustness so that <strong>small, imperceptible perturbations do not cause misclassification</strong> without significantly degrading clean-data performance.
        </div>
        
        <div class="definition">
            <strong>Golden Rule</strong>:<br>
            <strong>"There is no silver bullet."</strong> Most defenses offer partial protection and may fail against adaptive attackers.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Adversarial Training Methods</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p><strong>Augment training data</strong> with adversarial examples → teach model to correctly classify perturbed inputs.</p>
        <p>Most effective and widely adopted defense to date.</p>
        
        <h3>Standard Adversarial Training (Madry et al., 2018)</h3>
        <ul>
            <li><strong>Algorithm</strong>:
            <ol>
                <li>For each batch of clean data \( \mathbf{x} \):</li>
                <li>Generate adversarial examples \( \mathbf{x}_{\text{adv}} \) using a strong attack (e.g., <strong>PGD</strong>).</li>
                <li>Train model on \( (\mathbf{x}_{\text{adv}}, y) \) using standard loss (e.g., cross-entropy).</li>
            </ol>
            </li>
        </ul>
        <p><strong>Optimization</strong>:</p>
        <div class="equation">
            \[ \min_\theta \mathbb{E}_{(\mathbf{x},y)} \left[ \max_{\|\delta\|_\infty \leq \epsilon} \mathcal{L}(\theta, \mathbf{x} + \delta, y) \right] \]
        </div>
        <p>This is a <strong>min-max</strong> problem: inner max = attack, outer min = defense.</p>
        
        <h3>Variants & Improvements</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Key Idea</th>
                        <th>Advantage</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>TRADES</strong></td>
                        <td>Separates clean and adversarial loss: \( \mathcal{L}_{\text{clean}} + \beta \cdot \text{KL}(f(\mathbf{x}), f(\mathbf{x}_{\text{adv}})) \)</td>
                        <td>Better clean accuracy; smoother decision boundary</td>
                    </tr>
                    <tr>
                        <td><strong>MART</strong></td>
                        <td>Focuses on misclassified adversarial examples</td>
                        <td>Improves robustness on hard samples</td>
                    </tr>
                    <tr>
                        <td><strong>AWP (Adversarial Weight Perturbation)</strong></td>
                        <td>Perturbs <strong>weights</strong> (not just inputs) during training</td>
                        <td>Defends against weight-based attacks</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Strengths</h3>
        <ul>
            <li>Provides <strong>empirical robustness</strong> against strong attacks (e.g., PGD).</li>
            <li>Theoretically grounded in robust optimization.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Computationally expensive</strong> (2–10× training time).</li>
            <li><strong>Robustness-accuracy trade-off</strong>: Often reduces clean accuracy.</li>
            <li><strong>Limited generalization</strong>: May not defend against unseen attack types (e.g., \( L_2 \) if trained on \( L_\infty \)).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>: Use <strong>PGD-based adversarial training</strong> as baseline for robust models.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Defensive Distillation</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p><em>(Papernot et al., 2016)</em></p>
        <p>Train a model to output <strong>soft probabilities</strong> (from a "teacher" model) instead of hard labels → smoother decision surface → harder to find adversarial directions.</p>
        
        <h3>How It Works</h3>
        <ol>
            <li><strong>Teacher model</strong>: Train normally on hard labels.</li>
            <li><strong>Student model</strong>: Train on <strong>softened logits</strong> using <strong>temperature-scaled softmax</strong>:<br>
            \[ q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)} \]
            where \( T > 1 \) → softer probability distribution.</li>
            <li>At test time, use student model with \( T = 1 \).</li>
        </ol>
        
        <h3>Claimed Benefit</h3>
        <p>Reduces <strong>gradient magnitude</strong> → makes gradient-based attacks (e.g., FGSM) less effective.</p>
        
        <h3>Reality Check</h3>
        <p><strong>Broken by C&W attacks</strong>: Carlini & Wagner (2017) showed distillation <strong>does not reduce gradients</strong> it only <strong>masks</strong> them (see Section 4).</p>
        <p>Now considered <strong>ineffective</strong> against adaptive attackers.</p>
        
        <div class="definition">
            ⚠️ <strong>Lesson</strong>: Defenses that rely on <strong>gradient obfuscation</strong> are generally <strong>not robust</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Input Preprocessing and Transformation</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p><strong>Modify input at test time</strong> to remove or neutralize adversarial perturbations before feeding to model.</p>
        
        <h3>Common Techniques</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mechanism</th>
                        <th>Effectiveness</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>JPEG Compression</strong></td>
                        <td>Re-encode image → removes high-frequency noise</td>
                        <td>Works against weak attacks; fails vs. PGD/C&W</td>
                    </tr>
                    <tr>
                        <td><strong>Feature Squeezing</strong></td>
                        <td>Reduce color depth or smooth pixels (e.g., bit-depth reduction, spatial smoothing)</td>
                        <td>Detects some attacks; degrades image quality</td>
                    </tr>
                    <tr>
                        <td><strong>Randomized Smoothing</strong></td>
                        <td>Add random noise to input; predict via majority vote over samples</td>
                        <td>Provides <strong>certified robustness</strong> for \( L_2 \) attacks</td>
                    </tr>
                    <tr>
                        <td><strong>Input Reconstruction</strong></td>
                        <td>Use autoencoders or GANs to "denoise" input</td>
                        <td>Often bypassed if attacker knows defense</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Randomized Smoothing (Cohen et al., 2019)</h3>
        <ul>
            <li><strong>Only method with certified robustness guarantees</strong>.</li>
            <li><strong>Process</strong>:
            <ul>
                <li>At test time: Sample \( \mathbf{x} + \epsilon \), \( \epsilon \sim \mathcal{N}(0, \sigma^2 I) \)</li>
                <li>Run classifier on many samples → take majority class</li>
            </ul>
            </li>
            <li><strong>Guarantee</strong>: If \( \|\delta\|_2 < R \), prediction is provably unchanged.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Not certifiable for \( L_\infty \)</strong> (common in practice).</li>
            <li><strong>High computational cost</strong> at inference.</li>
            <li><strong>Accuracy drop</strong> on clean data.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use Case</strong>: High-assurance applications (e.g., medical imaging, autonomous systems).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Gradient Masking and Obfuscation</h2>
    <div class="content">
        <h3>What Is Gradient Masking?</h3>
        <p>A <strong>flawed defense strategy</strong> where the model's gradients are <strong>hidden, distorted, or removed</strong> making gradient-based attacks appear to fail.</p>
        <p><strong>Does not improve true robustness</strong> just makes attacks harder to compute.</p>
        
        <h3>Common Forms</h3>
        <ul>
            <li><strong>Non-differentiable operations</strong>: e.g., quantization, binarization.</li>
            <li><strong>Stochastic layers</strong>: Randomness breaks gradient flow.</li>
            <li><strong>Defensive distillation</strong>: Soft labels reduce gradient magnitude (but gradients still exist).</li>
        </ul>
        
        <h3>Why It Fails</h3>
        <ul>
            <li><strong>Adaptive attackers</strong> can:
            <ul>
                <li>Use <strong>approximate gradients</strong> (e.g., finite differences in black-box attacks).</li>
                <li>Switch to <strong>gradient-free attacks</strong> (e.g., C&W with \( L_0 \), ZOO).</li>
                <li><strong>Reverse-engineer</strong> the defense.</li>
            </ul>
            </li>
            <li><strong>Carlini & Wagner (2017)</strong> demonstrated that 9 out of 10 early defenses relied on gradient masking.</li>
        </ul>
        
        <div class="definition">
            🚫 <strong>Key Takeaway</strong>:<br>
            <strong>If your defense breaks gradient-based attacks but not black-box attacks → it's likely gradient masking.</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Robust Feature Representations</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Learn features that are <strong>inherently insensitive</strong> to small input perturbations.</p>
        <p>Focus on <strong>semantic, high-level features</strong> rather than brittle low-level patterns.</p>
        
        <h3>Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mechanism</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Adversarially Robust Features</strong></td>
                        <td>Extract features from an <strong>adversarially trained model</strong> → use as input to another classifier</td>
                    </tr>
                    <tr>
                        <td><strong>Invariant Representations</strong></td>
                        <td>Enforce invariance to perturbations via contrastive learning or data augmentation</td>
                    </tr>
                    <tr>
                        <td><strong>Spectral Normalization</strong></td>
                        <td>Constrain Lipschitz constant of network → limits output change per input change</td>
                    </tr>
                    <tr>
                        <td><strong>Parseval Networks</strong></td>
                        <td>Use orthogonal weight matrices → preserve input distances</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Lipschitz Constant & Robustness</h3>
        <p>A model with <strong>small Lipschitz constant \( L \)</strong> satisfies:</p>
        <div class="equation">
            \[ \|f(\mathbf{x}) - f(\mathbf{x} + \delta)\| \leq L \|\delta\| \]
        </div>
        <p><strong>Lower \( L \) → more robust</strong>.</p>
        <p>Enforced via <strong>spectral normalization</strong> (clip largest singular value of weight matrices).</p>
        
        <h3>Evidence</h3>
        <p>Features from robust models:</p>
        <ul>
            <li>Focus on <strong>object shape/texture</strong> (not background noise).</li>
            <li>Transfer better to downstream tasks.</li>
            <li>Are more interpretable.</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Research Frontier</strong>: Combining robust features with self-supervised learning for data-efficient robustness.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Defense Mechanisms Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Defense</th>
                        <th>Robust?</th>
                        <th>Certifiable?</th>
                        <th>Computational Cost</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Adversarial Training</strong></td>
                        <td>✅ Yes (empirical)</td>
                        <td>❌ No</td>
                        <td>High (training)</td>
                        <td><strong>State-of-the-art</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Defensive Distillation</strong></td>
                        <td>❌ No</td>
                        <td>❌ No</td>
                        <td>Low</td>
                        <td><strong>Broken</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Input Transformation</strong></td>
                        <td>⚠️ Partial</td>
                        <td>✅ Only randomized smoothing</td>
                        <td>Medium (inference)</td>
                        <td><strong>Useful in practice</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Gradient Masking</strong></td>
                        <td>❌ No</td>
                        <td>❌ No</td>
                        <td>Low</td>
                        <td><strong>Avoid</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Robust Features</strong></td>
                        <td>✅ Yes (emerging)</td>
                        <td>⚠️ Limited</td>
                        <td>Medium</td>
                        <td><strong>Promising research</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Adversarial training</strong> is the most reliable empirical defense use PGD for best results.</li>
            <li><strong>Defensive distillation and gradient masking</strong> provide <strong>false security</strong> avoid them.</li>
            <li><strong>Randomized smoothing</strong> is the only method with <strong>provable guarantees</strong> (for \( L_2 \)).</li>
            <li><strong>Input transformations</strong> can help but are often bypassed by adaptive attackers.</li>
            <li><strong>Robust features</strong> represent the future: build models that learn <strong>semantically meaningful representations</strong>.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Final Advice</strong>:<br>
            Always <strong>evaluate defenses against adaptive white-box attacks</strong> (e.g., PGD, C&W).<br>
            If a paper claims robustness but only tests FGSM be skeptical.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Defense Mechanisms in Adversarial Machine Learning</h2>
    <div class="content">
        <div class="definition">
            <strong>Goal</strong>: Improve model robustness so that <strong>small, imperceptible perturbations do not cause misclassification</strong> without significantly degrading clean-data performance.
        </div>
        
        <div class="definition">
            <strong>Golden Rule</strong>:<br>
            <strong>"There is no silver bullet."</strong> Most defenses offer partial protection and may fail against adaptive attackers.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Adversarial Training Methods</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p><strong>Augment training data</strong> with adversarial examples → teach model to correctly classify perturbed inputs.</p>
        <p>Most effective and widely adopted defense to date.</p>
        
        <h3>Standard Adversarial Training (Madry et al., 2018)</h3>
        <ul>
            <li><strong>Algorithm</strong>:
            <ol>
                <li>For each batch of clean data \( \mathbf{x} \):</li>
                <li>Generate adversarial examples \( \mathbf{x}_{\text{adv}} \) using a strong attack (e.g., <strong>PGD</strong>).</li>
                <li>Train model on \( (\mathbf{x}_{\text{adv}}, y) \) using standard loss (e.g., cross-entropy).</li>
            </ol>
            </li>
        </ul>
        <p><strong>Optimization</strong>:</p>
        <div class="equation">
            \[ \min_\theta \mathbb{E}_{(\mathbf{x},y)} \left[ \max_{\|\delta\|_\infty \leq \epsilon} \mathcal{L}(\theta, \mathbf{x} + \delta, y) \right] \]
        </div>
        <p>This is a <strong>min-max</strong> problem: inner max = attack, outer min = defense.</p>
        
        <h3>Variants & Improvements</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Key Idea</th>
                        <th>Advantage</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>TRADES</strong></td>
                        <td>Separates clean and adversarial loss: \( \mathcal{L}_{\text{clean}} + \beta \cdot \text{KL}(f(\mathbf{x}), f(\mathbf{x}_{\text{adv}})) \)</td>
                        <td>Better clean accuracy; smoother decision boundary</td>
                    </tr>
                    <tr>
                        <td><strong>MART</strong></td>
                        <td>Focuses on misclassified adversarial examples</td>
                        <td>Improves robustness on hard samples</td>
                    </tr>
                    <tr>
                        <td><strong>AWP (Adversarial Weight Perturbation)</strong></td>
                        <td>Perturbs <strong>weights</strong> (not just inputs) during training</td>
                        <td>Defends against weight-based attacks</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Strengths</h3>
        <ul>
            <li>Provides <strong>empirical robustness</strong> against strong attacks (e.g., PGD).</li>
            <li>Theoretically grounded in robust optimization.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Computationally expensive</strong> (2–10× training time).</li>
            <li><strong>Robustness-accuracy trade-off</strong>: Often reduces clean accuracy.</li>
            <li><strong>Limited generalization</strong>: May not defend against unseen attack types (e.g., \( L_2 \) if trained on \( L_\infty \)).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>: Use <strong>PGD-based adversarial training</strong> as baseline for robust models.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Defensive Distillation</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p><em>(Papernot et al., 2016)</em></p>
        <p>Train a model to output <strong>soft probabilities</strong> (from a "teacher" model) instead of hard labels → smoother decision surface → harder to find adversarial directions.</p>
        
        <h3>How It Works</h3>
        <ol>
            <li><strong>Teacher model</strong>: Train normally on hard labels.</li>
            <li><strong>Student model</strong>: Train on <strong>softened logits</strong> using <strong>temperature-scaled softmax</strong>:<br>
            \[ q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)} \]
            where \( T > 1 \) → softer probability distribution.</li>
            <li>At test time, use student model with \( T = 1 \).</li>
        </ol>
        
        <h3>Claimed Benefit</h3>
        <p>Reduces <strong>gradient magnitude</strong> → makes gradient-based attacks (e.g., FGSM) less effective.</p>
        
        <h3>Reality Check</h3>
        <p><strong>Broken by C&W attacks</strong>: Carlini & Wagner (2017) showed distillation <strong>does not reduce gradients</strong> it only <strong>masks</strong> them (see Section 4).</p>
        <p>Now considered <strong>ineffective</strong> against adaptive attackers.</p>
        
        <div class="definition">
            ⚠️ <strong>Lesson</strong>: Defenses that rely on <strong>gradient obfuscation</strong> are generally <strong>not robust</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Input Preprocessing and Transformation</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p><strong>Modify input at test time</strong> to remove or neutralize adversarial perturbations before feeding to model.</p>
        
        <h3>Common Techniques</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mechanism</th>
                        <th>Effectiveness</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>JPEG Compression</strong></td>
                        <td>Re-encode image → removes high-frequency noise</td>
                        <td>Works against weak attacks; fails vs. PGD/C&W</td>
                    </tr>
                    <tr>
                        <td><strong>Feature Squeezing</strong></td>
                        <td>Reduce color depth or smooth pixels (e.g., bit-depth reduction, spatial smoothing)</td>
                        <td>Detects some attacks; degrades image quality</td>
                    </tr>
                    <tr>
                        <td><strong>Randomized Smoothing</strong></td>
                        <td>Add random noise to input; predict via majority vote over samples</td>
                        <td>Provides <strong>certified robustness</strong> for \( L_2 \) attacks</td>
                    </tr>
                    <tr>
                        <td><strong>Input Reconstruction</strong></td>
                        <td>Use autoencoders or GANs to "denoise" input</td>
                        <td>Often bypassed if attacker knows defense</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Randomized Smoothing (Cohen et al., 2019)</h3>
        <ul>
            <li><strong>Only method with certified robustness guarantees</strong>.</li>
            <li><strong>Process</strong>:
            <ul>
                <li>At test time: Sample \( \mathbf{x} + \epsilon \), \( \epsilon \sim \mathcal{N}(0, \sigma^2 I) \)</li>
                <li>Run classifier on many samples → take majority class</li>
            </ul>
            </li>
            <li><strong>Guarantee</strong>: If \( \|\delta\|_2 < R \), prediction is provably unchanged.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Not certifiable for \( L_\infty \)</strong> (common in practice).</li>
            <li><strong>High computational cost</strong> at inference.</li>
            <li><strong>Accuracy drop</strong> on clean data.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use Case</strong>: High-assurance applications (e.g., medical imaging, autonomous systems).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Gradient Masking and Obfuscation</h2>
    <div class="content">
        <h3>What Is Gradient Masking?</h3>
        <p>A <strong>flawed defense strategy</strong> where the model's gradients are <strong>hidden, distorted, or removed</strong> making gradient-based attacks appear to fail.</p>
        <p><strong>Does not improve true robustness</strong> just makes attacks harder to compute.</p>
        
        <h3>Common Forms</h3>
        <ul>
            <li><strong>Non-differentiable operations</strong>: e.g., quantization, binarization.</li>
            <li><strong>Stochastic layers</strong>: Randomness breaks gradient flow.</li>
            <li><strong>Defensive distillation</strong>: Soft labels reduce gradient magnitude (but gradients still exist).</li>
        </ul>
        
        <h3>Why It Fails</h3>
        <ul>
            <li><strong>Adaptive attackers</strong> can:
            <ul>
                <li>Use <strong>approximate gradients</strong> (e.g., finite differences in black-box attacks).</li>
                <li>Switch to <strong>gradient-free attacks</strong> (e.g., C&W with \( L_0 \), ZOO).</li>
                <li><strong>Reverse-engineer</strong> the defense.</li>
            </ul>
            </li>
            <li><strong>Carlini & Wagner (2017)</strong> demonstrated that 9 out of 10 early defenses relied on gradient masking.</li>
        </ul>
        
        <div class="definition">
            🚫 <strong>Key Takeaway</strong>:<br>
            <strong>If your defense breaks gradient-based attacks but not black-box attacks → it's likely gradient masking.</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Robust Feature Representations</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Learn features that are <strong>inherently insensitive</strong> to small input perturbations.</p>
        <p>Focus on <strong>semantic, high-level features</strong> rather than brittle low-level patterns.</p>
        
        <h3>Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mechanism</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Adversarially Robust Features</strong></td>
                        <td>Extract features from an <strong>adversarially trained model</strong> → use as input to another classifier</td>
                    </tr>
                    <tr>
                        <td><strong>Invariant Representations</strong></td>
                        <td>Enforce invariance to perturbations via contrastive learning or data augmentation</td>
                    </tr>
                    <tr>
                        <td><strong>Spectral Normalization</strong></td>
                        <td>Constrain Lipschitz constant of network → limits output change per input change</td>
                    </tr>
                    <tr>
                        <td><strong>Parseval Networks</strong></td>
                        <td>Use orthogonal weight matrices → preserve input distances</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Lipschitz Constant & Robustness</h3>
        <p>A model with <strong>small Lipschitz constant \( L \)</strong> satisfies:</p>
        <div class="equation">
            \[ \|f(\mathbf{x}) - f(\mathbf{x} + \delta)\| \leq L \|\delta\| \]
        </div>
        <p><strong>Lower \( L \) → more robust</strong>.</p>
        <p>Enforced via <strong>spectral normalization</strong> (clip largest singular value of weight matrices).</p>
        
        <h3>Evidence</h3>
        <p>Features from robust models:</p>
        <ul>
            <li>Focus on <strong>object shape/texture</strong> (not background noise).</li>
            <li>Transfer better to downstream tasks.</li>
            <li>Are more interpretable.</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Research Frontier</strong>: Combining robust features with self-supervised learning for data-efficient robustness.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Defense Mechanisms Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Defense</th>
                        <th>Robust?</th>
                        <th>Certifiable?</th>
                        <th>Computational Cost</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Adversarial Training</strong></td>
                        <td>✅ Yes (empirical)</td>
                        <td>❌ No</td>
                        <td>High (training)</td>
                        <td><strong>State-of-the-art</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Defensive Distillation</strong></td>
                        <td>❌ No</td>
                        <td>❌ No</td>
                        <td>Low</td>
                        <td><strong>Broken</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Input Transformation</strong></td>
                        <td>⚠️ Partial</td>
                        <td>✅ Only randomized smoothing</td>
                        <td>Medium (inference)</td>
                        <td><strong>Useful in practice</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Gradient Masking</strong></td>
                        <td>❌ No</td>
                        <td>❌ No</td>
                        <td>Low</td>
                        <td><strong>Avoid</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Robust Features</strong></td>
                        <td>✅ Yes (emerging)</td>
                        <td>⚠️ Limited</td>
                        <td>Medium</td>
                        <td><strong>Promising research</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Adversarial training</strong> is the most reliable empirical defense use PGD for best results.</li>
            <li><strong>Defensive distillation and gradient masking</strong> provide <strong>false security</strong> avoid them.</li>
            <li><strong>Randomized smoothing</strong> is the only method with <strong>provable guarantees</strong> (for \( L_2 \)).</li>
            <li><strong>Input transformations</strong> can help but are often bypassed by adaptive attackers.</li>
            <li><strong>Robust features</strong> represent the future: build models that learn <strong>semantically meaningful representations</strong>.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Final Advice</strong>:<br>
            Always <strong>evaluate defenses against adaptive white-box attacks</strong> (e.g., PGD, C&W).<br>
            If a paper claims robustness but only tests FGSM be skeptical.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Robustness Evaluation</h2>
    <div class="content">
        <p><em>Measuring, Benchmarking, and Certifying Model Resilience to Adversarial Attacks</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>"You cannot improve what you cannot measure."</strong><br>
            Robustness must be evaluated using <strong>strong, adaptive attacks</strong> not just baseline methods like FGSM.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Robustness Metrics and Measurements</h2>
    <div class="content">
        <h3>Primary Metrics</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Definition</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Robust Accuracy</strong></td>
                        <td>Accuracy on <strong>adversarial examples</strong> (e.g., PGD, C&W)</td>
                        <td>Standard benchmark; reported alongside clean accuracy</td>
                    </tr>
                    <tr>
                        <td><strong>Attack Success Rate (ASR)</strong></td>
                        <td>% of adversarial examples that cause misclassification</td>
                        <td>Measures attacker effectiveness</td>
                    </tr>
                    <tr>
                        <td><strong>Perturbation Budget (\( \epsilon \))</strong></td>
                        <td>Maximum allowed perturbation (e.g., \( \epsilon = 8/255 \) for \( L_\infty \))</td>
                        <td>Defines threat model severity</td>
                    </tr>
                    <tr>
                        <td><strong>Empirical Robustness</strong></td>
                        <td>Average minimal perturbation needed to flip prediction</td>
                        <td>Higher = more robust</td>
                    </tr>
                    <tr>
                        <td><strong>Certified Radius</strong></td>
                        <td>Largest \( r \) such that model is guaranteed robust for \( \|\delta\| \leq r \)</td>
                        <td>Used in certified defenses</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Norms and Perturbation Types</h3>
        <ul>
            <li><strong>\( L_\infty \)</strong>: Max per-pixel change (common in vision; e.g., \( \epsilon = 0.031 \))</li>
            <li><strong>\( L_2 \)</strong>: Euclidean distance (used in certified defenses)</li>
            <li><strong>\( L_0 \)</strong>: Number of modified features (e.g., malware byte flips)</li>
        </ul>
        
        <div class="definition">
            📊 <strong>Reporting Best Practice</strong>:<br>
            Always report <strong>both clean accuracy and robust accuracy</strong> under a <strong>specified threat model</strong> (norm, \( \epsilon \), attack type).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Benchmarking Adversarial Robustness</h2>
    <div class="content">
        <h3>Standardized Benchmarks</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Description</th>
                        <th>Key Features</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>RobustBench</strong></td>
                        <td>Community-driven leaderboard for image classifiers</td>
                        <td>Uses <strong>AutoAttack</strong> (adaptive ensemble of 4 attacks)</td>
                    </tr>
                    <tr>
                        <td><strong>Adversarial Vision Challenge (NIPS 2018)</strong></td>
                        <td>Early large-scale robustness competition</td>
                        <td>Highlighted transferability and adaptive attacks</td>
                    </tr>
                    <tr>
                        <td><strong>CIFAR-10-C / ImageNet-C</strong></td>
                        <td>Corruption robustness (not adversarial, but related)</td>
                        <td>Tests generalization to noise, blur, etc.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>AutoAttack (Croce & Hein, 2020)</h3>
        <ul>
            <li><strong>Gold standard</strong> for robustness evaluation.</li>
            <li><strong>Ensemble of 4 attacks</strong>:
            <ol>
                <li>APGD-CE (targeted, cross-entropy loss)</li>
                <li>APGD-DLR (untargeted, DLR loss)</li>
                <li>FAB (minimal \( L_p \) perturbation)</li>
                <li>Square Attack (black-box, gradient-free)</li>
            </ol>
            </li>
            <li><strong>Fully automatic</strong>: No tuning needed; adaptive to model defenses.</li>
            <li><strong>Why it matters</strong>: Many defenses that beat PGD <strong>fail</strong> on AutoAttack.</li>
        </ul>
        
        <h3>Evaluation Protocol</h3>
        <ol>
            <li>Train model under chosen defense (e.g., adversarial training).</li>
            <li>Evaluate on <strong>held-out test set</strong>.</li>
            <li>Generate adversarial examples using <strong>strong adaptive attack</strong> (e.g., AutoAttack).</li>
            <li>Report <strong>robust accuracy</strong>.</li>
        </ol>
        
        <div class="definition">
            ⚠️ <strong>Pitfall to Avoid</strong>:<br>
            Using weak attacks (e.g., FGSM) or non-adaptive evaluations → <strong>overestimates robustness</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Certified Defenses and Guarantees</h2>
    <div class="content">
        <h3>What Is Certification?</h3>
        <p>Provide <strong>mathematical proof</strong> that a model's prediction <strong>will not change</strong> for any perturbation within a radius \( r \).</p>
        
        <h3>Types of Certification</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Approach</th>
                        <th>Norm</th>
                        <th>Scalability</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Randomized Smoothing</strong></td>
                        <td>Predict via smoothed classifier; certify via statistical bounds</td>
                        <td>\( L_2 \)</td>
                        <td>✅ High (works on ImageNet)</td>
                    </tr>
                    <tr>
                        <td><strong>Interval Bound Propagation (IBP)</strong></td>
                        <td>Bound output range using interval arithmetic</td>
                        <td>\( L_\infty \)</td>
                        <td>⚠️ Medium (small networks)</td>
                    </tr>
                    <tr>
                        <td><strong>CROWN / Fast-Lin</strong></td>
                        <td>Use linear relaxations of network activations</td>
                        <td>\( L_\infty \)</td>
                        <td>❌ Low (tiny networks)</td>
                    </tr>
                    <tr>
                        <td><strong>Lipschitz Constant Bounding</strong></td>
                        <td>Enforce global Lipschitz constraint</td>
                        <td>Any</td>
                        <td>⚠️ Medium</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Randomized Smoothing (Cohen et al., 2019) – Most Practical</h3>
        <ul>
            <li><strong>Certification</strong>: For input \( \mathbf{x} \), if:<br>
            \[ \underline{p_A} > 0.5 + \Phi\left( -\frac{r}{\sigma} \right) \]
            then model is robust for all \( \|\delta\|_2 < r \), where:
            <ul>
                <li>\( \underline{p_A} \): Lower bound on top-class probability</li>
                <li>\( \sigma \): Noise level</li>
                <li>\( \Phi \): Standard normal CDF</li>
            </ul>
            </li>
            <li><strong>Pros</strong>: Scales to large models (ResNet-50 on ImageNet).</li>
            <li><strong>Cons</strong>: Only for \( L_2 \); certification radius often small (e.g., \( r \approx 1.0 \) for CIFAR-10).</li>
        </ul>
        
        <div class="definition">
            🔒 <strong>Key Insight</strong>:<br>
            Certification provides <strong>worst-case guarantees</strong> unlike empirical robustness, which is <strong>best-effort</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Stress Testing Methodologies</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Simulate <strong>real-world adversarial conditions</strong> beyond standard \( L_p \) perturbations.</p>
        
        <h3>Advanced Stress Tests</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Test Type</th>
                        <th>Description</th>
                        <th>Tools/Methods</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Adaptive Attacks</strong></td>
                        <td>Attacker knows the defense and optimizes against it</td>
                        <td>Retrain surrogate model; use AutoAttack</td>
                    </tr>
                    <tr>
                        <td><strong>Transfer Attacks</strong></td>
                        <td>Evaluate robustness against black-box threats</td>
                        <td>Generate on surrogate → test on target</td>
                    </tr>
                    <tr>
                        <td><strong>Semantic Perturbations</strong></td>
                        <td>Perturbations that preserve semantics (e.g., rotate object, change lighting)</td>
                        <td>3D rendering, GANs</td>
                    </tr>
                    <tr>
                        <td><strong>Physical-World Attacks</strong></td>
                        <td>Print adversarial patch; test in real camera view</td>
                        <td>Expectation over transformations (EoT)</td>
                    </tr>
                    <tr>
                        <td><strong>Composite Attacks</strong></td>
                        <td>Combine multiple perturbations (e.g., noise + rotation)</td>
                        <td>Multi-objective optimization</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Red-Teaming</h3>
        <ul>
            <li><strong>Process</strong>: Hire external experts to <strong>break your model</strong> using any means.</li>
            <li><strong>Used by</strong>: Google, Microsoft, DARPA for high-stakes models (e.g., autonomous vehicles).</li>
        </ul>
        
        <div class="definition">
            🧪 <strong>Best Practice</strong>:<br>
            Test under <strong>diverse threat models</strong> don't assume attacker is limited to \( L_\infty \) noise.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Formal Verification of Robustness</h2>
    <div class="content">
        <h3>What Is Formal Verification?</h3>
        <p>Use <strong>mathematical logic</strong> to prove that <strong>for all inputs in a region</strong>, the model satisfies a property (e.g., "output class = cat").</p>
        
        <h3>Techniques</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>How It Works</th>
                        <th>Limitations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>SMT Solvers</strong></td>
                        <td>Encode network + property as logical formula; check satisfiability</td>
                        <td>Only tiny networks (< 100 neurons)</td>
                    </tr>
                    <tr>
                        <td><strong>Mixed-Integer Linear Programming (MILP)</strong></td>
                        <td>Model ReLU as binary constraints; solve optimization</td>
                        <td>Exponential time; small networks</td>
                    </tr>
                    <tr>
                        <td><strong>Abstract Interpretation</strong></td>
                        <td>Over-approximate neuron values using abstract domains (e.g., intervals)</td>
                        <td>Conservative (may fail to verify robust models)</td>
                    </tr>
                    <tr>
                        <td><strong>Reachability Analysis</strong></td>
                        <td>Compute exact output set for input region</td>
                        <td>Computationally intractable for deep nets</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Tools</h3>
        <ul>
            <li><strong>ERAN</strong>: Uses abstract interpretation for robustness verification.</li>
            <li><strong>Marabou</strong>: SMT-based neural network verifier.</li>
            <li><strong>Neurify</strong>: Combines symbolic execution and abstract interpretation.</li>
        </ul>
        
        <h3>Reality Check</h3>
        <ul>
            <li><strong>Not scalable</strong> to modern deep networks (ResNet, ViT).</li>
            <li><strong>Research focus</strong>: Developing <strong>scalable sound approximations</strong>.</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Future Direction</strong>:<br>
            Combine <strong>certified defenses</strong> (e.g., randomized smoothing) with <strong>formal methods</strong> for end-to-end guarantees.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Evaluation Approach</th>
                        <th>Strength</th>
                        <th>Weakness</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Empirical (AutoAttack)</strong></td>
                        <td>Realistic, scalable</td>
                        <td>No guarantees</td>
                        <td>Standard model validation</td>
                    </tr>
                    <tr>
                        <td><strong>Certified (Smoothing)</strong></td>
                        <td>Mathematically sound</td>
                        <td>Limited to \( L_2 \); small radii</td>
                        <td>High-assurance systems</td>
                    </tr>
                    <tr>
                        <td><strong>Formal Verification</strong></td>
                        <td>Complete correctness proof</td>
                        <td>Only for tiny networks</td>
                        <td>Safety-critical micro-models</td>
                    </tr>
                    <tr>
                        <td><strong>Stress Testing</strong></td>
                        <td>Mimics real-world threats</td>
                        <td>Ad-hoc; no standard</td>
                        <td>Red-teaming, physical systems</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Never trust a defense without adaptive evaluation</strong> use <strong>AutoAttack</strong> or <strong>strong PGD</strong>.</li>
            <li><strong>Robust accuracy</strong> is the standard metric always report it with <strong>clean accuracy</strong>.</li>
            <li><strong>Certified defenses</strong> (e.g., randomized smoothing) offer guarantees but are limited in scope.</li>
            <li><strong>Formal verification</strong> is powerful but not yet practical for deep learning.</li>
            <li><strong>Stress testing</strong> bridges the gap between lab and real-world threats.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Golden Rule of Robustness Evaluation</strong>:<br>
            <strong>"If you didn't test against an adaptive white-box attacker, you haven't tested at all."</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Explainable AI (XAI)</h2>
    <div class="content">
        <p><em>Making Black-Box Models Transparent and Trustworthy</em></p>
        
        <div class="definition">
            <strong>Why XAI Matters</strong>:<br>
            - <strong>Regulatory compliance</strong> (GDPR "right to explanation", EU AI Act)<br>
            - <strong>Debugging models</strong> (identify bias, data leaks)<br>
            - <strong>Building user trust</strong> (e.g., in healthcare, finance)<br>
            - <strong>Scientific discovery</strong> (e.g., in biology, physics)
        </div>
        
        <div class="definition">
            <strong>Two Key Concepts</strong>:<br>
            - <strong>Interpretability</strong>: How easily a human can understand the model's behavior.<br>
            - <strong>Explainability</strong>: Post-hoc methods to explain predictions of complex models.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Interpretability Techniques: Overview</h2>
    <div class="content">
        <h3>Model-Specific vs. Model-Agnostic</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Description</th>
                        <th>Examples</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Inherently Interpretable Models</strong></td>
                        <td>Simple models whose logic is transparent</td>
                        <td>Linear regression, decision trees, rule-based systems</td>
                    </tr>
                    <tr>
                        <td><strong>Post-hoc Explanations</strong></td>
                        <td>Explain predictions of any (black-box) model</td>
                        <td>LIME, SHAP, PDP</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Global vs. Local Explanations</h3>
        <ul>
            <li><strong>Global</strong>: Explain overall model behavior (e.g., "Which features matter most?")</li>
            <li><strong>Local</strong>: Explain a single prediction (e.g., "Why was <em>this</em> loan denied?")</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Feature Importance Methods</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Rank features by their contribution to model predictions.</p>
        
        <h3>Common Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>How It Works</th>
                        <th>Pros / Cons</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Permutation Importance</strong></td>
                        <td>Shuffle a feature → measure drop in model performance (e.g., accuracy)</td>
                        <td>✅ Model-agnostic<br>❌ Fails with correlated features</td>
                    </tr>
                    <tr>
                        <td><strong>Gini Importance (for trees)</strong></td>
                        <td>Total reduction in impurity (e.g., Gini) from splits on a feature</td>
                        <td>✅ Fast<br>❌ Biased toward high-cardinality features</td>
                    </tr>
                    <tr>
                        <td><strong>Coefficients (linear models)</strong></td>
                        <td>Absolute value of weights \( |w_i| \)</td>
                        <td>✅ Exact for linear models<br>❌ Not for non-linear models</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            📌 <strong>Best Practice</strong>: Use <strong>permutation importance</strong> for model-agnostic global explanations.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Partial Dependence Plots (PDP)</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Show <strong>marginal effect</strong> of one or two features on predicted outcome, <strong>averaged over all other features</strong>.</p>
        
        <h3>How It Works</h3>
        <p>For feature \( x_i \), compute:</p>
        <div class="equation">
            \[ \text{PD}(x_i) = \mathbb{E}_{\mathbf{x}_{-i}} \left[ f(x_i, \mathbf{x}_{-i}) \right] \]
        </div>
        <ul>
            <li>Approximated by averaging predictions over dataset with \( x_i \) fixed.</li>
        </ul>
        
        <h3>Example</h3>
        <p>In a house price model, PDP for "square footage" shows how price changes as size increases, <strong>holding other features constant on average</strong>.</p>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Assumes feature independence</strong> → misleading if features are correlated (e.g., "age" and "income").</li>
            <li><strong>Averaging hides heterogeneity</strong> → may mask subgroups with opposite effects.</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Alternative</strong>: <strong>Individual Conditional Expectation (ICE)</strong> plots show PDP for <strong>each instance</strong> reveals variation.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Local Interpretable Model-agnostic Explanations (LIME)</h2>
    <div class="content">
        <p><em>(Ribeiro et al., 2016)</em></p>
        
        <h3>Core Idea</h3>
        <p>Approximate complex model locally around a prediction with a <strong>simple, interpretable model</strong> (e.g., linear regression).</p>
        
        <h3>Algorithm</h3>
        <ol>
            <li>Select instance \( \mathbf{x} \) to explain.</li>
            <li>Generate <strong>perturbed samples</strong> \( \mathbf{x}' \) near \( \mathbf{x} \).</li>
            <li>Get predictions \( f(\mathbf{x}') \) from black-box model.</li>
            <li>Weight samples by <strong>proximity to \( \mathbf{x} \)</strong> (e.g., kernel distance).</li>
            <li>Train <strong>interpretable model</strong> \( g \) (e.g., sparse linear model) on weighted samples.</li>
            <li>Use \( g \) to explain \( f(\mathbf{x}) \).</li>
        </ol>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>Model-agnostic</strong>: Works with any \( f \).</li>
            <li><strong>Local fidelity</strong>: Accurate near \( \mathbf{x} \).</li>
            <li><strong>Intuitive</strong>: Shows which features pushed prediction up/down.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Instability</strong>: Small changes in \( \mathbf{x} \) → different explanations.</li>
            <li><strong>Kernel width sensitive</strong>: Poor choice → bad local approximation.</li>
            <li><strong>Fails with correlated features</strong>: Perturbations may create unrealistic samples.</li>
        </ul>
        
        <div class="definition">
            🎯 <strong>Use Case</strong>: Explaining individual predictions in text (e.g., "Why was this email classified as spam?").
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">SHapley Additive exPlanations (SHAP)</h2>
    <div class="content">
        <p><em>(Lundberg & Lee, 2017)</em></p>
        
        <h3>Core Idea</h3>
        <p>Assign each feature an importance value for a prediction using <strong>Shapley values</strong> from cooperative game theory.</p>
        
        <h3>Shapley Value Formula</h3>
        <p>For feature \( i \), its contribution to prediction \( f(\mathbf{x}) \) is:</p>
        <div class="equation">
            \[ \phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|! (|F| - |S| - 1)!}{|F|!} \left[ f(S \cup \{i\}) - f(S) \right] \]
        </div>
        <ul>
            <li>\( F \): All features</li>
            <li>\( S \): Subset of features</li>
            <li>\( f(S) \): Model prediction with only features in \( S \) (others "missing")</li>
        </ul>
        
        <h3>Key Properties (Why SHAP is Theoretically Grounded)</h3>
        <ol>
            <li><strong>Local accuracy</strong>: \( f(\mathbf{x}) = \phi_0 + \sum \phi_i \)</li>
            <li><strong>Missingness</strong>: If feature missing, \( \phi_i = 0 \)</li>
            <li><strong>Consistency</strong>: If feature contributes more in one model, \( \phi_i \) doesn't decrease</li>
        </ol>
        
        <h3>Practical Implementations</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>KernelSHAP</strong></td>
                        <td>Model-agnostic (slow; uses LIME-like sampling)</td>
                    </tr>
                    <tr>
                        <td><strong>TreeSHAP</strong></td>
                        <td>For tree ensembles (exact, fast)</td>
                    </tr>
                    <tr>
                        <td><strong>DeepSHAP</strong></td>
                        <td>For deep networks (approximate)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Visualization</h3>
        <ul>
            <li><strong>SHAP summary plot</strong>: Global feature importance + impact direction</li>
            <li><strong>SHAP dependence plot</strong>: Like PDP but shows interaction effects</li>
            <li><strong>Force plot</strong>: Local explanation (shows +/- contributions)</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>Theoretically sound</strong> (only method with all 3 properties above)</li>
            <li><strong>Unified framework</strong> for global + local explanations</li>
            <li><strong>Handles feature interactions</strong></li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Computationally expensive</strong> for KernelSHAP (\( O(2^M) \) for M features)</li>
            <li><strong>Baseline choice matters</strong>: \( f(\emptyset) \) requires defining "missing" features</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>: Use <strong>TreeSHAP</strong> for XGBoost/Random Forest; <strong>KernelSHAP</strong> for other models.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Counterfactual Explanations</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Answer: <strong>"What minimal changes would flip the prediction?"</strong><br>
        <em>Example</em>: "Your loan was denied. If your income were $5K higher, it would be approved."</p>
        
        <h3>Formal Definition</h3>
        <p>Find \( \mathbf{x}' \) such that:</p>
        <ul>
            <li>\( f(\mathbf{x}') \neq f(\mathbf{x}) \) (different prediction)</li>
            <li>\( \text{distance}(\mathbf{x}, \mathbf{x}') \) is minimized</li>
            <li>\( \mathbf{x}' \) is <strong>plausible</strong> (e.g., respects data manifold)</li>
        </ul>
        
        <h3>Methods</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>DiCE (Diverse Counterfactuals)</strong></td>
                        <td>Generates multiple diverse, feasible counterfactuals</td>
                    </tr>
                    <tr>
                        <td><strong>Gradient-based optimization</strong></td>
                        <td>Minimize \( \|\mathbf{x} - \mathbf{x}'\| + \lambda \cdot \text{loss}(f(\mathbf{x}'), y_{\text{target}}) \)</td>
                    </tr>
                    <tr>
                        <td><strong>VAE-based</strong></td>
                        <td>Use latent space of autoencoder to ensure plausibility</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Desirable Properties</h3>
        <ul>
            <li><strong>Validity</strong>: Flips prediction</li>
            <li><strong>Proximity</strong>: Close to original input</li>
            <li><strong>Sparsity</strong>: Changes few features</li>
            <li><strong>Plausibility</strong>: Lies in data distribution</li>
            <li><strong>Diversity</strong>: Multiple options for user</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li>Loan/insurance decisions</li>
            <li>Medical diagnosis ("What if your cholesterol were lower?")</li>
            <li>Algorithmic recourse</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Advantage over LIME/SHAP</strong>: Actionable tells user <strong>what to do</strong> to change outcome.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: XAI Methods Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Type</th>
                        <th>Global/Local</th>
                        <th>Model-Agnostic?</th>
                        <th>Key Strength</th>
                        <th>Key Weakness</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Permutation Importance</strong></td>
                        <td>Feature importance</td>
                        <td>Global</td>
                        <td>✅</td>
                        <td>Simple, reliable</td>
                        <td>Misses interactions</td>
                    </tr>
                    <tr>
                        <td><strong>PDP</strong></td>
                        <td>Feature effect</td>
                        <td>Global</td>
                        <td>✅</td>
                        <td>Shows functional form</td>
                        <td>Assumes independence</td>
                    </tr>
                    <tr>
                        <td><strong>LIME</strong></td>
                        <td>Local surrogate</td>
                        <td>Local</td>
                        <td>✅</td>
                        <td>Intuitive, flexible</td>
                        <td>Unstable, unrealistic perturbations</td>
                    </tr>
                    <tr>
                        <td><strong>SHAP</strong></td>
                        <td>Game-theoretic</td>
                        <td>Both</td>
                        <td>✅ (with approximations)</td>
                        <td>Theoretically grounded</td>
                        <td>Computationally heavy</td>
                    </tr>
                    <tr>
                        <td><strong>Counterfactuals</strong></td>
                        <td>Recourse</td>
                        <td>Local</td>
                        <td>✅</td>
                        <td>Actionable, user-centric</td>
                        <td>Hard to ensure plausibility</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>SHAP</strong> is the <strong>gold standard</strong> for feature attribution use it when possible.</li>
            <li><strong>LIME</strong> is intuitive but <strong>less reliable</strong> validate explanations.</li>
            <li><strong>PDP</strong> is great for global trends but <strong>beware of correlated features</strong>.</li>
            <li><strong>Counterfactuals</strong> provide <strong>actionable insights</strong> critical for high-stakes decisions.</li>
            <li><strong>No single method is perfect</strong>: Combine approaches (e.g., SHAP + counterfactuals).</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Ethical Note</strong>:<br>
            Explanations can be <strong>gamed</strong> or <strong>misinterpreted</strong>. Always pair XAI with <strong>human oversight</strong> and <strong>domain expertise</strong>.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Model-Specific Explanations</h2>
    <div class="content">
        <p><em>Architecture-Aware Interpretability for Transparent AI</em></p>
        
        <div class="definition">
            <strong>Why Model-Specific?</strong><br>
            While model-agnostic methods (e.g., LIME, SHAP) work universally, <strong>model-specific techniques</strong> leverage internal structure for <strong>more accurate, efficient, and insightful explanations</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Decision Tree Visualization</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Decision trees are <strong>inherently interpretable</strong> their logic is a series of human-readable if-else rules.</p>
        
        <h3>Visualization Techniques</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Description</th>
                        <th>Tools</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Tree Plot</strong></td>
                        <td>Graphical representation of nodes, splits, and leaf predictions</td>
                        <td><code>sklearn.tree.plot_tree</code>, Graphviz</td>
                    </tr>
                    <tr>
                        <td><strong>Text Rules</strong></td>
                        <td>Export tree as logical rules (e.g., "IF age > 30 AND income < 50K THEN class=0")</td>
                        <td><code>sklearn.tree.export_text</code></td>
                    </tr>
                    <tr>
                        <td><strong>Feature Importance</strong></td>
                        <td>Gini/entropy reduction per feature (global)</td>
                        <td>Built into scikit-learn</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Example Output (Text Rule)</h3>
        <pre>if income <= 50000:
    if age <= 30:
        class = Rejected
    else:
        class = Approved
else:
    class = Approved</pre>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>Exact fidelity</strong>: Explanation = model.</li>
            <li><strong>Global + local</strong>: Entire tree explains global behavior; path to leaf explains single prediction.</li>
            <li><strong>No approximation error</strong>.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li>Trees deeper than ~5 levels become <strong>unwieldy</strong>.</li>
            <li><strong>Ensembles (e.g., Random Forest)</strong> lose interpretability require post-hoc methods (e.g., SHAP).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>: Use shallow trees (< 4 levels) for high-stakes decisions (e.g., medical triage).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Neural Network Interpretation Techniques</h2>
    <div class="content">
        <h3>Challenges</h3>
        <p>Deep networks are <strong>high-dimensional, non-linear, and distributed</strong> → no single "feature weight" explains behavior.</p>
        
        <h3>Key Techniques</h3>
        
        <h4>(a) Saliency Maps</h4>
        <ul>
            <li><strong>Idea</strong>: Compute gradient of output w.r.t. input pixels:<br>
            \[ \text{Saliency}(\mathbf{x}) = \left| \frac{\partial f_c(\mathbf{x})}{\partial \mathbf{x}} \right| \]
            where \( f_c \) = logit for class \( c \).</li>
            <li><strong>Shows</strong>: Which input pixels most influence prediction.</li>
            <li><strong>Limitation</strong>: Noisy; highlights edges more than semantics.</li>
        </ul>
        
        <h4>(b) Grad-CAM (Gradient-weighted Class Activation Mapping)</h4>
        <ul>
            <li><strong>Idea</strong>: Use gradients flowing into final <strong>convolutional layer</strong> to weight feature maps.</li>
            <li><strong>Steps</strong>:
            <ol>
                <li>Compute gradient of class score w.r.t. feature map \( A^k \).</li>
                <li>Global average pool gradient → importance weight \( \alpha_k \).</li>
                <li>Weighted sum: \( L^c_{\text{Grad-CAM}} = \text{ReLU}\left( \sum_k \alpha_k A^k \right) \)</li>
            </ol>
            </li>
            <li><strong>Output</strong>: Heatmap highlighting <strong>discriminative regions</strong> (e.g., cat's face, not background).</li>
            <li><strong>Advantage</strong>: Works with <strong>any CNN architecture</strong>; class-discriminative.</li>
        </ul>
        
        <h4>(c) Layer-wise Relevance Propagation (LRP)</h4>
        <ul>
            <li><strong>Idea</strong>: Backpropagate prediction score to input using <strong>conservation principle</strong> (relevance = constant).</li>
            <li><strong>Rule</strong>: Redistribute relevance from layer to layer via propagation rules (e.g., \( z^+ \)-rule).</li>
            <li><strong>Output</strong>: Pixel-wise relevance scores (positive = supportive, negative = inhibitory).</li>
        </ul>
        
        <h4>(d) Activation Maximization</h4>
        <ul>
            <li><strong>Idea</strong>: Find input \( \mathbf{x}^* \) that maximally activates a neuron/filter:<br>
            \[ \mathbf{x}^* = \arg\max_{\mathbf{x}} f_k(\mathbf{x}) - \lambda \|\mathbf{x}\|^2 \]</li>
            <li><strong>Shows</strong>: What pattern a neuron "looks for" (e.g., edges, textures, objects).</li>
        </ul>
        
        <div class="definition">
            🖼️ <strong>Use Case</strong>:<br>
            - <strong>Medical imaging</strong>: Grad-CAM highlights tumor regions.<br>
            - <strong>Autonomous driving</strong>: Saliency maps show attention to pedestrians.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Attention Mechanism Visualization</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Attention weights <strong>explicitly indicate</strong> which parts of input the model focuses on making Transformers <strong>self-explaining</strong>.</p>
        
        <h3>Visualization Methods</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>How It Works</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Attention Heatmaps</strong></td>
                        <td>Plot attention weights \( \alpha_{ij} \) between tokens \( i \) (query) and \( j \) (key)</td>
                        <td>NLP: Show which words influence prediction</td>
                    </tr>
                    <tr>
                        <td><strong>Head Analysis</strong></td>
                        <td>Visualize individual attention heads (some learn syntax, others semantics)</td>
                        <td>Debug model behavior</td>
                    </tr>
                    <tr>
                        <td><strong>Integrated Gradients + Attention</strong></td>
                        <td>Combine attention with gradient-based methods for robustness</td>
                        <td>Reduce noise in attention maps</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Example (Sentiment Analysis)</h3>
        <ul>
            <li>Input: "The movie was <strong>not good</strong>."</li>
            <li>Attention heatmap shows high weight on "<strong>not</strong>" and "<strong>good</strong>" → correctly predicts negative sentiment.</li>
        </ul>
        
        <h3>Caveats</h3>
        <ul>
            <li><strong>Attention ≠ Explanation</strong>: High attention doesn't always mean causal importance (Jain & Wallace, 2019).</li>
            <li><strong>Multi-head complexity</strong>: 12+ heads in BERT → hard to interpret all.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Best Practice</strong>: Use attention as <strong>hypothesis generator</strong>, not proof of causality.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Neural Network (GNN) Explanations</h2>
    <div class="content">
        <h3>Challenge</h3>
        <p>GNNs aggregate neighborhood information → hard to isolate which nodes/edges drove prediction.</p>
        
        <h3>Key Methods</h3>
        
        <h4>(a) GNNExplainer (Ying et al., 2019)</h4>
        <ul>
            <li><strong>Idea</strong>: Find <strong>minimal subgraph</strong> and <strong>feature dimensions</strong> that maximize prediction confidence.</li>
            <li><strong>Optimization</strong>:<br>
            \[ \max_{M, F} \ \text{MI}(Y, \hat{Y}_{M,F}) - \lambda_1 \|M\|_1 - \lambda_2 \|F\|_1 \]
            where \( M \) = edge mask, \( F \) = feature mask.</li>
            <li><strong>Output</strong>: Subgraph highlighting <strong>critical edges/nodes</strong>.</li>
        </ul>
        
        <h4>(b) PGExplainer (Luo et al., 2020)</h4>
        <ul>
            <li><strong>Idea</strong>: Train a <strong>parameterized explainer</strong> to predict edge importance for any graph.</li>
            <li><strong>Advantage</strong>: <strong>Inductive</strong> generalizes to unseen graphs.</li>
        </ul>
        
        <h4>(c) SubgraphX (Yuan et al., 2021)</h4>
        <ul>
            <li><strong>Idea</strong>: Use <strong>Monte Carlo Tree Search (MCTS)</strong> to find optimal subgraph with Shapley values.</li>
            <li><strong>Output</strong>: Faithful, minimal explanations.</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Drug discovery</strong>: Which atoms/bonds make molecule toxic?</li>
            <li><strong>Fraud detection</strong>: Which transaction links indicate fraud?</li>
            <li><strong>Recommendation</strong>: Why was this item recommended?</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Visualization</strong>: Highlight important nodes/edges in graph plot (e.g., using PyG or NetworkX).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Rule Extraction from Complex Models</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Approximate black-box models (e.g., neural nets, ensembles) with <strong>human-readable rules</strong>.</p>
        
        <h3>Approaches</h3>
        
        <h4>(a) Pedagogical Methods</h4>
        <ul>
            <li>Treat black-box as <strong>oracle</strong>; train rule-based model on its predictions.</li>
            <li><strong>Algorithms</strong>:
            <ul>
                <li><strong>Decision trees</strong> (e.g., CART) trained on \( (\mathbf{x}, f(\mathbf{x})) \)</li>
                <li><strong>Rule lists</strong> (e.g., CORELS): Optimal sparse rule sets</li>
            </ul>
            </li>
        </ul>
        
        <h4>(b) Decompositional Methods</h4>
        <ul>
            <li>Extract rules from <strong>internal structure</strong> of model.</li>
            <li><strong>Example</strong>:
            <ul>
                <li>For a <strong>neural net with discretized activations</strong>, trace paths from input to output.</li>
                <li>For <strong>Random Forest</strong>, combine tree paths into rules.</li>
            </ul>
            </li>
        </ul>
        
        <h4>(c) Anchors (Ribeiro et al., 2018)</h4>
        <ul>
            <li><strong>Idea</strong>: Find <strong>sufficient conditions</strong> (anchor) that guarantee prediction:<br>
            > "IF income > $50K AND credit_score > 700, THEN approved (with 95% precision)"</li>
            <li><strong>Advantage</strong>: High <strong>coverage</strong> and <strong>precision</strong>; more stable than LIME.</li>
        </ul>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Fidelity-interpretability trade-off</strong>: Simpler rules → lower fidelity.</li>
            <li><strong>Scalability</strong>: Rule extraction is NP-hard for large models.</li>
        </ul>
        
        <div class="definition">
            📜 <strong>Use Case</strong>:<br>
            - <strong>Credit scoring</strong>: Replace black-box with certified rule set.<br>
            - <strong>Healthcare</strong>: Extract diagnostic rules from deep models.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Model-Specific Explanation Methods</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Model Type</th>
                        <th>Explanation Method</th>
                        <th>Key Output</th>
                        <th>Strength</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Decision Tree</strong></td>
                        <td>Tree plot / text rules</td>
                        <td>If-else logic</td>
                        <td>Exact, global + local</td>
                    </tr>
                    <tr>
                        <td><strong>CNN</strong></td>
                        <td>Grad-CAM, Saliency Maps</td>
                        <td>Heatmaps</td>
                        <td>Visual, class-discriminative</td>
                    </tr>
                    <tr>
                        <td><strong>Transformer</strong></td>
                        <td>Attention heatmaps</td>
                        <td>Token-token weights</td>
                        <td>Built-in, scalable</td>
                    </tr>
                    <tr>
                        <td><strong>GNN</strong></td>
                        <td>GNNExplainer, PGExplainer</td>
                        <td>Critical subgraph</td>
                        <td>Structure-aware</td>
                    </tr>
                    <tr>
                        <td><strong>Any (Rule Extraction)</strong></td>
                        <td>Anchors, CORELS</td>
                        <td>IF-THEN rules</td>
                        <td>Actionable, high precision</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Decision trees</strong> are the gold standard for interpretability use when possible.</li>
            <li><strong>Grad-CAM</strong> is the go-to for CNN explanation; <strong>attention maps</strong> for Transformers.</li>
            <li><strong>GNNExplainer</strong> reveals critical graph structures essential for scientific discovery.</li>
            <li><strong>Rule extraction</strong> bridges black-box performance and regulatory compliance.</li>
            <li><strong>Always validate</strong>: Model-specific explanations can still be <strong>misleading</strong> (e.g., attention ≠ importance).</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Ethical Reminder</strong>:<br>
            Even "interpretable" models can encode bias. <strong>Explanations must be audited for fairness and robustness</strong>.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Trust and Transparency in AI</h2>
    <div class="content">
        <p><em>From Technical Explanations to Ethical Deployment</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>Explainability alone is not enough</strong> AI systems must be <strong>fair, user-aligned, ethically sound, and compliant</strong> to earn genuine trust.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Building Trust in AI Systems</h2>
    <div class="content">
        <h3>What is Trust in AI?</h3>
        <p><strong>Definition</strong>: A user's <strong>willingness to rely on</strong> an AI system based on perceived <strong>competence, reliability, and intent</strong>.</p>
        
        <h3>Dimensions of Trust (Adapted from Psychology)</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Dimension</th>
                        <th>AI Manifestation</th>
                        <th>How to Foster</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Competence</strong></td>
                        <td>System performs accurately and robustly</td>
                        <td>High accuracy, uncertainty quantification</td>
                    </tr>
                    <tr>
                        <td><strong>Predictability</strong></td>
                        <td>Behavior is consistent and understandable</td>
                        <td>Clear explanations, stable outputs</td>
                    </tr>
                    <tr>
                        <td><strong>Reliability</strong></td>
                        <td>Works consistently across contexts</td>
                        <td>Rigorous testing, monitoring</td>
                    </tr>
                    <tr>
                        <td><strong>Integrity</strong></td>
                        <td>Acts in user's best interest</td>
                        <td>Fairness, transparency, no hidden manipulation</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Strategies to Build Trust</h3>
        <ul>
            <li><strong>Provide appropriate explanations</strong>: Match explanation type to user needs (see Section 3).</li>
            <li><strong>Show uncertainty</strong>: Use confidence scores or "I don't know" responses.</li>
            <li><strong>Enable user control</strong>: Allow overrides, feedback loops, and recourse.</li>
            <li><strong>Be transparent about limitations</strong>: Disclose training data scope, known failure modes.</li>
            <li><strong>Demonstrate consistency</strong>: Same input → same output (unless stochasticity is explained).</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Key Insight</strong>:<br>
            Trust is <strong>earned over time</strong> through repeated positive interactions not just a one-time explanation.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Fairness and Bias in AI Explanations</h2>
    <div class="content">
        <h3>The Problem</h3>
        <p>Explanations can <strong>mask, amplify, or legitimize bias</strong> even if the model is accurate.</p>
        <p><strong>Example</strong>: A loan model denies applicants from a ZIP code; explanation says "low income," but ZIP code is a proxy for race.</p>
        
        <h3>Types of Bias in Explanations</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Bias Type</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Feature Bias</strong></td>
                        <td>Explanations highlight biased features as "important"</td>
                        <td>Using gender to explain hiring decisions</td>
                    </tr>
                    <tr>
                        <td><strong>Omission Bias</strong></td>
                        <td>Failing to mention protected attributes that influence outcome</td>
                        <td>Not revealing race was used indirectly</td>
                    </tr>
                    <tr>
                        <td><strong>Confirmation Bias</strong></td>
                        <td>Explanations reinforce user's preconceptions</td>
                        <td>Showing only data that supports a diagnosis</td>
                    </tr>
                    <tr>
                        <td><strong>Algorithmic Bias</strong></td>
                        <td>Explanation method itself is biased (e.g., SHAP vs. LIME give different rankings)</td>
                        <td>Permutation importance fails with correlated features</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Mitigation Strategies</h3>
        <ul>
            <li><strong>Audit explanations for fairness</strong>: Use metrics like <strong>explanation parity</strong> (do similar individuals get similar explanations?).</li>
            <li><strong>Include counterfactuals</strong>: Show what <em>would</em> change the outcome reveals recourse gaps.</li>
            <li><strong>Use causal explanations</strong>: Distinguish correlation from causation (e.g., "ZIP code → denial" vs. "income → denial").</li>
            <li><strong>Diverse evaluation</strong>: Test explanations across subgroups (race, gender, age).</li>
        </ul>
        
        <div class="definition">
            ⚖️ <strong>Ethical Imperative</strong>:<br>
            An explanation that <strong>hides systemic bias</strong> is worse than no explanation it creates <strong>false legitimacy</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">User-Centered Explanation Design</h2>
    <div class="content">
        <h3>Core Principle</h3>
        <div class="definition">
            <strong>"Explanations are for humans, not algorithms."</strong><br>
            Design must align with <strong>user goals, expertise, and context</strong>.
        </div>
        
        <h3>User Typology and Explanation Needs</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>User Type</th>
                        <th>Goal</th>
                        <th>Preferred Explanation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>End User</strong> (e.g., patient, loan applicant)</td>
                        <td>Understand outcome; seek recourse</td>
                        <td><strong>Counterfactuals</strong>, simple feature highlights</td>
                    </tr>
                    <tr>
                        <td><strong>Domain Expert</strong> (e.g., doctor, engineer)</td>
                        <td>Verify model logic; integrate into workflow</td>
                        <td><strong>Local feature importance</strong>, uncertainty estimates</td>
                    </tr>
                    <tr>
                        <td><strong>Data Scientist</strong></td>
                        <td>Debug model; improve performance</td>
                        <td><strong>Global feature importance</strong>, error analysis</td>
                    </tr>
                    <tr>
                        <td><strong>Regulator / Auditor</strong></td>
                        <td>Ensure compliance; assess risk</td>
                        <td><strong>Rule extraction</strong>, fairness metrics, audit trails</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Design Guidelines (Based on HCI Research)</h3>
        <ol>
            <li><strong>Contrastive</strong>: Explain <strong>why this, not that?</strong> (e.g., "Approved because income > $50K, not because of age").</li>
            <li><strong>Selective</strong>: Focus on <strong>2–5 key factors</strong> avoid overwhelming users.</li>
            <li><strong>Social</strong>: Mimic how humans explain (e.g., use analogies, narratives).</li>
            <li><strong>Interactive</strong>: Allow users to <strong>ask follow-ups</strong> (e.g., "What if I change X?").</li>
            <li><strong>Contextual</strong>: Adapt to task (e.g., high-stakes decisions need more detail).</li>
        </ol>
        
        <h3>Evaluation Metrics for User-Centered XAI</h3>
        <ul>
            <li><strong>User satisfaction</strong> (surveys)</li>
            <li><strong>Task performance</strong> (e.g., faster decisions, fewer errors)</li>
            <li><strong>Mental model accuracy</strong> (can user predict model behavior?)</li>
            <li><strong>Trust calibration</strong> (trust matches actual reliability)</li>
        </ul>
        
        <div class="definition">
            🎯 <strong>Best Practice</strong>:<br>
            Conduct <strong>user studies</strong> don't assume what users need. A doctor and a patient want <em>very different</em> explanations.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Ethical Considerations in Explainable AI</h2>
    <div class="content">
        <h3>Key Ethical Tensions</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Tension</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Transparency vs. Privacy</strong></td>
                        <td>Detailed explanations may leak training data (e.g., membership inference)</td>
                    </tr>
                    <tr>
                        <td><strong>Explainability vs. Intellectual Property</strong></td>
                        <td>Companies may hide model details to protect IP</td>
                    </tr>
                    <tr>
                        <td><strong>Autonomy vs. Paternalism</strong></td>
                        <td>Too much explanation may overwhelm; too little reduces agency</td>
                    </tr>
                    <tr>
                        <td><strong>Accountability vs. Automation Bias</strong></td>
                        <td>Users may over-trust "explained" AI, reducing critical thinking</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Core Ethical Principles (from EU AI Ethics Guidelines)</h3>
        <ol>
            <li><strong>Human agency and oversight</strong>: AI must not manipulate or coerce.</li>
            <li><strong>Technical robustness and safety</strong>: Include uncertainty and fail-safes.</li>
            <li><strong>Privacy and data governance</strong>: Explanations must not violate privacy.</li>
            <li><strong>Diversity, non-discrimination</strong>: Explanations must not reinforce bias.</li>
            <li><strong>Societal and environmental well-being</strong>: Consider broader impact.</li>
            <li><strong>Accountability</strong>: Clear responsibility for AI outcomes.</li>
        </ol>
        
        <h3>Red Lines</h3>
        <ul>
            <li><strong>Deceptive explanations</strong>: "Lying" to make AI seem fairer/safer than it is.</li>
            <li><strong>Explanation washing</strong>: Providing superficial explanations to appear compliant.</li>
            <li><strong>Ignoring power dynamics</strong>: Explanations that favor institutions over individuals (e.g., "You were denied because of your data" not systemic issues).</li>
        </ul>
        
        <div class="definition">
            🌍 <strong>Global Perspective</strong>:<br>
            Ethics must account for <strong>cultural differences</strong> (e.g., individualism vs. collectivism in explanation preferences).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Regulatory Requirements for Explainability</h2>
    <div class="content">
        <h3>Key Regulations</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Regulation</th>
                        <th>Jurisdiction</th>
                        <th>Explainability Requirement</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>GDPR (Article 22)</strong></td>
                        <td>EU</td>
                        <td>Right not to be subject to <strong>automated decision-making</strong> producing legal effects; <strong>meaningful information about logic</strong></td>
                    </tr>
                    <tr>
                        <td><strong>EU AI Act (2024)</strong></td>
                        <td>EU</td>
                        <td><strong>High-risk AI systems</strong> must be <strong>transparent, traceable, and documented</strong>; provide <strong>accessible explanations</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Algorithmic Accountability Act (proposed)</strong></td>
                        <td>USA</td>
                        <td>Require <strong>impact assessments</strong> for automated systems, including bias and explainability</td>
                    </tr>
                    <tr>
                        <td><strong>NIST AI RMF</strong></td>
                        <td>USA (Guideline)</td>
                        <td>Recommends <strong>explainability</strong> as core component of trustworthy AI</td>
                    </tr>
                    <tr>
                        <td><strong>CPRA / State Laws</strong></td>
                        <td>California, USA</td>
                        <td>Similar to GDPR for automated decision-making</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>What "Meaningful Information" Means (GDPR Guidance)</h3>
        <p>Not just model architecture but <strong>factors weighted in decision</strong>, <strong>importance of variables</strong>, and <strong>how to contest</strong>.</p>
        <p><strong>Example</strong>: A credit denial must explain:</p>
        <blockquote>"Your application was denied primarily due to high debt-to-income ratio (weight: 60%) and short credit history (weight: 30%). You may appeal by providing updated income documentation."</blockquote>
        
        <h3>Compliance Best Practices</h3>
        <ul>
            <li><strong>Document everything</strong>: Model cards, data sheets, explanation methods used.</li>
            <li><strong>Implement right to explanation</strong>: Build APIs to generate explanations on request.</li>
            <li><strong>Conduct bias audits</strong>: Include explanation fairness in testing.</li>
            <li><strong>Train staff</strong>: Ensure customer service can interpret and communicate explanations.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Penalties</strong>:<br>
            GDPR fines up to <strong>€20M or 4% of global revenue</strong>; EU AI Act fines up to <strong>€35M or 7% of revenue</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Trust & Transparency Framework</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Pillar</th>
                        <th>Key Question</th>
                        <th>Actionable Step</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Trust Building</strong></td>
                        <td>Do users understand and rely on the system?</td>
                        <td>Provide uncertainty estimates + user control</td>
                    </tr>
                    <tr>
                        <td><strong>Fairness</strong></td>
                        <td>Do explanations hide or reveal bias?</td>
                        <td>Audit explanations across subgroups</td>
                    </tr>
                    <tr>
                        <td><strong>User-Centered Design</strong></td>
                        <td>Is the explanation useful to <em>this</em> user?</td>
                        <td>Conduct user studies; tailor by role</td>
                    </tr>
                    <tr>
                        <td><strong>Ethics</strong></td>
                        <td>Does the explanation respect human dignity?</td>
                        <td>Avoid deception; prioritize user autonomy</td>
                    </tr>
                    <tr>
                        <td><strong>Regulation</strong></td>
                        <td>Are we compliant with "right to explanation"?</td>
                        <td>Implement GDPR/EU AI Act-compliant workflows</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Trust is multidimensional</strong>: Accuracy + transparency + fairness + user alignment.</li>
            <li><strong>Explanations can harm</strong>: Biased or misleading explanations erode trust faster than no explanation.</li>
            <li><strong>One-size-fits-all fails</strong>: A doctor, patient, and regulator need different explanations.</li>
            <li><strong>Ethics ≠ compliance</strong>: Go beyond legal minimums to build truly responsible AI.</li>
            <li><strong>Regulation is accelerating</strong>: GDPR and EU AI Act set global standards design for compliance early.</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Final Thought</strong>:<br>
            <strong>"Explainable AI is not a feature it's a commitment to human dignity in the age of automation."</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Edge Computing</h2>
    <div class="content">
        <p><em>Bringing Computation Closer to the Data Source</em></p>
        
        <div class="definition">
            <strong>Core Motivation</strong>:<br>
            Reduce <strong>latency</strong>, <strong>bandwidth usage</strong>, and <strong>cloud dependency</strong> by processing data <strong>near its source</strong> enabling real-time, scalable, and resilient applications.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Edge Computing Principles</h2>
    <div class="content">
        <h3>What is Edge Computing?</h3>
        <p>A distributed computing paradigm that <strong>moves computation, storage, and networking services closer to end-users or data sources</strong> (e.g., sensors, cameras, mobile devices).</p>
        
        <h3>Key Principles</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Principle</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Proximity</strong></td>
                        <td>Compute resources are physically near data producers (e.g., base stations, routers, on-premise servers)</td>
                    </tr>
                    <tr>
                        <td><strong>Context Awareness</strong></td>
                        <td>Leverages local context (location, time, user identity) for intelligent decisions</td>
                    </tr>
                    <tr>
                        <td><strong>Real-Time Processing</strong></td>
                        <td>Enables ultra-low-latency responses (< 10–100 ms)</td>
                    </tr>
                    <tr>
                        <td><strong>Bandwidth Optimization</strong></td>
                        <td>Filters, aggregates, or processes data locally → reduces cloud traffic</td>
                    </tr>
                    <tr>
                        <td><strong>Autonomy</strong></td>
                        <td>Operates during cloud outages (critical for industrial IoT, autonomous vehicles)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Use Cases</h3>
        <ul>
            <li><strong>Smart cities</strong>: Traffic light optimization using real-time camera feeds</li>
            <li><strong>Industrial IoT</strong>: Predictive maintenance on factory equipment</li>
            <li><strong>Autonomous vehicles</strong>: On-board perception and decision-making</li>
            <li><strong>AR/VR</strong>: Low-latency rendering for immersive experiences</li>
            <li><strong>Healthcare</strong>: Real-time patient monitoring in ambulances</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Edge Computing Architecture and Models</h2>
    <div class="content">
        <h3>Typical Edge Architecture Layers</h3>
        <ol>
            <li><strong>Device Layer</strong>: Sensors, cameras, smartphones, IoT devices (data generation)</li>
            <li><strong>Edge Layer</strong>: Edge servers, gateways, micro-data centers (local processing)</li>
            <li><strong>Fog Layer</strong> (optional): Intermediate aggregation points (e.g., campus routers)</li>
            <li><strong>Cloud Layer</strong>: Centralized data centers (long-term storage, heavy analytics)</li>
        </ol>
        
        <h3>Deployment Models</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>On-Premise Edge</strong></td>
                        <td>Dedicated edge server in enterprise facility</td>
                        <td>Factory control system</td>
                    </tr>
                    <tr>
                        <td><strong>Telco Edge (MEC)</strong></td>
                        <td>Compute at cellular base stations (Multi-access Edge Computing)</td>
                        <td>5G-connected drones</td>
                    </tr>
                    <tr>
                        <td><strong>CDN Edge</strong></td>
                        <td>Content delivery networks repurposed for compute</td>
                        <td>Cloudflare Workers, AWS Lambda@Edge</td>
                    </tr>
                    <tr>
                        <td><strong>Device-Embedded</strong></td>
                        <td>Compute directly on device (e.g., smartphone, camera)</td>
                        <td>Apple Neural Engine, Tesla FSD chip</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Components</h3>
        <ul>
            <li><strong>Edge OS</strong>: Lightweight OS (e.g., Ubuntu Core, AWS IoT Greengrass)</li>
            <li><strong>Orchestration</strong>: Kubernetes (K3s, KubeEdge), OpenYurt</li>
            <li><strong>Security</strong>: Zero Trust, hardware-rooted attestation (e.g., TPM)</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Fog Computing vs. Edge Computing</h2>
    <div class="content">
        <h3>Fog Computing (Cisco, 2012)</h3>
        <ul>
            <li><strong>Definition</strong>: Extends cloud computing to the <strong>network edge</strong>, but includes <strong>intermediate layers</strong> between devices and cloud.</li>
            <li><strong>Hierarchy</strong>: Devices → Fog nodes (routers, switches) → Cloud</li>
            <li><strong>Focus</strong>: <strong>Hierarchical, system-level orchestration</strong> across LAN/MAN.</li>
        </ul>
        
        <h3>Edge Computing</h3>
        <ul>
            <li><strong>Definition</strong>: Compute happens <strong>at or near the data source</strong> often <strong>single-hop</strong> from device.</li>
            <li><strong>Focus</strong>: <strong>Ultra-low latency</strong>, device-centric processing.</li>
        </ul>
        
        <h3>Key Differences</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Fog Computing</th>
                        <th>Edge Computing</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Location</strong></td>
                        <td>Anywhere in network (LAN, MAN)</td>
                        <td>At/near data source (last mile)</td>
                    </tr>
                    <tr>
                        <td><strong>Latency</strong></td>
                        <td>Medium (10–100 ms)</td>
                        <td>Ultra-low (< 10 ms)</td>
                    </tr>
                    <tr>
                        <td><strong>Scope</strong></td>
                        <td>System-wide (multi-node coordination)</td>
                        <td>Device/application-specific</td>
                    </tr>
                    <tr>
                        <td><strong>Origin</strong></td>
                        <td>Cisco (network-centric)</td>
                        <td>Industry/academia (application-centric)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            💡 <strong>Modern View</strong>:<br>
            The terms are often used interchangeably, but <strong>edge = subset of fog</strong>. In practice, "edge computing" is the dominant term today.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Edge-Cloud Continuum</h2>
    <div class="content">
        <h3>Concept</h3>
        <p>A <strong>spectrum of compute resources</strong> from cloud data centers to end devices with <strong>seamless workload migration</strong> based on latency, cost, and resource needs.</p>
        
        <h3>Continuum Layers</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Latency</th>
                        <th>Compute Power</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Public Cloud</strong></td>
                        <td>50–200 ms</td>
                        <td>Very High</td>
                        <td>Batch analytics, model training</td>
                    </tr>
                    <tr>
                        <td><strong>Regional Edge</strong></td>
                        <td>10–50 ms</td>
                        <td>High</td>
                        <td>Video transcoding, regional AI inference</td>
                    </tr>
                    <tr>
                        <td><strong>Telco Edge (MEC)</strong></td>
                        <td>1–10 ms</td>
                        <td>Medium</td>
                        <td>AR, connected cars, smart factories</td>
                    </tr>
                    <tr>
                        <td><strong>On-Premise Edge</strong></td>
                        <td>< 1–5 ms</td>
                        <td>Low-Medium</td>
                        <td>Real-time control, robotics</td>
                    </tr>
                    <tr>
                        <td><strong>Device</strong></td>
                        <td>< 1 ms</td>
                        <td>Low</td>
                        <td>Sensor fusion, simple inference</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Dynamic Orchestration</h3>
        <p><strong>Workload placement</strong>: Decide where to run a task based on:</p>
        <ul>
            <li>Latency SLA</li>
            <li>Data gravity (keep compute near data)</li>
            <li>Cost (cloud vs. edge)</li>
            <li>Resource availability</li>
        </ul>
        <p><strong>Tools</strong>: Kubernetes with topology-aware scheduling, serverless edge (e.g., OpenFaaS)</p>
        
        <div class="definition">
            🌐 <strong>Example</strong>:<br>
            A self-driving car uses <strong>on-device AI</strong> for immediate braking, <strong>telco edge</strong> for traffic coordination, and <strong>cloud</strong> for fleet-wide model updates.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Distributed Computing Paradigms</h2>
    <div class="content">
        <h3>How Edge Fits into Broader Distributed Models</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Paradigm</th>
                        <th>Characteristics</th>
                        <th>Relation to Edge</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Cloud Computing</strong></td>
                        <td>Centralized, elastic, pay-per-use</td>
                        <td>Edge complements cloud for latency-sensitive tasks</td>
                    </tr>
                    <tr>
                        <td><strong>Grid Computing</strong></td>
                        <td>High-throughput, loosely coupled tasks</td>
                        <td>Not real-time; edge is more interactive</td>
                    </tr>
                    <tr>
                        <td><strong>Peer-to-Peer (P2P)</strong></td>
                        <td>Decentralized, node equality</td>
                        <td>Edge nodes are <strong>not equal</strong> hierarchical</td>
                    </tr>
                    <tr>
                        <td><strong>Serverless/FaaS</strong></td>
                        <td>Event-driven, ephemeral functions</td>
                        <td><strong>Edge serverless</strong> (e.g., AWS Lambda@Edge) enables per-request scaling at edge</td>
                    </tr>
                    <tr>
                        <td><strong>Microservices</strong></td>
                        <td>Modular, containerized services</td>
                        <td>Deployed across edge-cloud continuum</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Edge-Specific Challenges</h3>
        <ul>
            <li><strong>Heterogeneity</strong>: Diverse hardware (ARM/x86), OS, connectivity</li>
            <li><strong>Resource Constraints</strong>: Limited CPU, memory, power</li>
            <li><strong>Management Complexity</strong>: 1000s of geographically dispersed nodes</li>
            <li><strong>Security</strong>: Physical access risks, supply chain attacks</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Latency and Bandwidth Considerations</h2>
    <div class="content">
        <h3>Why Latency Matters</h3>
        <ul>
            <li><strong>Human perception</strong>: >100 ms delay breaks immersion (VR, gaming)</li>
            <li><strong>Control systems</strong>: Industrial robots require < 10 ms</li>
            <li><strong>Safety-critical</strong>: Autonomous vehicles need < 5 ms for emergency braking</li>
        </ul>
        
        <h3>Bandwidth Challenges</h3>
        <ul>
            <li><strong>IoT explosion</strong>: Billions of devices → massive data volumes</li>
            <li><strong>Example</strong>: A single smart factory camera can generate <strong>1 Gbps</strong> of video</li>
            <li><strong>Cost</strong>: Uploading all data to cloud is <strong>prohibitively expensive</strong></li>
        </ul>
        
        <h3>Edge as a Solution</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Cloud-Only</th>
                        <th>Edge + Cloud</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Latency</strong></td>
                        <td>50–200 ms</td>
                        <td>1–10 ms</td>
                    </tr>
                    <tr>
                        <td><strong>Bandwidth Usage</strong></td>
                        <td>100% raw data</td>
                        <td>< 10% (after filtering/aggregation)</td>
                    </tr>
                    <tr>
                        <td><strong>Cloud Cost</strong></td>
                        <td>High (storage + egress)</td>
                        <td>Reduced by 50–90%</td>
                    </tr>
                    <tr>
                        <td><strong>Resilience</strong></td>
                        <td>Fails if cloud unreachable</td>
                        <td>Continues operating locally</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Quantitative Example</h3>
        <p><strong>Smart city with 10,000 cameras</strong>:</p>
        <ul>
            <li>Raw video: 10,000 × 5 Mbps = <strong>50 Gbps</strong></li>
            <li>Edge processing (object detection only): 10,000 × 10 Kbps = <strong>100 Mbps</strong> (500× reduction)</li>
        </ul>
        
        <div class="definition">
            📉 <strong>Rule of Thumb</strong>:<br>
            <strong>"Process at the edge if latency < 50 ms or bandwidth > 1 Mbps per device."</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Insight</th>
                        <th>Real-World Impact</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Edge Principles</strong></td>
                        <td>Proximity + autonomy enable real-time AI</td>
                        <td>Self-driving cars, remote surgery</td>
                    </tr>
                    <tr>
                        <td><strong>Architecture</strong></td>
                        <td>Hierarchical: device → edge → cloud</td>
                        <td>Flexible deployment across industries</td>
                    </tr>
                    <tr>
                        <td><strong>Fog vs. Edge</strong></td>
                        <td>Fog = broader network layer; Edge = device-adjacent</td>
                        <td>Edge dominates modern discourse</td>
                    </tr>
                    <tr>
                        <td><strong>Edge-Cloud Continuum</strong></td>
                        <td>Seamless workload placement across layers</td>
                        <td>Optimize cost, latency, and performance</td>
                    </tr>
                    <tr>
                        <td><strong>Distributed Paradigms</strong></td>
                        <td>Edge extends cloud/serverless to the last mile</td>
                        <td>Enables scalable IoT and 5G apps</td>
                    </tr>
                    <tr>
                        <td><strong>Latency/Bandwidth</strong></td>
                        <td>Edge reduces cloud traffic by 10–1000×</td>
                        <td>Cuts costs, enables real-time use cases</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Edge computing is not "cloud vs. edge"</strong> it's <strong>cloud + edge</strong> working together.</li>
            <li><strong>Latency and bandwidth</strong> are the primary drivers not just cost.</li>
            <li><strong>Fog computing</strong> is a historical term; <strong>edge computing</strong> is the practical standard today.</li>
            <li><strong>Orchestration</strong> (e.g., Kubernetes at edge) is critical for managing scale.</li>
            <li><strong>Security and heterogeneity</strong> remain top challenges in edge deployments.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Future Trend</strong>:<br>
            <strong>AI at the edge</strong> (TinyML, on-device learning) will blur the line between edge and device enabling truly autonomous systems.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Edge Optimization</h2>
    <div class="content">
        <p><em>Efficient AI and Computing for Resource-Constrained Environments</em></p>
        
        <div class="definition">
            <strong>Core Goal</strong>:<br>
            Maximize performance (accuracy, throughput) while minimizing <strong>latency, memory, energy, and bandwidth</strong> on edge devices.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Model Compression Techniques</h2>
    <div class="content">
        <h3>Why Compress Models?</h3>
        <p>Edge devices have limited:</p>
        <ul>
            <li><strong>Memory</strong> (RAM/ROM): Often < 1 GB</li>
            <li><strong>Compute</strong>: Low-power CPUs, no GPUs</li>
            <li><strong>Energy</strong>: Battery-operated (e.g., drones, wearables)</li>
        </ul>
        <p>Large models (e.g., ResNet-50: 95 MB, BERT: 440 MB) are impractical.</p>
        
        <h3>Key Compression Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>How It Works</th>
                        <th>Compression Ratio</th>
                        <th>Accuracy Impact</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Pruning</strong></td>
                        <td>Remove redundant weights/neurons</td>
                        <td>2–10×</td>
                        <td>Low (if done carefully)</td>
                    </tr>
                    <tr>
                        <td><strong>Quantization</strong></td>
                        <td>Reduce numerical precision (e.g., FP32 → INT8)</td>
                        <td>2–4×</td>
                        <td>Very low (often <1% drop)</td>
                    </tr>
                    <tr>
                        <td><strong>Knowledge Distillation</strong></td>
                        <td>Train small "student" to mimic large "teacher"</td>
                        <td>3–20×</td>
                        <td>Moderate (depends on task)</td>
                    </tr>
                    <tr>
                        <td><strong>Low-Rank Factorization</strong></td>
                        <td>Decompose weight matrices (e.g., SVD)</td>
                        <td>2–5×</td>
                        <td>Moderate</td>
                    </tr>
                    <tr>
                        <td><strong>Neural Architecture Search (NAS)</strong></td>
                        <td>Auto-design efficient architectures (e.g., MobileNet)</td>
                        <td>5–50×</td>
                        <td>Optimized for target hardware</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>: Combine techniques (e.g., <strong>prune → quantize → distill</strong>) for multiplicative gains.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantization and Pruning</h2>
    <div class="content">
        <h3>Quantization</h3>
        <ul>
            <li><strong>Idea</strong>: Represent weights/activations with <strong>fewer bits</strong>.</li>
            <li><strong>Types</strong>:
            <ul>
                <li><strong>Post-Training Quantization (PTQ)</strong>:<br>
                Convert trained FP32 model to INT8 <strong>without retraining</strong>.<br>
                → Fast, but may lose accuracy on sensitive models.</li>
                <li><strong>Quantization-Aware Training (QAT)</strong>:<br>
                Simulate quantization during training → recover accuracy.<br>
                → Slower, but higher fidelity.</li>
            </ul>
            </li>
            <li><strong>Precision Levels</strong>:
            <ul>
                <li><strong>INT8</strong>: Standard for edge (TensorRT, TensorFlow Lite)</li>
                <li><strong>INT4 / Binary</strong>: Extreme compression (research stage)</li>
                <li><strong>Mixed Precision</strong>: Keep critical layers in FP16/FP32</li>
            </ul>
            </li>
            <li><strong>Tools</strong>: TensorFlow Lite, PyTorch Quantization, NVIDIA TensorRT</li>
        </ul>
        
        <div class="definition">
            📉 <strong>Impact</strong>:<br>
            - ResNet-50: 95 MB (FP32) → <strong>23 MB (INT8)</strong><br>
            - Inference speed: <strong>2–4× faster</strong> on edge NPUs/GPUs
        </div>
        
        <h3>Pruning</h3>
        <ul>
            <li><strong>Idea</strong>: Remove <strong>unimportant weights or neurons</strong>.</li>
            <li><strong>Types</strong>:
            <ul>
                <li><strong>Unstructured Pruning</strong>: Remove individual weights → sparse matrices.<br>
                → Requires specialized hardware (e.g., NVIDIA Sparse Tensor Cores).</li>
                <li><strong>Structured Pruning</strong>: Remove entire <strong>channels, filters, or layers</strong>.<br>
                → Works on standard hardware; easier to deploy.</li>
            </ul>
            </li>
            <li><strong>Pruning Criteria</strong>:
            <ul>
                <li><strong>Magnitude-based</strong>: Remove smallest weights (L1 norm)</li>
                <li><strong>Gradient-based</strong>: Remove weights with low impact on loss</li>
                <li><strong>Iterative pruning</strong>: Prune → fine-tune → repeat</li>
            </ul>
            </li>
            <li><strong>Example</strong>:<br>
            VGG-16 pruned to 10% sparsity → <strong>90% fewer parameters</strong>, <0.5% accuracy drop.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Challenge</strong>:<br>
            Unstructured pruning rarely speeds up inference on CPUs <strong>structured pruning is preferred for edge</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Knowledge Distillation</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p><em>(Hinton et al., 2015)</em></p>
        <p>Train a small <strong>student model</strong> to mimic the behavior of a large <strong>teacher model</strong>.</p>
        
        <h3>How It Works</h3>
        <ul>
            <li><strong>Loss function</strong> combines:
            <ol>
                <li><strong>Hard targets</strong>: True labels (one-hot)</li>
                <li><strong>Soft targets</strong>: Teacher's output probabilities (smoothed by temperature \( T \))<br>
                \[ \mathcal{L} = \alpha \cdot \text{CE}(y, \hat{y}_{\text{student}}) + (1 - \alpha) \cdot \text{CE}(\text{softmax}(z_{\text{teacher}}/T), \text{softmax}(z_{\text{student}}/T)) \]</li>
            </ol>
            </li>
            <li><strong>Why soft targets help</strong>:<br>
            Teacher encodes <strong>dark knowledge</strong> (e.g., "cat is more like dog than car") → student learns richer representations.</li>
        </ul>
        
        <h3>Variants for Edge</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Edge Benefit</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>FitNets</strong></td>
                        <td>Student mimics teacher's intermediate features</td>
                    </tr>
                    <tr>
                        <td><strong>TinyBERT</strong></td>
                        <td>Distill BERT → 7.5× smaller, 9.4× faster</td>
                    </tr>
                    <tr>
                        <td><strong>Self-Distillation</strong></td>
                        <td>Model distills itself (no teacher needed)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            📱 <strong>Use Case</strong>:<br>
            - <strong>MobileNet</strong> distilled from ResNet → high accuracy in <5 MB<br>
            - <strong>DistilBERT</strong> → 40% smaller, 60% faster than BERT
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Resource Management Strategies</h2>
    <div class="content">
        <h3>Challenges at the Edge</h3>
        <ul>
            <li>Heterogeneous hardware (CPU, GPU, NPU, DSP)</li>
            <li>Dynamic workloads (e.g., bursty video analytics)</li>
            <li>Energy/battery constraints</li>
        </ul>
        
        <h3>Key Strategies</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Strategy</th>
                        <th>Description</th>
                        <th>Tools/Frameworks</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Workload Orchestration</strong></td>
                        <td>Place tasks on optimal node (device/edge/cloud)</td>
                        <td>Kubernetes (K3s), AWS Greengrass</td>
                    </tr>
                    <tr>
                        <td><strong>Dynamic Voltage Scaling (DVS)</strong></td>
                        <td>Lower CPU frequency when load is low</td>
                        <td>Linux CPUFreq</td>
                    </tr>
                    <tr>
                        <td><strong>Model Switching</strong></td>
                        <td>Swap between high/low-accuracy models based on context</td>
                        <td>Adaptive deep learning</td>
                    </tr>
                    <tr>
                        <td><strong>Caching & Prefetching</strong></td>
                        <td>Cache frequent inferences; prefetch data</td>
                        <td>Redis, custom edge caches</td>
                    </tr>
                    <tr>
                        <td><strong>Containerization</strong></td>
                        <td>Isolate apps; manage dependencies</td>
                        <td>Docker, containerd</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Adaptive Inference</h3>
        <ul>
            <li><strong>Early Exiting</strong>: Add exits in deep networks; shallow exit for easy samples.</li>
            <li><strong>Input Filtering</strong>: Skip inference if input is uninformative (e.g., empty camera frame).</li>
            <li><strong>Example</strong>: BranchyNet reduces average latency by 30–50%.</li>
        </ul>
        
        <div class="definition">
            🔋 <strong>Energy-Aware Scheduling</strong>:<br>
            Prioritize tasks during high battery; defer non-critical work when low.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Energy Efficiency Optimization</h2>
    <div class="content">
        <h3>Why Energy Matters</h3>
        <ul>
            <li>Edge devices are often <strong>battery-powered</strong> (sensors, drones, wearables).</li>
            <li><strong>Inference dominates energy use</strong> (up to 80% in AI workloads).</li>
        </ul>
        
        <h3>Optimization Levers</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Technique</th>
                        <th>Energy Savings</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Hardware</strong></td>
                        <td>Use NPUs/TPUs instead of CPU</td>
                        <td>5–10× more efficient</td>
                    </tr>
                    <tr>
                        <td><strong>Model</strong></td>
                        <td>Quantization, pruning, efficient architectures</td>
                        <td>2–5×</td>
                    </tr>
                    <tr>
                        <td><strong>Software</strong></td>
                        <td>Kernel fusion, memory access optimization</td>
                        <td>1.5–3×</td>
                    </tr>
                    <tr>
                        <td><strong>System</strong></td>
                        <td>Sleep modes, duty cycling</td>
                        <td>10–100× for idle time</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Methods</h3>
        <ul>
            <li><strong>Duty Cycling</strong>: Turn off sensors/compute when not needed (e.g., sample every 10 sec).</li>
            <li><strong>Approximate Computing</strong>: Trade accuracy for energy (e.g., lower resolution video).</li>
            <li><strong>Compiler Optimizations</strong>:
            <ul>
                <li><strong>TensorFlow Lite Micro</strong>: Optimized kernels for microcontrollers</li>
                <li><strong>Apache TVM</strong>: Auto-tune for target hardware</li>
            </ul>
            </li>
        </ul>
        
        <h3>Measurement & Profiling</h3>
        <ul>
            <li><strong>Tools</strong>:
            <ul>
                <li><code>perf</code> (Linux), Android Battery Historian</li>
                <li>Joulescope (hardware power meter)</li>
            </ul>
            </li>
            <li><strong>Metrics</strong>:
            <ul>
                <li><strong>Joules per inference</strong></li>
                <li><strong>mW during active/idle states</strong></li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            🌱 <strong>Sustainability Impact</strong>:<br>
            Optimizing 1 billion edge devices by 10% → saves <strong>terawatt-hours</strong> of energy annually.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Edge Optimization Techniques</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Primary Benefit</th>
                        <th>Accuracy Impact</th>
                        <th>Deployment Ease</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Quantization (INT8)</strong></td>
                        <td>2–4× smaller/faster</td>
                        <td>Very Low</td>
                        <td>⭐⭐⭐⭐⭐ (built into TF/PyTorch)</td>
                    </tr>
                    <tr>
                        <td><strong>Structured Pruning</strong></td>
                        <td>2–10× smaller</td>
                        <td>Low</td>
                        <td>⭐⭐⭐ (requires fine-tuning)</td>
                    </tr>
                    <tr>
                        <td><strong>Knowledge Distillation</strong></td>
                        <td>3–20× smaller models</td>
                        <td>Moderate</td>
                        <td>⭐⭐ (needs teacher model)</td>
                    </tr>
                    <tr>
                        <td><strong>Model Switching</strong></td>
                        <td>Adaptive latency/energy</td>
                        <td>None (uses multiple models)</td>
                        <td>⭐⭐ (complex orchestration)</td>
                    </tr>
                    <tr>
                        <td><strong>Duty Cycling</strong></td>
                        <td>Massive energy savings</td>
                        <td>None (reduces data frequency)</td>
                        <td>⭐⭐⭐⭐</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Quantization is the #1 go-to</strong> for edge deployment minimal accuracy loss, huge gains.</li>
            <li><strong>Pruning + distillation</strong> enable extreme compression for microcontrollers (TinyML).</li>
            <li><strong>Resource management</strong> must be <strong>adaptive</strong> static allocation fails in dynamic edge environments.</li>
            <li><strong>Energy efficiency</strong> requires co-design across <strong>hardware, model, and system</strong> layers.</li>
            <li><strong>Always profile</strong>: Measure latency, memory, and energy on <strong>target hardware</strong> simulators lie.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>:<br>
            Start with <strong>TensorFlow Lite</strong> or <strong>PyTorch Mobile</strong> they integrate quantization, pruning, and hardware acceleration out of the box.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Edge Deployment</h2>
    <div class="content">
        <p><em>Operationalizing AI and Applications at the Edge</em></p>
        
        <div class="definition">
            <strong>Core Challenge</strong>:<br>
            Deploy, secure, monitor, and scale applications across <strong>thousands of heterogeneous, remote, and often offline edge devices</strong> while ensuring reliability, security, and performance.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Containerization for Edge Devices</h2>
    <div class="content">
        <h3>Why Containers at the Edge?</h3>
        <ul>
            <li><strong>Isolation</strong>: Run multiple apps/services without interference.</li>
            <li><strong>Portability</strong>: Package app + dependencies → deploy across diverse hardware.</li>
            <li><strong>Efficiency</strong>: Lightweight vs. VMs (critical for memory-constrained devices).</li>
        </ul>
        
        <h3>Edge-Optimized Container Runtimes</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Runtime</th>
                        <th>Key Features</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Docker</strong></td>
                        <td>Full-featured; large footprint (~500 MB)</td>
                        <td>Gateways, edge servers</td>
                    </tr>
                    <tr>
                        <td><strong>containerd</strong></td>
                        <td>Lightweight core of Docker (~50 MB)</td>
                        <td>Industrial edge nodes</td>
                    </tr>
                    <tr>
                        <td><strong>CRI-O</strong></td>
                        <td>Kubernetes-native; minimal</td>
                        <td>K8s-based edge clusters</td>
                    </tr>
                    <tr>
                        <td><strong>Podman</strong></td>
                        <td>Daemonless; rootless mode</td>
                        <td>Security-sensitive devices</td>
                    </tr>
                    <tr>
                        <td><strong>K3s</strong></td>
                        <td>Lightweight Kubernetes (40 MB)</td>
                        <td>Orchestrating edge containers</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Best Practices</h3>
        <ul>
            <li><strong>Multi-stage builds</strong>: Reduce image size (e.g., compile in one stage, copy binaries to minimal base).</li>
            <li><strong>Use distroless images</strong>: No shell or package manager → smaller attack surface.</li>
            <li><strong>ARM support</strong>: Build multi-arch images (<code>docker buildx</code>) for Raspberry Pi, NVIDIA Jetson, etc.</li>
            <li><strong>Offline operation</strong>: Pre-pull images; handle air-gapped deployments.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Limitation</strong>:<br>
            Containers still require OS support <strong>not suitable for microcontrollers</strong> (use RTOS or bare-metal instead).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Edge-Specific Security Considerations</h2>
    <div class="content">
        <h3>Unique Threats at the Edge</h3>
        <ul>
            <li><strong>Physical access</strong>: Attackers can tamper with devices (e.g., extract keys).</li>
            <li><strong>Untrusted networks</strong>: Public Wi-Fi, cellular, or mesh networks.</li>
            <li><strong>Supply chain risks</strong>: Compromised hardware/firmware.</li>
            <li><strong>Long lifecycles</strong>: Devices deployed for 5–10 years → hard to patch.</li>
        </ul>
        
        <h3>Security Strategies</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Technique</th>
                        <th>Tools/Standards</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Hardware</strong></td>
                        <td>Secure boot, TPM/TEE, hardware root of trust</td>
                        <td>ARM TrustZone, Intel SGX</td>
                    </tr>
                    <tr>
                        <td><strong>OS/App</strong></td>
                        <td>Minimal OS, signed containers, sandboxing</td>
                        <td>SELinux, seccomp, gVisor</td>
                    </tr>
                    <tr>
                        <td><strong>Network</strong></td>
                        <td>Zero Trust, mutual TLS (mTLS), encrypted tunnels</td>
                        <td>WireGuard, SPIFFE/SPIRE</td>
                    </tr>
                    <tr>
                        <td><strong>Data</strong></td>
                        <td>Encrypt data at rest (AES-256) and in transit (TLS 1.3)</td>
                        <td>LUKS, OpenSSL</td>
                    </tr>
                    <tr>
                        <td><strong>Updates</strong></td>
                        <td>Signed, atomic OTA updates</td>
                        <td>Mender, RAUC, AWS IoT Jobs</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Zero Trust at the Edge</h3>
        <ul>
            <li><strong>Never trust, always verify</strong>:
            <ul>
                <li>Devices authenticate to cloud (certificates, not passwords)</li>
                <li>Services authenticate to each other (mTLS)</li>
                <li>Least privilege access (e.g., device can only publish to its topic)</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Critical Practice</strong>:<br>
            <strong>Hardware-backed key storage</strong> (e.g., TPM, HSM) prevents key extraction even if device is compromised.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Remote Management and Monitoring</h2>
    <div class="content">
        <h3>Challenges</h3>
        <ul>
            <li>Devices are <strong>geographically dispersed</strong> (e.g., farms, oil rigs).</li>
            <li><strong>Intermittent connectivity</strong>: May be offline for hours/days.</li>
            <li><strong>Heterogeneous fleet</strong>: Different OS, hardware, vendors.</li>
        </ul>
        
        <h3>Key Capabilities</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Function</th>
                        <th>Description</th>
                        <th>Tools</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Remote Access</strong></td>
                        <td>SSH/tunnel into devices for debugging</td>
                        <td>Tailscale, inlets, AWS SSM</td>
                    </tr>
                    <tr>
                        <td><strong>Configuration Management</strong></td>
                        <td>Push config changes (e.g., update model threshold)</td>
                        <td>Ansible, SaltStack, Fleet</td>
                    </tr>
                    <tr>
                        <td><strong>OTA Updates</strong></td>
                        <td>Deploy new apps/models securely</td>
                        <td>Mender, Balena, Azure IoT Hub</td>
                    </tr>
                    <tr>
                        <td><strong>Telemetry & Logging</strong></td>
                        <td>Collect metrics, logs, traces</td>
                        <td>Prometheus (edge exporter), Fluent Bit, Grafana</td>
                    </tr>
                    <tr>
                        <td><strong>Alerting</strong></td>
                        <td>Notify on failures (e.g., disk full, model drift)</td>
                        <td>Alertmanager, PagerDuty</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Edge-Native Monitoring</h3>
        <ul>
            <li><strong>Push vs. Pull</strong>:
            <ul>
                <li><strong>Push</strong>: Device sends data when online (best for intermittent connectivity)</li>
                <li><strong>Pull</strong>: Cloud polls device (fails if offline)</li>
            </ul>
            </li>
            <li><strong>Local buffering</strong>: Store logs/metrics locally → sync when online.</li>
            <li><strong>Edge analytics</strong>: Detect anomalies locally (e.g., sudden CPU spike) → reduce cloud load.</li>
        </ul>
        
        <div class="definition">
            📊 <strong>Golden Metrics for Edge</strong>:<br>
            - <strong>Uptime</strong><br>
            - <strong>Latency (inference, network)</strong><br>
            - <strong>Resource usage (CPU, memory, disk)</strong><br>
            - <strong>Model accuracy (via shadow mode)</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Edge AI Frameworks and Tools</h2>
    <div class="content">
        <h3>Purpose-Built Edge AI Stacks</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Framework</th>
                        <th>Key Features</th>
                        <th>Target Hardware</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>TensorFlow Lite</strong></td>
                        <td>Quantization, delegate APIs (GPU/NPU), micro support</td>
                        <td>Android, iOS, Raspberry Pi, microcontrollers</td>
                    </tr>
                    <tr>
                        <td><strong>PyTorch Mobile</strong></td>
                        <td>TorchScript, executor backend, ONNX export</td>
                        <td>iOS, Android</td>
                    </tr>
                    <tr>
                        <td><strong>ONNX Runtime</strong></td>
                        <td>Cross-platform, hardware accelerators (TensorRT, OpenVINO)</td>
                        <td>x86, ARM, GPUs, NPUs</td>
                    </tr>
                    <tr>
                        <td><strong>Apache TVM</strong></td>
                        <td>Auto-tuning for target hardware, compiler-based</td>
                        <td>Custom ASICs, FPGAs, edge GPUs</td>
                    </tr>
                    <tr>
                        <td><strong>NVIDIA TAO Toolkit</strong></td>
                        <td>Transfer learning, pruning, quantization for Jetson</td>
                        <td>NVIDIA Jetson, DRIVE</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>TinyML (Ultra-Low-Power AI)</h3>
        <ul>
            <li><strong>TensorFlow Lite Micro</strong>: Runs ML on <strong>microcontrollers</strong> (no OS, < 100 KB RAM).</li>
            <li><strong>Use Cases</strong>:
            <ul>
                <li>Wake-word detection (e.g., "Hey Google")</li>
                <li>Vibration anomaly detection in motors</li>
                <li>Gesture recognition on wearables</li>
            </ul>
            </li>
        </ul>
        
        <h3>Model Deployment Workflow</h3>
        <ol>
            <li>Train model in cloud</li>
            <li>Optimize (quantize, prune, distill)</li>
            <li>Convert to edge format (TFLite, ONNX)</li>
            <li>Deploy via OTA update</li>
            <li>Monitor accuracy/performance</li>
        </ol>
        
        <div class="definition">
            🧪 <strong>Testing Tip</strong>:<br>
            Use <strong>device emulation</strong> (e.g., Android Emulator, QEMU) for initial validation but always test on <strong>real hardware</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Scalability in Edge Environments</h2>
    <div class="content">
        <h3>Scalability Challenges</h3>
        <ul>
            <li><strong>Fleet size</strong>: 10,000+ devices across regions</li>
            <li><strong>Heterogeneity</strong>: Mix of cameras, sensors, gateways</li>
            <li><strong>Network constraints</strong>: Low bandwidth, high latency to cloud</li>
            <li><strong>Operational complexity</strong>: Manual management doesn't scale</li>
        </ul>
        
        <h3>Scalability Solutions</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Strategy</th>
                        <th>How It Works</th>
                        <th>Tools</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Hierarchical Architecture</strong></td>
                        <td>Group devices → local edge hub → cloud</td>
                        <td>AWS IoT Greengrass, Azure IoT Edge</td>
                    </tr>
                    <tr>
                        <td><strong>Declarative Configuration</strong></td>
                        <td>Define desired state → system reconciles</td>
                        <td>Kubernetes CRDs, Fleet</td>
                    </tr>
                    <tr>
                        <td><strong>Edge Clustering</strong></td>
                        <td>Run K3s on groups of devices for local HA</td>
                        <td>K3s, KubeEdge</td>
                    </tr>
                    <tr>
                        <td><strong>Bandwidth Optimization</strong></td>
                        <td>Compress telemetry; batch uploads</td>
                        <td>Protocol buffers, MQTT</td>
                    </tr>
                    <tr>
                        <td><strong>Autonomous Operation</strong></td>
                        <td>Devices self-heal (restart failed services)</td>
                        <td>systemd, Docker restart policies</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Kubernetes at the Edge</h3>
        <ul>
            <li><strong>K3s</strong>: Lightweight K8s for edge servers</li>
            <li><strong>KubeEdge</strong>: Extends K8s API to edge devices (even offline)</li>
            <li><strong>OpenYurt</strong>: Alibaba's edge K8s with node autonomy</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Scalability Rule</strong>:<br>
            <strong>"Manage groups, not individual devices."</strong> Use <strong>tags, labels, and policies</strong> to manage fleets.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Edge Deployment Stack</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Key Technology</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Containerization</strong></td>
                        <td>Docker, containerd, K3s</td>
                        <td>Package and isolate apps</td>
                    </tr>
                    <tr>
                        <td><strong>Security</strong></td>
                        <td>TPM, mTLS, signed OTA</td>
                        <td>Protect against physical/network attacks</td>
                    </tr>
                    <tr>
                        <td><strong>Management</strong></td>
                        <td>Mender, AWS IoT Jobs, Fleet</td>
                        <td>Remote updates and config</td>
                    </tr>
                    <tr>
                        <td><strong>Monitoring</strong></td>
                        <td>Prometheus edge, Fluent Bit</td>
                        <td>Observe health and performance</td>
                    </tr>
                    <tr>
                        <td><strong>AI Frameworks</strong></td>
                        <td>TensorFlow Lite, ONNX Runtime</td>
                        <td>Run optimized models</td>
                    </tr>
                    <tr>
                        <td><strong>Orchestration</strong></td>
                        <td>K3s, KubeEdge</td>
                        <td>Scale across 1000s of nodes</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Containers + Kubernetes (K3s)</strong> are the foundation of modern edge deployment.</li>
            <li><strong>Security must be hardware-rooted</strong> software-only defenses fail at the edge.</li>
            <li><strong>Remote management is non-negotiable</strong>: Assume devices are inaccessible physically.</li>
            <li><strong>Edge AI frameworks</strong> (TFLite, ONNX) abstract hardware complexity use them.</li>
            <li><strong>Scalability requires automation</strong>: Declarative configs, fleet management, and edge autonomy.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>:<br>
            Start small: Deploy to <strong>10 devices</strong>, validate your pipeline, then scale.<br>
            <strong>"If it's not automated, it doesn't scale."</strong>
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Security Data Analysis</h2>
    <div class="content">
        <p><em>Understanding and Leveraging Security Data for Threat Detection</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>Effective security analytics starts with knowing your data</strong> its structure, semantics, collection methods, and limitations.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Network Traffic Data</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Monitor communication between systems to detect anomalies, data exfiltration, C2 traffic, and reconnaissance.</p>
        
        <h3>Key Formats</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Data Type</th>
                        <th>Description</th>
                        <th>Key Fields</th>
                        <th>Use Cases</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>NetFlow / IPFIX</strong></td>
                        <td><strong>Metadata</strong> about network flows (not packet content)</td>
                        <td>Source/Dest IP, Port, Protocol, Bytes, Packets, Start/End Time</td>
                        <td>
                            <ul>
                                <li>Detect DDoS</li>
                                <li>Identify data exfiltration</li>
                                <li>Baseline normal traffic</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>PCAP (Packet Capture)</strong></td>
                        <td><strong>Full packet payloads</strong> (raw network data)</td>
                        <td>Ethernet/IP/TCP headers + payload</td>
                        <td>
                            <ul>
                                <li>Deep protocol analysis</li>
                                <li>Malware payload extraction</li>
                                <li>Forensic investigation</li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Comparison</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>NetFlow</th>
                        <th>PCAP</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Storage</strong></td>
                        <td>Low (KB per flow)</td>
                        <td>Very high (GB/TB per hour)</td>
                    </tr>
                    <tr>
                        <td><strong>Privacy</strong></td>
                        <td>No payload → GDPR-friendly</td>
                        <td>Contains sensitive data (emails, passwords)</td>
                    </tr>
                    <tr>
                        <td><strong>Analysis Depth</strong></td>
                        <td>Flow-level (who talked to whom)</td>
                        <td>Packet-level (what was said)</td>
                    </tr>
                    <tr>
                        <td><strong>Retention</strong></td>
                        <td>Weeks to months</td>
                        <td>Hours to days (due to size)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Tools</h3>
        <ul>
            <li><strong>NetFlow</strong>: nfdump, SiLK, Elastic Flow</li>
            <li><strong>PCAP</strong>: Wireshark, tcpdump, Zeek (Bro), Arkime</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Best Practice</strong>:<br>
            Use <strong>NetFlow for broad monitoring</strong> and <strong>PCAP for targeted deep dives</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">System Logs and Event Data</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Record activities on hosts (servers, workstations) for auditing, troubleshooting, and attack detection.</p>
        
        <h3>Common Log Sources</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Source</th>
                        <th>Key Events</th>
                        <th>Example Tools</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Windows Event Logs</strong></td>
                        <td>Logon attempts, process creation, registry changes</td>
                        <td>Windows Event Viewer, Winlogbeat</td>
                    </tr>
                    <tr>
                        <td><strong>Syslog (Linux/Unix)</strong></td>
                        <td>Authentication, kernel messages, service status</td>
                        <td>rsyslog, syslog-ng</td>
                    </tr>
                    <tr>
                        <td><strong>Application Logs</strong></td>
                        <td>Custom app events (e.g., failed logins, errors)</td>
                        <td>App-specific (e.g., Apache, Nginx)</td>
                    </tr>
                    <tr>
                        <td><strong>Auditd (Linux)</strong></td>
                        <td>File access, system calls, user commands</td>
                        <td>auditd, auditbeat</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Critical Log Types for Security</h3>
        <ul>
            <li><strong>Authentication logs</strong>: Detect brute-force, pass-the-hash</li>
            <li><strong>Process creation logs</strong>: Spot malware execution (e.g., <code>powershell.exe -enc ...</code>)</li>
            <li><strong>File integrity monitoring (FIM)</strong>: Alert on critical file changes (e.g., <code>/etc/passwd</code>)</li>
        </ul>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Volume</strong>: A single server can generate 10K+ events/minute</li>
            <li><strong>Noise</strong>: 99% of logs are benign</li>
            <li><strong>Normalization</strong>: Different formats (CEF, LEEF, JSON, plain text)</li>
        </ul>
        
        <div class="definition">
            🛠️ <strong>Solution</strong>:<br>
            Use <strong>log shippers</strong> (Filebeat, Fluentd) + <strong>SIEM</strong> for parsing, enrichment, and correlation.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Endpoint Detection and Response (EDR) Data</h2>
    <div class="content">
        <h3>What is EDR?</h3>
        <p>Advanced endpoint security that <strong>records granular process-level activity</strong> and enables <strong>threat hunting and response</strong>.</p>
        
        <h3>Key Data Collected</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Data Type</th>
                        <th>Description</th>
                        <th>Security Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Process Tree</strong></td>
                        <td>Parent-child relationships of processes</td>
                        <td>Detect process injection, living-off-the-land</td>
                    </tr>
                    <tr>
                        <td><strong>File Events</strong></td>
                        <td>File creation, modification, deletion</td>
                        <td>Track malware droppers, ransomware</td>
                    </tr>
                    <tr>
                        <td><strong>Network Connections</strong></td>
                        <td>Process → IP/port connections</td>
                        <td>Identify C2 beacons</td>
                    </tr>
                    <tr>
                        <td><strong>Registry Modifications</strong></td>
                        <td>Changes to Windows registry</td>
                        <td>Spot persistence mechanisms</td>
                    </tr>
                    <tr>
                        <td><strong>Memory Dumps</strong></td>
                        <td>Snapshots of process memory</td>
                        <td>Extract shellcode, credentials</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>EDR vs. Traditional AV</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Antivirus (AV)</th>
                        <th>EDR</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Detection</strong></td>
                        <td>Signature-based</td>
                        <td>Behavioral + historical</td>
                    </tr>
                    <tr>
                        <td><strong>Response</strong></td>
                        <td>Quarantine file</td>
                        <td>Kill process, isolate host, rollback</td>
                    </tr>
                    <tr>
                        <td><strong>Visibility</strong></td>
                        <td>Limited</td>
                        <td>Full process lineage + telemetry</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Leading EDR Platforms</h3>
        <ul>
            <li>CrowdStrike Falcon</li>
            <li>Microsoft Defender for Endpoint</li>
            <li>SentinelOne</li>
            <li>Elastic Endpoint Security</li>
        </ul>
        
        <div class="definition">
            🕵️ <strong>Threat Hunting Use Case</strong>:<br>
            Search for <code>powershell.exe</code> spawning <code>cmd.exe</code> with encoded commands → likely malicious.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Threat Intelligence Feeds</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Enrich internal data with <strong>external knowledge</strong> about known threats (IOCs, TTPs, actors).</p>
        
        <h3>Types of Feeds</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feed Type</th>
                        <th>Content</th>
                        <th>Format</th>
                        <th>Example Sources</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Indicator-Based (IOC)</strong></td>
                        <td>Malicious IPs, domains, hashes</td>
                        <td>STIX/TAXII, CSV, JSON</td>
                        <td>AlienVault OTX, AbuseIPDB, VirusTotal</td>
                    </tr>
                    <tr>
                        <td><strong>Tactical (TTPs)</strong></td>
                        <td>MITRE ATT&CK techniques used by threat groups</td>
                        <td>STIX, MISP</td>
                        <td>MITRE ATT&CK, Mandiant, CrowdStrike</td>
                    </tr>
                    <tr>
                        <td><strong>Strategic</strong></td>
                        <td>Reports on threat actor motives, campaigns</td>
                        <td>PDF, HTML</td>
                        <td>FireEye, Symantec, CISA Alerts</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Integration into Security Stack</h3>
        <ul>
            <li><strong>SIEM</strong>: Auto-block IPs/domains from IOC feeds</li>
            <li><strong>Firewall/Proxy</strong>: Sinkhole malicious domains</li>
            <li><strong>EDR</strong>: Alert on file hashes matching malware feeds</li>
        </ul>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>False positives</strong>: Legitimate sites may appear in feeds temporarily</li>
            <li><strong>Timeliness</strong>: IOCs expire quickly (average lifespan: hours to days)</li>
            <li><strong>Volume</strong>: Millions of IOCs → need filtering (e.g., only high-confidence)</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>:<br>
            <strong>Enrich, don't just block</strong> use intelligence to <strong>contextualize alerts</strong> (e.g., "This IP is in a feed AND connected to HR server").
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">User Behavior Analytics (UBA) / User and Entity Behavior Analytics (UEBA) Data</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Detect <strong>anomalous user activity</strong> that may indicate compromise (e.g., insider threat, account takeover).</p>
        
        <h3>Data Sources</h3>
        <ul>
            <li>Authentication logs (logon time, location, device)</li>
            <li>Email activity (volume, recipients)</li>
            <li>File access (sensitive documents, unusual downloads)</li>
            <li>Application usage (ERP, CRM systems)</li>
        </ul>
        
        <h3>Key Behavioral Metrics</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Normal Behavior</th>
                        <th>Anomalous Behavior</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Logon Location</strong></td>
                        <td>Office IP range</td>
                        <td>Foreign country at 3 AM</td>
                    </tr>
                    <tr>
                        <td><strong>Data Access Volume</strong></td>
                        <td>10 files/day</td>
                        <td>10,000 files in 1 hour</td>
                    </tr>
                    <tr>
                        <td><strong>Peer Group Deviation</strong></td>
                        <td>Similar to team</td>
                        <td>Accessing systems peers don't use</td>
                    </tr>
                    <tr>
                        <td><strong>Sequence Anomaly</strong></td>
                        <td>Login → Email → CRM</td>
                        <td>Login → HR DB → External USB</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Analytics Techniques</h3>
        <ul>
            <li><strong>Statistical baselining</strong>: Z-scores, moving averages</li>
            <li><strong>Machine learning</strong>: Clustering (k-means), outlier detection (Isolation Forest)</li>
            <li><strong>Graph analysis</strong>: User → resource interaction graphs</li>
        </ul>
        
        <h3>UEBA Platforms</h3>
        <ul>
            <li>Exabeam</li>
            <li>Splunk UBA</li>
            <li>Microsoft Sentinel (with Azure AD Identity Protection)</li>
            <li>Elastic Security (via ML jobs)</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Privacy Note</strong>:<br>
            UBA must comply with <strong>GDPR, CCPA</strong> anonymize data, get user consent, limit retention.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Security Data Types Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Data Type</th>
                        <th>Granularity</th>
                        <th>Retention</th>
                        <th>Primary Use</th>
                        <th>Key Tools</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>NetFlow</strong></td>
                        <td>Flow-level</td>
                        <td>Weeks–months</td>
                        <td>Network anomaly detection</td>
                        <td>nfdump, SiLK</td>
                    </tr>
                    <tr>
                        <td><strong>PCAP</strong></td>
                        <td>Packet-level</td>
                        <td>Hours–days</td>
                        <td>Forensic deep dive</td>
                        <td>Wireshark, Zeek</td>
                    </tr>
                    <tr>
                        <td><strong>System Logs</strong></td>
                        <td>Event-level</td>
                        <td>Days–months</td>
                        <td>Host-based detection</td>
                        <td>Winlogbeat, auditd</td>
                    </tr>
                    <tr>
                        <td><strong>EDR Data</strong></td>
                        <td>Process-level</td>
                        <td>Months–years</td>
                        <td>Threat hunting, response</td>
                        <td>CrowdStrike, Defender</td>
                    </tr>
                    <tr>
                        <td><strong>Threat Intel</strong></td>
                        <td>Indicator/TTP</td>
                        <td>Real-time</td>
                        <td>Alert enrichment</td>
                        <td>MISP, OTX</td>
                    </tr>
                    <tr>
                        <td><strong>UBA Data</strong></td>
                        <td>User-level</td>
                        <td>Months</td>
                        <td>Insider threat, account takeover</td>
                        <td>Exabeam, Sentinel</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>No single data source is enough</strong>: Correlate <strong>network + endpoint + logs + intel</strong> for full visibility.</li>
            <li><strong>EDR and UBA provide high-fidelity signals</strong> prioritize their integration.</li>
            <li><strong>Threat intelligence is most valuable when contextualized</strong> not used in isolation.</li>
            <li><strong>PCAP is powerful but expensive</strong> use selectively for high-priority investigations.</li>
            <li><strong>Data quality > quantity</strong>: Normalize, enrich, and filter data before analysis.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Golden Rule of Security Data</strong>:<br>
            <strong>"Collect with purpose. Analyze with context. Respond with speed."</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Analysis Techniques for Security Data</h2>
    <div class="content">
        <p><em>From Raw Logs to Actionable Threat Intelligence</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>Effective detection = Right data + Right technique + Right context</strong><br>
            No single method works universally combine statistical, temporal, and behavioral approaches.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Statistical Analysis of Security Data</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Quantify normal vs. abnormal behavior using probability and distribution theory.</p>
        
        <h3>Key Techniques</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Use Case</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Descriptive Statistics</strong></td>
                        <td>Summarize data (mean, median, std dev)</td>
                        <td>Avg. failed logins/hour = 2; spike to 50 → alert</td>
                    </tr>
                    <tr>
                        <td><strong>Z-Score / Standard Score</strong></td>
                        <td>Measure deviation from mean in std devs</td>
                        <td>\( z = \frac{x - \mu}{\sigma} \); \( |z| > 3 \) → outlier</td>
                    </tr>
                    <tr>
                        <td><strong>Chi-Square Test</strong></td>
                        <td>Detect categorical anomalies (e.g., protocol distribution)</td>
                        <td>HTTP traffic suddenly drops from 80% → 20%</td>
                    </tr>
                    <tr>
                        <td><strong>Entropy Analysis</strong></td>
                        <td>Measure randomness (e.g., in domain names, passwords)</td>
                        <td>Low entropy: `aaaa.com`; High entropy: `xk9q2m.com` → likely DGA</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Entropy in Security</h3>
        <ul>
            <li><strong>Domain Generation Algorithms (DGAs)</strong>: Malware generates random domains for C2.</li>
            <li><strong>Entropy formula</strong> (Shannon entropy for string \( S \)):<br>
            \[ H(S) = -\sum_{c \in \text{alphabet}} p(c) \log_2 p(c) \]
            <ul>
                <li>Legitimate domains: \( H \approx 3–4 \)</li>
                <li>DGA domains: \( H > 4.5 \)</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            📊 <strong>Best Practice</strong>:<br>
            Use <strong>rolling windows</strong> (e.g., 1-hour) to compute stats avoid static baselines.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Time Series Analysis for Security Events</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Model and forecast security events over time to detect trends, seasonality, and anomalies.</p>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>Stationarity</strong>: Mean/variance constant over time (required for many models).</li>
            <li><strong>Seasonality</strong>: Regular patterns (e.g., daily login peaks at 9 AM).</li>
            <li><strong>Autocorrelation</strong>: Correlation of signal with lagged version of itself.</li>
        </ul>
        
        <h3>Techniques & Models</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>How It Works</th>
                        <th>Security Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Moving Average (MA)</strong></td>
                        <td>Smooth data using windowed average</td>
                        <td>Filter noise in firewall deny counts</td>
                    </tr>
                    <tr>
                        <td><strong>Exponential Smoothing</strong></td>
                        <td>Weight recent observations more heavily</td>
                        <td>Detect sudden spikes in SSH attempts</td>
                    </tr>
                    <tr>
                        <td><strong>ARIMA (AutoRegressive Integrated Moving Average)</strong></td>
                        <td>Model trends + seasonality</td>
                        <td>Forecast normal traffic; flag deviations</td>
                    </tr>
                    <tr>
                        <td><strong>Fourier Transform</strong></td>
                        <td>Decompose signal into frequency components</td>
                        <td>Detect periodic C2 beacons (e.g., every 10 min)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Practical Workflow</h3>
        <ol>
            <li><strong>Ingest</strong> time-stamped events (e.g., <code>timestamp, event_count</code>)</li>
            <li><strong>Resample</strong> to fixed intervals (e.g., 5-min bins)</li>
            <li><strong>Decompose</strong> into trend, seasonal, residual components</li>
            <li><strong>Model</strong> (e.g., ARIMA) on historical data</li>
            <li><strong>Alert</strong> when observed > predicted + threshold</li>
        </ol>
        
        <div class="definition">
            ⚠️ <strong>Challenge</strong>:<br>
            Security events are often <strong>sparse and bursty</strong> traditional time series models may underperform.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Correlation Analysis Techniques</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Link related events across sources to reduce alert fatigue and reveal multi-stage attacks.</p>
        
        <h3>Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Rule-Based Correlation</strong></td>
                        <td>Predefined logic (e.g., "If A and B, then alert")</td>
                        <td><code>Failed login → Firewall block → Successful login = brute-force</code></td>
                    </tr>
                    <tr>
                        <td><strong>Statistical Correlation</strong></td>
                        <td>Pearson/Spearman correlation between event streams</td>
                        <td>High correlation between <code>port_scans</code> and <code>exploit_attempts</code></td>
                    </tr>
                    <tr>
                        <td><strong>Graph-Based Correlation</strong></td>
                        <td>Model entities (users, hosts) as nodes; events as edges</td>
                        <td>Detect lateral movement: <code>User A → Host1 → Host2 → Host3</code></td>
                    </tr>
                    <tr>
                        <td><strong>Temporal Correlation</strong></td>
                        <td>Events within time window (e.g., 5 min)</td>
                        <td><code>Phishing email opened → Malware beacon within 10 min</code></td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>SIEM Correlation Rules (Example)</h3>
        <pre>IF
  (EventID = 4625 AND Count > 10 in 5 min)  // Failed logins
  AND
  (EventID = 5156 AND DestPort = 445)       // SMB connection
THEN
  Alert: "Possible Pass-the-Hash Attack"</pre>
        
        <div class="definition">
            🔗 <strong>Advanced</strong>:<br>
            Use <strong>MITRE ATT&CK</strong> to correlate events into <strong>attack chains</strong> (e.g., Initial Access → Execution → Lateral Movement).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Pattern Recognition in Security Data</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Identify known malicious patterns (signatures) or learn new ones (behavioral).</p>
        
        <h3>Techniques</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Mechanism</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Signature Matching</strong></td>
                        <td>Exact match against known patterns</td>
                        <td>Snort rule: <code>alert tcp any any -> any 80 (content:"/etc/passwd";)</code></td>
                    </tr>
                    <tr>
                        <td><strong>Regular Expressions</strong></td>
                        <td>Pattern matching in logs</td>
                        <td>Detect encoded PowerShell: <code>.*[Cc][Oo][Nn][Vv][Ee][Rr][Tt].*</code></td>
                    </tr>
                    <tr>
                        <td><strong>Sequence Mining</strong></td>
                        <td>Find frequent event sequences</td>
                        <td><code>Download PDF → Enable macros → PowerShell spawn</code></td>
                    </tr>
                    <tr>
                        <td><strong>Clustering (k-means, DBSCAN)</strong></td>
                        <td>Group similar events</td>
                        <td>Cluster malware samples by API calls</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Deep Learning for Pattern Recognition</h3>
        <ul>
            <li><strong>CNNs</strong>: Analyze byte sequences of malware (MalConv)</li>
            <li><strong>RNNs/LSTMs</strong>: Model system call sequences for intrusion detection</li>
            <li><strong>Transformers</strong>: Log parsing and anomaly detection (e.g., LogBERT)</li>
        </ul>
        
        <div class="definition">
            🕵️ <strong>Threat Hunting Tip</strong>:<br>
            Search for <strong>TTPs (Tactics, Techniques, Procedures)</strong> instead of IOCs patterns persist even when malware changes.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Anomaly Detection Methodologies</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Identify rare events that deviate significantly from normal behavior.</p>
        
        <h3>Categories of Anomaly Detection</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Description</th>
                        <th>Algorithms</th>
                        <th>Pros / Cons</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Supervised</strong></td>
                        <td>Train on labeled normal/anomalous data</td>
                        <td>SVM, Random Forest</td>
                        <td>✅ High accuracy<br>❌ Needs labeled anomalies (rare!)</td>
                    </tr>
                    <tr>
                        <td><strong>Unsupervised</strong></td>
                        <td>Learn normal behavior; flag deviations</td>
                        <td>Isolation Forest, Autoencoders, One-Class SVM</td>
                        <td>✅ No labels needed<br>❌ High false positives</td>
                    </tr>
                    <tr>
                        <td><strong>Semi-Supervised</strong></td>
                        <td>Train only on normal data</td>
                        <td>Autoencoders, GANs</td>
                        <td>✅ Realistic for security<br>❌ Sensitive to concept drift</td>
                    </tr>
                    <tr>
                        <td><strong>Reconstruction-Based</strong></td>
                        <td>Measure reconstruction error</td>
                        <td>Autoencoders, PCA</td>
                        <td>✅ Works on high-dim data<br>❌ May miss subtle anomalies</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Algorithms in Practice</h3>
        <ul>
            <li><strong>Isolation Forest</strong>:
            <ul>
                <li>Isolates anomalies by random feature splits</li>
                <li>Fast, works well on tabular data (e.g., NetFlow)</li>
            </ul>
            </li>
            <li><strong>Autoencoders</strong>:
            <ul>
                <li>High reconstruction error = anomaly</li>
                <li>Great for EDR process trees, logs</li>
            </ul>
            </li>
            <li><strong>Local Outlier Factor (LOF)</strong>:
            <ul>
                <li>Density-based; good for multi-modal data</li>
                <li>Detects anomalies in user behavior</li>
            </ul>
            </li>
        </ul>
        
        <h3>Evaluation Metrics</h3>
        <ul>
            <li><strong>Precision/Recall</strong>: Critical due to class imbalance</li>
            <li><strong>ROC-AUC</strong>: When anomaly prevalence is known</li>
            <li><strong>Mean Time to Detect (MTTD)</strong>: Operational metric</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Real-World Challenge</strong>:<br>
            <strong>Concept drift</strong> normal behavior changes over time (e.g., new app deployment).<br>
            <strong>Solution</strong>: Retrain models weekly; use online learning.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Analysis Techniques Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Best For</th>
                        <th>Data Type</th>
                        <th>Complexity</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Statistical (Z-score, Entropy)</strong></td>
                        <td>Quick outlier detection</td>
                        <td>Numeric, categorical</td>
                        <td>Low</td>
                    </tr>
                    <tr>
                        <td><strong>Time Series (ARIMA, Fourier)</strong></td>
                        <td>Temporal patterns, forecasting</td>
                        <td>Timestamped counts</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td><strong>Correlation (Rules, Graphs)</strong></td>
                        <td>Multi-event attack chains</td>
                        <td>Multi-source logs</td>
                        <td>Medium-High</td>
                    </tr>
                    <tr>
                        <td><strong>Pattern Recognition (Regex, DL)</strong></td>
                        <td>Known TTPs, malware</td>
                        <td>Logs, binaries</td>
                        <td>High</td>
                    </tr>
                    <tr>
                        <td><strong>Anomaly Detection (Isolation Forest, AE)</strong></td>
                        <td>Unknown threats, zero-days</td>
                        <td>Any</td>
                        <td>Medium-High</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Start simple</strong>: Z-scores and entropy catch 80% of basic threats.</li>
            <li><strong>Time is critical</strong>: Use time series to model <strong>when</strong> attacks happen.</li>
            <li><strong>Correlate across sources</strong>: A single alert is noise; a chain is a threat.</li>
            <li><strong>Patterns > IOCs</strong>: Focus on <strong>behavioral patterns</strong> (TTPs) for resilience.</li>
            <li><strong>Anomaly detection is powerful but noisy</strong>: Always <strong>triage</strong> with human analysts.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>:<br>
            <strong>Combine techniques</strong>:<br>
            Use <strong>statistical baselining</strong> → <strong>anomaly detection</strong> → <strong>correlation with threat intel</strong> → <strong>pattern recognition for validation</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Feature Engineering for Security Data</h2>
    <div class="content">
        <p><em>Transforming Raw Logs into Actionable Signals</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>"Garbage in, garbage out."</strong><br>
            The quality of your detection model depends <strong>more on features than algorithms</strong> especially in security, where data is noisy, sparse, and high-dimensional.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Feature Extraction from Raw Security Data</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Convert unstructured or semi-structured security data (logs, packets, events) into <strong>structured, numerical features</strong> suitable for ML models.</p>
        
        <h3>By Data Source</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Data Type</th>
                        <th>Raw Example</th>
                        <th>Extracted Features</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Network (NetFlow)</strong></td>
                        <td><code>src=192.168.1.10, dst=10.0.0.5, bytes=1500, proto=TCP</code></td>
                        <td>
                            <ul>
                                <li>Bytes per flow</li>
                                <li>Flows per minute</li>
                                <li>Unique destination IPs</li>
                                <li>Protocol distribution entropy</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>PCAP</strong></td>
                        <td>Raw Ethernet frame with HTTP payload</td>
                        <td>
                            <ul>
                                <li>Packet size distribution</li>
                                <li>TLS cipher suites</li>
                                <li>HTTP user-agent entropy</li>
                                <li>DNS query name length</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Windows Event Logs</strong></td>
                        <td><code>EventID=4688, ProcessName=powershell.exe, CommandLine=-enc ...</code></td>
                        <td>
                            <ul>
                                <li>Parent-child process depth</li>
                                <li>Command-line entropy</li>
                                <li>Rare process combinations</li>
                                <li>Time since last logon</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>EDR Data</strong></td>
                        <td><code>Process: cmd.exe spawned by winword.exe</code></td>
                        <td>
                            <ul>
                                <li>Process ancestry hash</li>
                                <li>File write count</li>
                                <li>Network connections per process</li>
                                <li>Registry key access frequency</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Authentication Logs</strong></td>
                        <td><code>User=alice, IP=203.0.113.5, Time=02:15 AM</code></td>
                        <td>
                            <ul>
                                <li>Login hour (cyclic encoding)</li>
                                <li>Geolocation distance from last login</li>
                                <li>Peer group deviation (vs. team)</li>
                                <li>Failed login streak</li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Extraction Strategies</h3>
        <ul>
            <li><strong>Count-based</strong>: Frequency of events (e.g., SSH attempts/hour)</li>
            <li><strong>Time-based</strong>: Inter-arrival times, session duration</li>
            <li><strong>Graph-based</strong>: Degree, centrality in user-host interaction graphs</li>
            <li><strong>Text-based</strong>: N-grams, entropy, regex matches in command lines</li>
        </ul>
        
        <div class="definition">
            🛠️ <strong>Tool Tip</strong>:<br>
            Use <strong>Zeek (Bro)</strong> to auto-extract 100+ network features from PCAP → structured TSV/JSON.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Feature Selection Techniques</h2>
    <div class="content">
        <h3>Why Select Features?</h3>
        <ul>
            <li>Reduce dimensionality → faster training, less overfitting</li>
            <li>Remove irrelevant/noisy features → improve model interpretability</li>
            <li>Focus on high-signal indicators → better detection accuracy</li>
        </ul>
        
        <h3>Methods</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Category</th>
                        <th>Technique</th>
                        <th>How It Works</th>
                        <th>Security Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Filter Methods</strong></td>
                        <td>Correlation, Mutual Information</td>
                        <td>Rank features by statistical relationship with label</td>
                        <td>Remove features with |corr| < 0.1</td>
                    </tr>
                    <tr>
                        <td><strong>Wrapper Methods</strong></td>
                        <td>Recursive Feature Elimination (RFE)</td>
                        <td>Train model iteratively; remove least important feature</td>
                        <td>Select top 20 features for malware classifier</td>
                    </tr>
                    <tr>
                        <td><strong>Embedded Methods</strong></td>
                        <td>L1 Regularization (Lasso), Tree-based importance</td>
                        <td>Feature selection built into model training</td>
                        <td>XGBoost feature importance for phishing detection</td>
                    </tr>
                    <tr>
                        <td><strong>Unsupervised</strong></td>
                        <td>PCA, Variance Threshold</td>
                        <td>Select features with high variance or low redundancy</td>
                        <td>Remove constant features (e.g., all "proto=TCP")</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Security-Specific Considerations</h3>
        <ul>
            <li><strong>Concept drift</strong>: Re-select features periodically (e.g., monthly)</li>
            <li><strong>Adversarial robustness</strong>: Avoid features easily spoofed (e.g., User-Agent)</li>
            <li><strong>Interpretability</strong>: Prefer features with clear security meaning (e.g., "entropy" over PCA component #7)</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>:<br>
            Start with <strong>tree-based importance (XGBoost/Random Forest)</strong> → fast, handles non-linearity, robust to outliers.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Feature Transformation Methods</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Rescale, encode, or combine features to improve model performance and stability.</p>
        
        <h3>Common Transformations</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Purpose</th>
                        <th>Security Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Normalization</strong></td>
                        <td>Scale to [0,1] or unit variance</td>
                        <td>Scale <code>bytes_transferred</code> to prevent dominance over binary features</td>
                    </tr>
                    <tr>
                        <td><strong>Log Transform</strong></td>
                        <td>Handle skewed distributions</td>
                        <td><code>log(bytes + 1)</code> for file sizes (long-tailed)</td>
                    </tr>
                    <tr>
                        <td><strong>Cyclic Encoding</strong></td>
                        <td>Preserve cyclical nature</td>
                        <td>Encode <code>hour_of_day</code> as <code>(sin(2πh/24), cos(2πh/24))</code></td>
                    </tr>
                    <tr>
                        <td><strong>One-Hot Encoding</strong></td>
                        <td>Convert categorical to binary</td>
                        <td><code>protocol → [is_TCP, is_UDP, is_ICMP]</code></td>
                    </tr>
                    <tr>
                        <td><strong>Target Encoding</strong></td>
                        <td>Replace category with mean target</td>
                        <td><code>domain_reputation = avg(malicious_rate for domain)</code></td>
                    </tr>
                    <tr>
                        <td><strong>Binning/Discretization</strong></td>
                        <td>Convert continuous to categorical</td>
                        <td><code>login_hour → [night, morning, afternoon, evening]</code></td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Advanced: Interaction Features</h3>
        <p>Combine features to capture complex behaviors:</p>
        <ul>
            <li><code>is_weekend * login_hour</code> → detect off-hours activity</li>
            <li><code>process_entropy * network_connections</code> → spot encoded C2</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Caution</strong>:<br>
            Avoid <strong>data leakage</strong>: Never use future information (e.g., "total logins today" at 9 AM).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Domain-Specific Feature Engineering</h2>
    <div class="content">
        <h3>Network Security</h3>
        <ul>
            <li><strong>Flow-based</strong>:
            <ul>
                <li><strong>Flow duration</strong>, <strong>bytes/packet ratio</strong>, <strong>bidirectional flow symmetry</strong></li>
            </ul>
            </li>
            <li><strong>DNS</strong>:
            <ul>
                <li><strong>Domain name length</strong>, <strong>vowel/consonant ratio</strong>, <strong>TTL variance</strong></li>
            </ul>
            </li>
            <li><strong>HTTP</strong>:
            <ul>
                <li><strong>URL path depth</strong>, <strong>query parameter count</strong>, <strong>user-agent entropy</strong></li>
            </ul>
            </li>
        </ul>
        
        <h3>Endpoint Security</h3>
        <ul>
            <li><strong>Process lineage</strong>:
            <ul>
                <li><strong>Ancestry hash</strong> (hash of parent chain), <strong>rare child processes</strong></li>
            </ul>
            </li>
            <li><strong>File activity</strong>:
            <ul>
                <li><strong>File entropy</strong> (detect packed malware), <strong>write-to-read ratio</strong></li>
            </ul>
            </li>
            <li><strong>Registry</strong>:
            <ul>
                <li><strong>Persistence key modifications</strong>, <strong>run-key changes</strong></li>
            </ul>
            </li>
        </ul>
        
        <h3>User Behavior Analytics (UBA)</h3>
        <ul>
            <li><strong>Peer group analysis</strong>:
            <ul>
                <li><strong>Z-score vs. team average</strong> for data access</li>
            </ul>
            </li>
            <li><strong>Sequence features</strong>:
            <ul>
                <li><strong>Markov transition probabilities</strong> between systems</li>
            </ul>
            </li>
            <li><strong>Risk scoring</strong>:
            <ul>
                <li>Combine location, device, time into <strong>session risk score</strong></li>
            </ul>
            </li>
        </ul>
        
        <h3>Malware Analysis</h3>
        <ul>
            <li><strong>Static features</strong>:
            <ul>
                <li><strong>PE header entropy</strong>, <strong>imported DLL count</strong>, <strong>section permissions</strong></li>
            </ul>
            </li>
            <li><strong>Dynamic features</strong>:
            <ul>
                <li><strong>API call sequences</strong>, <strong>mutex names</strong>, <strong>network callback domains</strong></li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Pro Insight</strong>:<br>
            <strong>"The best features encode attacker economics."</strong><br>
            Example: High entropy + rare process + external IP = high cost for attacker to spoof → high-fidelity signal.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Automated Feature Engineering</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Automate the creation of features to reduce manual effort and discover non-obvious signals.</p>
        
        <h3>Tools & Frameworks</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Tool</th>
                        <th>Approach</th>
                        <th>Security Application</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Featuretools</strong></td>
                        <td>Deep Feature Synthesis (DFS): auto-generates features from relational data</td>
                        <td>Build features from <code>users → logins → files</code> tables</td>
                    </tr>
                    <tr>
                        <td><strong>TSFEL</strong></td>
                        <td>Time-series feature extraction library</td>
                        <td>Extract 60+ features from NetFlow time series</td>
                    </tr>
                    <tr>
                        <td><strong>tsfresh</strong></td>
                        <td>Automated time-series feature extraction</td>
                        <td>Detect periodic C2 beacons from connection logs</td>
                    </tr>
                    <tr>
                        <td><strong>AutoFeat</strong></td>
                        <td>Symbolic regression for feature construction</td>
                        <td>Discover non-linear combos like <code>log(bytes) * entropy</code></td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>How Featuretools Works (Example)</h3>
        <p>Given tables:</p>
        <ul>
            <li><code>events</code>: event_id, user_id, timestamp, event_type</li>
            <li><code>users</code>: user_id, department</li>
        </ul>
        <p>DFS generates:</p>
        <ul>
            <li><code>users.num_events</code></li>
            <li><code>users.most_common_event_type</code></li>
            <li><code>users.time_since_last_event</code></li>
        </ul>
        
        <h3>Limitations in Security</h3>
        <ul>
            <li><strong>Domain knowledge is irreplaceable</strong>: Auto-generated features may lack interpretability.</li>
            <li><strong>Adversarial environments</strong>: Attackers adapt static auto-features may become obsolete.</li>
            <li><strong>Computational cost</strong>: Generating 10,000 features → need strong selection.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Hybrid Approach</strong>:<br>
            Use <strong>automated tools for candidate generation</strong> → <strong>security expert for filtering and validation</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Feature Engineering Workflow</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Step</th>
                        <th>Key Question</th>
                        <th>Security-Specific Tip</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Extraction</strong></td>
                        <td>What raw signals matter?</td>
                        <td>Focus on <strong>attacker constraints</strong> (e.g., entropy, rarity)</td>
                    </tr>
                    <tr>
                        <td><strong>Selection</strong></td>
                        <td>Which features drive detection?</td>
                        <td>Prioritize <strong>interpretable, hard-to-spoof</strong> features</td>
                    </tr>
                    <tr>
                        <td><strong>Transformation</strong></td>
                        <td>How to represent features best?</td>
                        <td>Use <strong>cyclic encoding for time</strong>, <strong>log for skewed data</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Domain Engineering</strong></td>
                        <td>What's unique to my threat model?</td>
                        <td>Build <strong>TTP-aligned features</strong> (e.g., "living-off-the-land" indicators)</td>
                    </tr>
                    <tr>
                        <td><strong>Automation</strong></td>
                        <td>Can I scale feature creation?</td>
                        <td>Use <strong>Featuretools + expert review</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Start with domain knowledge</strong>: The best features come from understanding attacker TTPs.</li>
            <li><strong>Entropy, rarity, and sequence</strong> are golden features in security.</li>
            <li><strong>Avoid data leakage at all costs</strong> security data is time-sensitive.</li>
            <li><strong>Automate extraction, not judgment</strong>: Tools generate candidates; humans validate.</li>
            <li><strong>Monitor feature drift</strong>: Retrain and re-engineer as threats evolve.</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Final Thought</strong>:<br>
            <strong>"In security ML, features are your first line of defense and your last hope when models fail."</strong>
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Research Methodology</h2>
    <div class="content">
        <p><em>Designing Rigorous and Reproducible Security & AI Experiments</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>"An experiment without proper design is just data collection."</strong><br>
            Rigorous methodology ensures your conclusions are <strong>valid, generalizable, and trustworthy</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Hypothesis Formulation and Testing</h2>
    <div class="content">
        <h3>What is a Hypothesis?</h3>
        <p>A <strong>testable, falsifiable statement</strong> about the relationship between variables.</p>
        <p>Must be <strong>specific, measurable, and grounded in theory</strong>.</p>
        
        <h3>Types of Hypotheses</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Description</th>
                        <th>Example (Security Context)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Null Hypothesis (\(H_0\))</strong></td>
                        <td>No effect or no difference</td>
                        <td>"Adversarial training does <strong>not improve</strong> robust accuracy."</td>
                    </tr>
                    <tr>
                        <td><strong>Alternative Hypothesis (\(H_1\))</strong></td>
                        <td>There is an effect or difference</td>
                        <td>"Adversarial training <strong>improves</strong> robust accuracy by ≥5%."</td>
                    </tr>
                    <tr>
                        <td><strong>Directional</strong></td>
                        <td>Specifies direction of effect</td>
                        <td>"Model A is <strong>faster</strong> than Model B."</td>
                    </tr>
                    <tr>
                        <td><strong>Non-directional</strong></td>
                        <td>Only states difference exists</td>
                        <td>"Model A and Model B have <strong>different</strong> accuracy."</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Good Hypothesis Checklist</h3>
        <ul>
            <li>✅ Clear and concise</li>
            <li>✅ Based on prior work or theory</li>
            <li>✅ Testable with available data/methods</li>
            <li>✅ Falsifiable (can be proven wrong)</li>
        </ul>
        
        <div class="definition">
            🎯 <strong>Tip</strong>: Frame hypotheses around <strong>research questions</strong>:<br>
            <em>"Does technique X improve metric Y under condition Z?"</em>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Controlled Experiment Design</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Isolate the effect of an <strong>independent variable</strong> on a <strong>dependent variable</strong> by controlling for confounding factors.</p>
        
        <h3>Key Elements</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Element</th>
                        <th>Role</th>
                        <th>Security Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Independent Variable (IV)</strong></td>
                        <td>Manipulated by researcher</td>
                        <td>Type of defense (e.g., PGD training vs. baseline)</td>
                    </tr>
                    <tr>
                        <td><strong>Dependent Variable (DV)</strong></td>
                        <td>Measured outcome</td>
                        <td>Robust accuracy under AutoAttack</td>
                    </tr>
                    <tr>
                        <td><strong>Control Group</strong></td>
                        <td>Baseline (no treatment)</td>
                        <td>Model trained without adversarial examples</td>
                    </tr>
                    <tr>
                        <td><strong>Treatment Group(s)</strong></td>
                        <td>Receives intervention</td>
                        <td>Model trained with adversarial examples</td>
                    </tr>
                    <tr>
                        <td><strong>Confounding Variables</strong></td>
                        <td>External factors that distort results</td>
                        <td>Dataset version, random seed, hardware</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Common Experimental Designs</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Design</th>
                        <th>When to Use</th>
                        <th>Pros / Cons</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Between-Subjects</strong></td>
                        <td>Different subjects per condition</td>
                        <td>✅ No carryover effects<br>❌ Needs more samples</td>
                    </tr>
                    <tr>
                        <td><strong>Within-Subjects</strong></td>
                        <td>Same subjects in all conditions</td>
                        <td>✅ Higher statistical power<br>❌ Order effects (e.g., learning)</td>
                    </tr>
                    <tr>
                        <td><strong>Factorial Design</strong></td>
                        <td>Test multiple IVs simultaneously</td>
                        <td>✅ Efficient; detects interactions<br>❌ Complex analysis</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🔬 <strong>Security Example</strong>:<br>
            - <strong>IV</strong>: Defense method (None, FGSM, PGD)<br>
            - <strong>DV</strong>: Robust accuracy on CIFAR-10<br>
            - <strong>Control</strong>: Standard training<br>
            - <strong>Confounding controls</strong>: Same architecture, optimizer, seed
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Variable Selection and Control</h2>
    <div class="content">
        <h3>Types of Variables</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Definition</th>
                        <th>How to Handle</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Independent (Predictor)</strong></td>
                        <td>Input you manipulate</td>
                        <td>Systematically vary (e.g., defense type, \( \epsilon \))</td>
                    </tr>
                    <tr>
                        <td><strong>Dependent (Response)</strong></td>
                        <td>Output you measure</td>
                        <td>Define precisely (e.g., "robust accuracy @ \( \epsilon=8/255 \)")</td>
                    </tr>
                    <tr>
                        <td><strong>Control Variables</strong></td>
                        <td>Held constant</td>
                        <td>Fix random seed, dataset version, hardware</td>
                    </tr>
                    <tr>
                        <td><strong>Confounding Variables</strong></td>
                        <td>Uncontrolled influences</td>
                        <td>Measure and adjust (e.g., via ANCOVA) or randomize</td>
                    </tr>
                    <tr>
                        <td><strong>Extraneous Variables</strong></td>
                        <td>Unmeasured noise</td>
                        <td>Minimize via replication and randomization</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Best Practices for Control</h3>
        <ul>
            <li><strong>Randomization</strong>: Assign samples/models to groups randomly → distribute confounders evenly.</li>
            <li><strong>Blinding</strong>: If human judgment is involved (e.g., alert triage), hide group assignment.</li>
            <li><strong>Standardization</strong>: Use same preprocessing, evaluation protocol, and metrics across all conditions.</li>
            <li><strong>Baseline Matching</strong>: Ensure control and treatment groups start from same point (e.g., same clean accuracy).</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Pitfall in Security Research</strong>:<br>
            Comparing your method to a <strong>weak baseline</strong> (e.g., FGSM-only defense) → inflates perceived contribution.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Statistical Significance Considerations</h2>
    <div class="content">
        <h3>Why Statistics Matter</h3>
        <p>Determine if observed differences are <strong>real</strong> or due to <strong>random chance</strong>.</p>
        
        <h3>Key Concepts</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Description</th>
                        <th>Security Relevance</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>p-value</strong></td>
                        <td>Probability of observing result if \(H_0\) is true</td>
                        <td>\( p < 0.05 \) → reject \(H_0\) (convention)</td>
                    </tr>
                    <tr>
                        <td><strong>Confidence Interval (CI)</strong></td>
                        <td>Range where true effect likely lies (e.g., 95% CI)</td>
                        <td>"Accuracy gain: 4.2% [3.1%, 5.3%]"</td>
                    </tr>
                    <tr>
                        <td><strong>Effect Size</strong></td>
                        <td>Magnitude of difference (e.g., Cohen's d)</td>
                        <td>Distinguish <strong>statistical</strong> vs. <strong>practical</strong> significance</td>
                    </tr>
                    <tr>
                        <td><strong>Power Analysis</strong></td>
                        <td>Probability of detecting true effect</td>
                        <td>Avoid <strong>underpowered</strong> studies (common in ML)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Common Tests</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Test</th>
                        <th>Assumptions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Compare 2 groups (normal data)</td>
                        <td>t-test</td>
                        <td>Normality, equal variance</td>
                    </tr>
                    <tr>
                        <td>Compare 2 groups (non-normal)</td>
                        <td>Mann-Whitney U</td>
                        <td>Non-parametric</td>
                    </tr>
                    <tr>
                        <td>Compare >2 groups</td>
                        <td>ANOVA (+ post-hoc Tukey)</td>
                        <td>Normality, homogeneity of variance</td>
                    </tr>
                    <tr>
                        <td>Categorical outcomes</td>
                        <td>Chi-square test</td>
                        <td>Expected counts >5</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Critical Guidelines</h3>
        <ul>
            <li><strong>Never rely on p-values alone</strong>: Report <strong>effect size + CI</strong>.</li>
            <li><strong>Correct for multiple comparisons</strong>: Bonferroni, Holm-Bonferroni (e.g., when testing 10 defenses).</li>
            <li><strong>Check assumptions</strong>: Use Shapiro-Wilk (normality), Levene's test (equal variance).</li>
        </ul>
        
        <div class="definition">
            📉 <strong>Example</strong>:<br>
            "Our method improves robust accuracy by 6.2% (95% CI [5.1, 7.3], \( p = 0.003 \), Cohen's d = 1.2)."
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Reproducibility Principles</h2>
    <div class="content">
        <h3>Why Reproducibility Matters</h3>
        <ul>
            <li>Foundation of scientific integrity.</li>
            <li>Enables validation, extension, and real-world adoption.</li>
            <li>Required by top conferences (e.g., NeurIPS, IEEE S&P).</li>
        </ul>
        
        <h3>Reproducibility Spectrum</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Level</th>
                        <th>Definition</th>
                        <th>How to Achieve</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Repeatability</strong></td>
                        <td>Same team, same setup</td>
                        <td>Document code, data, environment</td>
                    </tr>
                    <tr>
                        <td><strong>Replicability</strong></td>
                        <td>Different team, same setup</td>
                        <td>Public code, data, detailed methods</td>
                    </tr>
                    <tr>
                        <td><strong>Generalizability</strong></td>
                        <td>Different setup (e.g., dataset)</td>
                        <td>Test on multiple benchmarks</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Best Practices for Reproducible Research</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Area</th>
                        <th>Action</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Code</strong></td>
                        <td>
                            <ul>
                                <li>Public GitHub repo</li>
                                <li>Versioned (Git tags)</li>
                                <li>Docker/Singularity container</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Data</strong></td>
                        <td>
                            <ul>
                                <li>Public datasets (CIFAR-10, CIC-IDS2017)</li>
                                <li>Synthetic data generators if sensitive</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Environment</strong></td>
                        <td>
                            <ul>
                                <li><code>requirements.txt</code> or <code>environment.yml</code></li>
                                <li>Hardware specs (GPU type, RAM)</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Randomness</strong></td>
                        <td>
                            <ul>
                                <li>Fix all seeds (Python, NumPy, PyTorch, TensorFlow)</li>
                                <li>Report seed values</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Evaluation</strong></td>
                        <td>
                            <ul>
                                <li>Use standard benchmarks (RobustBench, OGB)</li>
                                <li>Report mean ± std over ≥5 runs</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Documentation</strong></td>
                        <td>
                            <ul>
                                <li>README with setup/run instructions</li>
                                <li>Preprint (arXiv) with full methodology</li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Checklist for Security/AI Papers</h3>
        <ul>
            <li>✅ Code and model weights public</li>
            <li>✅ All hyperparameters reported</li>
            <li>✅ Evaluation against <strong>strong baselines</strong> (e.g., AutoAttack, not just FGSM)</li>
            <li>✅ Statistical significance tested</li>
            <li>✅ Multiple random seeds used</li>
            <li>✅ Compute resources disclosed</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Initiatives</strong>:<br>
            - <strong>ML Reproducibility Challenge</strong><br>
            - <strong>ACM Artifact Evaluation</strong><br>
            - <strong>IEEE Reproducible Research Initiative</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Experimental Design Checklist</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Phase</th>
                        <th>Key Question</th>
                        <th>Action</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Hypothesis</strong></td>
                        <td>Is it testable and falsifiable?</td>
                        <td>Write \(H_0\) and \(H_1\) clearly</td>
                    </tr>
                    <tr>
                        <td><strong>Design</strong></td>
                        <td>Are confounders controlled?</td>
                        <td>Use randomization, blinding, standardization</td>
                    </tr>
                    <tr>
                        <td><strong>Variables</strong></td>
                        <td>Are IV/DV well-defined?</td>
                        <td>Precisely specify metrics and conditions</td>
                    </tr>
                    <tr>
                        <td><strong>Statistics</strong></td>
                        <td>Is the result significant <em>and</em> meaningful?</td>
                        <td>Report p-value, CI, effect size</td>
                    </tr>
                    <tr>
                        <td><strong>Reproducibility</strong></td>
                        <td>Can others replicate this?</td>
                        <td>Publish code, data, seeds, environment</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Start with a strong hypothesis</strong> it guides your entire experiment.</li>
            <li><strong>Control is everything</strong>: Uncontrolled variables invalidate conclusions.</li>
            <li><strong>Statistics ≠ magic</strong>: Understand what your tests assume and mean.</li>
            <li><strong>Reproducibility is non-negotiable</strong>: It's part of scientific ethics.</li>
            <li><strong>In security/AI, benchmarking rigor is as important as novelty</strong>.</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Final Advice</strong>:<br>
            <strong>"If you wouldn't trust your own experiment as a reviewer, redesign it."</strong>
        </div>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Performance Metrics</h2>
    <div class="content">
        <p><em>Measuring Effectiveness, Efficiency, and Reliability</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>"You can't improve what you don't measure."</strong><br>
            Choosing the right metrics is critical especially in <strong>imbalanced security datasets</strong> (e.g., 99.9% benign, 0.1% malicious).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Detection Rate and Recall Metrics</h2>
    <div class="content">
        <h3>Detection Rate (True Positive Rate / Sensitivity / Recall)</h3>
        <ul>
            <li><strong>Definition</strong>: Proportion of <strong>actual positives correctly identified</strong>.<br>
            \[ \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}} \]</li>
            <li><strong>Security Context</strong>:
            <ul>
                <li><strong>High recall</strong> = Few missed threats (low false negatives).</li>
                <li>Critical for <strong>high-stakes scenarios</strong> (e.g., malware detection, fraud).</li>
            </ul>
            </li>
        </ul>
        
        <h3>Why Recall Matters in Security</h3>
        <ul>
            <li><strong>False negatives = breaches</strong>: Missed attacks can be catastrophic.</li>
            <li><strong>Trade-off</strong>: Increasing recall often increases false positives.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use when</strong>: Cost of missing a threat > cost of false alarm (e.g., ransomware detection).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">False Positive Rate and Precision</h2>
    <div class="content">
        <h3>False Positive Rate (FPR)</h3>
        <ul>
            <li><strong>Definition</strong>: Proportion of <strong>actual negatives incorrectly flagged as positive</strong>.<br>
            \[ \text{FPR} = \frac{\text{False Positives (FP)}}{\text{False Positives (FP)} + \text{True Negatives (TN)}} \]</li>
            <li><strong>Security Impact</strong>:
            <ul>
                <li>High FPR → <strong>alert fatigue</strong>, wasted analyst time, operational disruption.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Precision</h3>
        <ul>
            <li><strong>Definition</strong>: Proportion of <strong>predicted positives that are correct</strong>.<br>
            \[ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \]</li>
            <li><strong>Security Context</strong>:
            <ul>
                <li><strong>High precision</strong> = Most alerts are real threats.</li>
                <li>Critical for <strong>resource-constrained teams</strong> (e.g., small SOC).</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            ⚖️ <strong>Trade-off</strong>:<br>
            - <strong>High recall, low precision</strong>: Catch all threats, but many false alarms.<br>
            - <strong>High precision, low recall</strong>: Only high-confidence alerts, but miss subtle attacks.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">F1-Score and Balanced Accuracy</h2>
    <div class="content">
        <h3>F1-Score</h3>
        <ul>
            <li><strong>Definition</strong>: <strong>Harmonic mean</strong> of precision and recall (balances both).<br>
            \[ \text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \]</li>
            <li><strong>Why harmonic mean?</strong>
            <ul>
                <li>Penalizes extreme values (e.g., 100% precision + 0% recall → F1 = 0).</li>
            </ul>
            </li>
            <li><strong>Best for</strong>: <strong>Imbalanced datasets</strong> (common in security).</li>
        </ul>
        
        <h3>Balanced Accuracy</h3>
        <ul>
            <li><strong>Definition</strong>: Average of recall for each class.<br>
            \[ \text{Balanced Acc} = \frac{1}{2} \left( \frac{\text{TP}}{\text{TP} + \text{FN}} + \frac{\text{TN}}{\text{TN} + \text{FP}} \right) \]</li>
            <li><strong>Use Case</strong>: When <strong>class distribution is skewed</strong> (e.g., 99% benign traffic).</li>
            <li><strong>Advantage over accuracy</strong>: Not fooled by majority class.</li>
        </ul>
        
        <div class="definition">
            📊 <strong>Example</strong>:<br>
            - Dataset: 990 benign, 10 malicious<br>
            - Model flags all as benign → <strong>Accuracy = 99%</strong>, but <strong>Balanced Acc = 50%</strong>, <strong>Recall = 0%</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Receiver Operating Characteristic (ROC) Curves</h2>
    <div class="content">
        <h3>What is an ROC Curve?</h3>
        <p>Plots <strong>True Positive Rate (Recall)</strong> vs. <strong>False Positive Rate</strong> at <strong>various classification thresholds</strong>.</p>
        
        <h3>How to Interpret</h3>
        <ul>
            <li><strong>Top-left corner (0,1)</strong>: Perfect classifier.</li>
            <li><strong>Diagonal line (y=x)</strong>: Random guessing.</li>
            <li><strong>Closer to top-left</strong> = Better model.</li>
        </ul>
        
        <h3>Security Use Cases</h3>
        <ul>
            <li>Compare models <strong>independent of class imbalance</strong>.</li>
            <li>Select <strong>optimal threshold</strong> based on operational needs:
            <ul>
                <li><strong>High-security mode</strong>: Prioritize recall (accept higher FPR).</li>
                <li><strong>Low-noise mode</strong>: Prioritize precision (accept lower recall).</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Limitation</strong>:<br>
            ROC can be <strong>overly optimistic</strong> for highly imbalanced data (e.g., 0.1% positives).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Area Under Curve (AUC) Measurements</h2>
    <div class="content">
        <h3>AUC-ROC</h3>
        <ul>
            <li><strong>Definition</strong>: Area under the ROC curve (ranges from 0 to 1).</li>
            <li><strong>Interpretation</strong>:
            <ul>
                <li><strong>AUC = 1.0</strong>: Perfect classifier</li>
                <li><strong>AUC = 0.5</strong>: Random classifier</li>
                <li><strong>AUC > 0.8</strong>: Generally acceptable; >0.9 is excellent</li>
            </ul>
            </li>
            <li><strong>Advantage</strong>: Threshold-invariant; summarizes performance across all thresholds.</li>
        </ul>
        
        <h3>AUC-PR (Precision-Recall)</h3>
        <ul>
            <li><strong>Definition</strong>: Area under the <strong>Precision-Recall curve</strong>.</li>
            <li><strong>Why use it?</strong>
            <ul>
                <li>More <strong>informative than AUC-ROC for imbalanced data</strong>.</li>
                <li>Focuses on <strong>positive class performance</strong>.</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            📉 <strong>Rule of Thumb</strong>:<br>
            - <strong>Use AUC-ROC</strong> for balanced datasets.<br>
            - <strong>Use AUC-PR</strong> for imbalanced security datasets (e.g., intrusion detection).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Latency and Throughput Metrics</h2>
    <div class="content">
        <h3>Why System Metrics Matter</h3>
        <p>Security systems must operate in <strong>real-time</strong> (e.g., network inspection, fraud prevention).</p>
        <p>Poor performance → <strong>bypassed systems</strong> or <strong>degraded user experience</strong>.</p>
        
        <h3>Key Metrics</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Definition</th>
                        <th>Security Relevance</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Latency</strong></td>
                        <td>Time from input to output (e.g., ms per packet)</td>
                        <td>Must be < network RTT (e.g., < 10 ms for inline IPS)</td>
                    </tr>
                    <tr>
                        <td><strong>Throughput</strong></td>
                        <td>Volume processed per second (e.g., Gbps, events/sec)</td>
                        <td>Must handle peak traffic (e.g., 10 Gbps for enterprise firewall)</td>
                    </tr>
                    <tr>
                        <td><strong>Jitter</strong></td>
                        <td>Variability in latency</td>
                        <td>Critical for real-time systems (e.g., VoIP security)</td>
                    </tr>
                    <tr>
                        <td><strong>Resource Utilization</strong></td>
                        <td>CPU, memory, GPU usage</td>
                        <td>Impacts scalability and cost</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Measurement Best Practices</h3>
        <ul>
            <li><strong>Test under load</strong>: Use tools like <strong>iperf</strong>, <strong>tcpreplay</strong>, <strong>Apache Bench</strong>.</li>
            <li><strong>Measure end-to-end</strong>: Include I/O, network, and processing.</li>
            <li><strong>Profile bottlenecks</strong>: Use <strong>perf</strong>, <strong>FlameGraphs</strong>, <strong>NVIDIA Nsight</strong>.</li>
        </ul>
        
        <div class="definition">
            ⚡ <strong>Real-World Targets</strong>:<br>
            - <strong>Network IDS</strong>: < 1 ms latency, > 10 Gbps throughput<br>
            - <strong>EDR agent</strong>: < 5% CPU overhead<br>
            - <strong>API-based threat detection</strong>: < 100 ms response time
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Performance Metrics at a Glance</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Formula</th>
                        <th>Best For</th>
                        <th>Security Priority</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Recall (TPR)</strong></td>
                        <td>TP / (TP + FN)</td>
                        <td>Minimizing missed threats</td>
                        <td>High (ransomware, APTs)</td>
                    </tr>
                    <tr>
                        <td><strong>Precision</strong></td>
                        <td>TP / (TP + FP)</td>
                        <td>Minimizing false alarms</td>
                        <td>High (SOC with limited staff)</td>
                    </tr>
                    <tr>
                        <td><strong>F1-Score</strong></td>
                        <td>2·(Prec·Rec)/(Prec+Rec)</td>
                        <td>Imbalanced data</td>
                        <td>Medium-High</td>
                    </tr>
                    <tr>
                        <td><strong>Balanced Accuracy</strong></td>
                        <td>(TPR + TNR)/2</td>
                        <td>Skewed datasets</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td><strong>AUC-ROC</strong></td>
                        <td>Area under TPR vs. FPR</td>
                        <td>General model comparison</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td><strong>AUC-PR</strong></td>
                        <td>Area under Prec vs. Rec</td>
                        <td>Highly imbalanced data</td>
                        <td><strong>High</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Latency</strong></td>
                        <td>Time per inference</td>
                        <td>Real-time systems</td>
                        <td><strong>Critical</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Throughput</strong></td>
                        <td>Events/sec or Gbps</td>
                        <td>Scalability</td>
                        <td><strong>Critical</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>In security, recall is often more important than precision</strong> but balance based on operational capacity.</li>
            <li><strong>Never use accuracy alone</strong> for imbalanced data <strong>F1, AUC-PR, or balanced accuracy</strong> are better.</li>
            <li><strong>ROC is intuitive, but AUC-PR is more honest</strong> for rare threats.</li>
            <li><strong>System performance (latency/throughput) is as important as detection quality</strong> a slow detector is a useless detector.</li>
            <li><strong>Always report confidence intervals</strong> (e.g., F1 ± std over 5 runs) single numbers are misleading.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>:<br>
            <strong>"Choose metrics that mirror your operational reality."</strong><br>
            - SOC team drowning in alerts? → Optimize <strong>precision</strong>.<br>
            - Protecting critical infrastructure? → Maximize <strong>recall</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Validation Strategies</h2>
    <div class="content">
        <p><em>Ensuring Models Generalize Beyond Training Data</em></p>
        
        <div class="definition">
            <strong>Core Principle</strong>:<br>
            <strong>"A model that works only on your data isn't a solution it's a hypothesis."</strong><br>
            Validation answers: <em>Will this model perform reliably in production?</em>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Cross-Validation Techniques</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Estimate model performance by <strong>reusing data for both training and validation</strong> maximizing data efficiency.</p>
        
        <h3>Common Methods</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>How It Works</th>
                        <th>Pros</th>
                        <th>Cons</th>
                        <th>Security Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>k-Fold CV</strong></td>
                        <td>Split data into <em>k</em> folds; train on <em>k-1</em>, validate on 1; repeat <em>k</em> times</td>
                        <td>Low bias; uses all data</td>
                        <td>Computationally expensive</td>
                        <td>Malware detection on static datasets (e.g., EMBER)</td>
                    </tr>
                    <tr>
                        <td><strong>Stratified k-Fold</strong></td>
                        <td>Preserves class distribution in each fold</td>
                        <td>Essential for imbalanced data</td>
                        <td>Slightly more complex</td>
                        <td>Phishing detection (99% benign URLs)</td>
                    </tr>
                    <tr>
                        <td><strong>Leave-One-Out (LOO)</strong></td>
                        <td><em>k = n</em> (each sample is a fold)</td>
                        <td>Unbiased for small datasets</td>
                        <td>Extremely slow for large <em>n</em></td>
                        <td>Rare attack detection (e.g., APTs)</td>
                    </tr>
                    <tr>
                        <td><strong>Group k-Fold</strong></td>
                        <td>Ensure samples from same group (e.g., user, device) stay together</td>
                        <td>Prevents data leakage</td>
                        <td>Requires group metadata</td>
                        <td>User behavior analytics (avoid mixing user sessions)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Best Practices</h3>
        <ul>
            <li>Use <strong>stratified k-fold (k=5 or 10)</strong> as default for imbalanced security data.</li>
            <li><strong>Never use standard k-fold for time-series or grouped data</strong> leads to <strong>data leakage</strong>.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Pitfall</strong>:<br>
            In cybersecurity, <strong>samples are often correlated</strong> (e.g., multiple alerts from same host). Group-aware CV is critical.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Hold-Out Validation Strategies</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Simple train/validation/test split fast and intuitive.</p>
        
        <h3>Standard Approach</h3>
        <ul>
            <li><strong>Train set</strong>: 60–80% → model training</li>
            <li><strong>Validation set</strong>: 10–20% → hyperparameter tuning</li>
            <li><strong>Test set</strong>: 10–20% → final unbiased evaluation</li>
        </ul>
        
        <h3>Variants for Security</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Strategy</th>
                        <th>When to Use</th>
                        <th>Risk</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random Hold-Out</strong></td>
                        <td>Static, IID data (e.g., malware binaries)</td>
                        <td>Safe if data is truly random</td>
                    </tr>
                    <tr>
                        <td><strong>Temporal Hold-Out</strong></td>
                        <td>Time-ordered data (e.g., logs)</td>
                        <td><strong>Must split by time</strong> (see Section 3)</td>
                    </tr>
                    <tr>
                        <td><strong>Adversarial Hold-Out</strong></td>
                        <td>Evaluate robustness</td>
                        <td>Include worst-case samples in test set</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Critical Rules</h3>
        <ul>
            <li><strong>Test set must be untouched</strong> until final evaluation.</li>
            <li><strong>Never tune hyperparameters on test set</strong> this invalidates results.</li>
            <li>For imbalanced data, <strong>stratify the split</strong> to preserve minority class.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use Case</strong>:<br>
            Initial prototyping of a new phishing classifier on a labeled URL dataset.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Temporal Validation for Time-Series Data</h2>
    <div class="content">
        <h3>Why Standard Splits Fail</h3>
        <p>Security data is <strong>inherently temporal</strong> (e.g., logs, network flows, alerts).</p>
        <p>Random splits leak <strong>future information into training</strong> → <strong>overly optimistic results</strong>.</p>
        
        <h3>Correct Approach: Time-Based Splitting</h3>
        <ul>
            <li><strong>Training</strong>: Data from <strong>t₀ to t₁</strong></li>
            <li><strong>Validation</strong>: Data from <strong>t₁ to t₂</strong></li>
            <li><strong>Test</strong>: Data from <strong>t₂ to t₃</strong></li>
            <li><strong>Direction</strong>: Always <strong>past → future</strong></li>
        </ul>
        
        <h3>Advanced Temporal Strategies</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Description</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Rolling Window</strong></td>
                        <td>Fixed-size window slides forward in time</td>
                        <td>Concept drift detection (e.g., evolving malware)</td>
                    </tr>
                    <tr>
                        <td><strong>Expanding Window</strong></td>
                        <td>Training set grows over time</td>
                        <td>Long-term model monitoring</td>
                    </tr>
                    <tr>
                        <td><strong>Purged Group CV</strong></td>
                        <td>Remove samples near validation window to prevent leakage</td>
                        <td>Financial fraud (with embargo period)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Implementation (Python Example)</h3>
        <pre>from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    # Train and evaluate</pre>
        
        <div class="definition">
            🔮 <strong>Security Reality</strong>:<br>
            Attackers evolve <strong>models trained on 2023 data may fail in 2024</strong>. Temporal validation mimics real-world deployment.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">External Validation on Independent Datasets</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Test generalization to <strong>unseen environments, sources, or time periods</strong>.</p>
        
        <h3>Why It Matters</h3>
        <ul>
            <li>Internal validation (CV, hold-out) measures <strong>within-dataset performance</strong>.</li>
            <li><strong>External validation</strong> measures <strong>real-world robustness</strong>.</li>
        </ul>
        
        <h3>Types of External Datasets</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Example</th>
                        <th>Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Different Time Period</strong></td>
                        <td>Train on 2022 data, test on 2023 data</td>
                        <td>Tests temporal generalization</td>
                    </tr>
                    <tr>
                        <td><strong>Different Organization</strong></td>
                        <td>Train on Company A logs, test on Company B</td>
                        <td>Tests domain shift robustness</td>
                    </tr>
                    <tr>
                        <td><strong>Public Benchmarks</strong></td>
                        <td>Train on custom data, test on CIC-IDS2017</td>
                        <td>Enables fair comparison</td>
                    </tr>
                    <tr>
                        <td><strong>Red-Team Data</strong></td>
                        <td>Test on adversarial examples generated by experts</td>
                        <td>Measures worst-case robustness</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Data heterogeneity</strong>: Different logging formats, features, label definitions.</li>
            <li><strong>Label scarcity</strong>: External datasets often lack ground truth.</li>
            <li><strong>Privacy</strong>: Cannot share raw logs across organizations.</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Best Practice</strong>:<br>
            Use <strong>standardized benchmarks</strong> (e.g., <strong>CIC-IDS2017</strong> for network intrusion, <strong>EMBER</strong> for malware) for external validation.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Real-World Validation Approaches</h2>
    <div class="content">
        <h3>Beyond Offline Metrics</h3>
        <p>Offline validation (AUC, F1) ≠ real-world performance.</p>
        <p><strong>Operational factors</strong> dominate in production.</p>
        
        <h3>Key Real-World Validation Methods</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>How It Works</th>
                        <th>Security Application</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Shadow Mode Deployment</strong></td>
                        <td>Run model in parallel with production system; log predictions but don't act</td>
                        <td>Validate new EDR detection logic without risk</td>
                    </tr>
                    <tr>
                        <td><strong>Canary Releases</strong></td>
                        <td>Deploy to small % of users/devices; monitor performance</td>
                        <td>Roll out new phishing classifier to 5% of email traffic</td>
                    </tr>
                    <tr>
                        <td><strong>A/B Testing</strong></td>
                        <td>Compare model A vs. B on live traffic</td>
                        <td>Test two fraud detection models on payment transactions</td>
                    </tr>
                    <tr>
                        <td><strong>Red Teaming</strong></td>
                        <td>Hire ethical hackers to bypass the system</td>
                        <td>Test resilience of network detection system</td>
                    </tr>
                    <tr>
                        <td><strong>User Feedback Loops</strong></td>
                        <td>Collect analyst feedback on alerts (e.g., "true/false positive")</td>
                        <td>Improve SOC alert quality over time</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Operational Metrics to Track</h3>
        <ul>
            <li><strong>Mean Time to Detect (MTTD)</strong></li>
            <li><strong>Mean Time to Respond (MTTR)</strong></li>
            <li><strong>Analyst workload</strong> (alerts per hour)</li>
            <li><strong>False positive rate in production</strong></li>
            <li><strong>System overhead</strong> (CPU, latency)</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Golden Rule</strong>:<br>
            <strong>"If you haven't tested it in a realistic environment, you haven't validated it."</strong>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Validation Strategy Selection Guide</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Recommended Strategy</th>
                        <th>Why</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Static, balanced dataset</strong></td>
                        <td>Stratified k-fold CV</td>
                        <td>Maximizes data use; reliable estimate</td>
                    </tr>
                    <tr>
                        <td><strong>Imbalanced security data</strong></td>
                        <td>Stratified k-fold + AUC-PR</td>
                        <td>Handles class skew</td>
                    </tr>
                    <tr>
                        <td><strong>Time-series (logs, flows)</strong></td>
                        <td>Temporal hold-out or TimeSeriesSplit</td>
                        <td>Prevents future leakage</td>
                    </tr>
                    <tr>
                        <td><strong>User/device-level data</strong></td>
                        <td>Group k-fold CV</td>
                        <td>Avoids data leakage across entities</td>
                    </tr>
                    <tr>
                        <td><strong>Production readiness</strong></td>
                        <td>Shadow mode + red teaming</td>
                        <td>Tests real-world robustness</td>
                    </tr>
                    <tr>
                        <td><strong>Research benchmarking</strong></td>
                        <td>External validation on public datasets</td>
                        <td>Enables fair comparison</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Never use random splits for time-series or grouped data</strong> it invalidates results.</li>
            <li><strong>Cross-validation is great for prototyping, but not enough for production</strong>.</li>
            <li><strong>Temporal validation is non-negotiable</strong> in cybersecurity threats evolve.</li>
            <li><strong>External validation exposes hidden biases</strong> (e.g., model works only on your network).</li>
            <li><strong>Real-world validation (shadow mode, red teaming) is the ultimate test</strong>.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Final Advice</strong>:<br>
            <strong>"Validate like an attacker, not just a data scientist."</strong><br>
            Assume your model will face <strong>worst-case, adaptive adversaries</strong> design validation accordingly.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Advanced Topics: Federated Learning</h2>
    <div class="content">
        <p><em>Training Models Collaboratively Without Sharing Raw Data</em></p>
        
        <div class="definition">
            <strong>Core Idea</strong>:<br>
            <strong>"Bring the code to the data, not the data to the code."</strong><br>
            Federated Learning enables collaborative model training across decentralized devices or organizations while keeping raw data <strong>local</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Federated Learning Principles and Architectures</h2>
    <div class="content">
        <h3>What is Federated Learning?</h3>
        <p>A distributed machine learning paradigm where:</p>
        <ul>
            <li><strong>Data remains on local devices</strong> (e.g., smartphones, hospitals, edge sensors).</li>
            <li>Only <strong>model updates</strong> (not raw data) are shared with a central server.</li>
            <li>A <strong>global model</strong> is learned by aggregating local updates.</li>
        </ul>
        
        <h3>Key Principles</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Principle</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Data Privacy</strong></td>
                        <td>Raw data never leaves the device</td>
                    </tr>
                    <tr>
                        <td><strong>Decentralization</strong></td>
                        <td>Training happens on edge/client devices</td>
                    </tr>
                    <tr>
                        <td><strong>Collaborative Learning</strong></td>
                        <td>Global model benefits from diverse data sources</td>
                    </tr>
                    <tr>
                        <td><strong>Heterogeneity Awareness</strong></td>
                        <td>Handles non-IID data and device variability</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Standard FL Architecture (Federated Averaging - FedAvg)</h3>
        <ol>
            <li><strong>Initialization</strong>: Server broadcasts global model \( \mathbf{w}_0 \) to selected clients.</li>
            <li><strong>Local Training</strong>: Each client \( k \) trains on local data for \( E \) epochs:<br>
            \[ \mathbf{w}_k = \mathbf{w}_t - \eta \nabla \mathcal{L}_k(\mathbf{w}_t) \]</li>
            <li><strong>Upload</strong>: Clients send updated weights \( \mathbf{w}_k \) to server.</li>
            <li><strong>Aggregation</strong>: Server computes weighted average:<br>
            \[ \mathbf{w}_{t+1} = \sum_{k=1}^K \frac{n_k}{n} \mathbf{w}_k \]
            where \( n_k \) = local data size, \( n = \sum n_k \).</li>
            <li><strong>Repeat</strong> for \( T \) rounds.</li>
        </ol>
        
        <h3>FL Deployment Models</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Participants</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Cross-Device FL</strong></td>
                        <td>Millions of mobile/IoT devices</td>
                        <td>Gboard keyboard prediction (Google)</td>
                    </tr>
                    <tr>
                        <td><strong>Cross-Silo FL</strong></td>
                        <td>Few organizations (e.g., hospitals, banks)</td>
                        <td>Medical imaging, financial fraud detection</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🌐 <strong>Real-World Example</strong>:<br>
            Google uses FL to train <strong>Gboard's next-word prediction</strong> on 100M+ devices without collecting user keystrokes.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Privacy-Preserving Federated Learning</h2>
    <div class="content">
        <h3>Why Privacy Isn't Guaranteed by FL Alone</h3>
        <p><strong>Model updates can leak data</strong>: Gradients/weights may reveal sensitive information (e.g., membership inference, model inversion attacks).</p>
        
        <h3>Privacy-Enhancing Technologies (PETs) in FL</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>How It Works</th>
                        <th>Trade-offs</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Differential Privacy (DP)</strong></td>
                        <td>Add calibrated noise to updates: \( \mathbf{w}_k + \mathcal{N}(0, \sigma^2) \)</td>
                        <td>↓ Accuracy, ↑ Privacy (tunable via \( \epsilon \))</td>
                    </tr>
                    <tr>
                        <td><strong>Secure Aggregation (SecAgg)</strong></td>
                        <td>Cryptographic protocol: Server learns only sum \( \sum \mathbf{w}_k \), not individual \( \mathbf{w}_k \)</td>
                        <td>↑ Communication overhead; requires honest majority</td>
                    </tr>
                    <tr>
                        <td><strong>Homomorphic Encryption (HE)</strong></td>
                        <td>Encrypt updates; server aggregates in encrypted domain</td>
                        <td>Very high computational cost</td>
                    </tr>
                    <tr>
                        <td><strong>Trusted Execution Environments (TEEs)</strong></td>
                        <td>Run aggregation in hardware-secured enclaves (e.g., Intel SGX)</td>
                        <td>Hardware dependency; side-channel risks</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Hybrid Approaches</h3>
        <ul>
            <li><strong>DP + SecAgg</strong>: Strong privacy with practical utility (used in Apple, Google FL systems).</li>
            <li><strong>Local DP</strong>: Add noise on device before sending → stronger privacy but lower utility.</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Best Practice</strong>:<br>
            Use <strong>SecAgg for confidentiality</strong> + <strong>DP for formal privacy guarantees</strong> in high-risk domains (e.g., healthcare).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Federated Optimization Algorithms</h2>
    <div class="content">
        <h3>Challenges in FL Optimization</h3>
        <ul>
            <li><strong>Non-IID data</strong>: Clients have different data distributions (e.g., users type different words).</li>
            <li><strong>Client heterogeneity</strong>: Devices vary in compute, bandwidth, availability.</li>
            <li><strong>Partial participation</strong>: Only a fraction of clients participate per round.</li>
        </ul>
        
        <h3>Key Algorithms</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Key Innovation</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>FedAvg</strong> (McMahan et al., 2017)</td>
                        <td>Local SGD + weighted averaging</td>
                        <td>Baseline for most FL systems</td>
                    </tr>
                    <tr>
                        <td><strong>FedProx</strong></td>
                        <td>Adds proximal term to handle client drift: \( \mu \|\mathbf{w} - \mathbf{w}_t\|^2 \)</td>
                        <td>Highly non-IID data</td>
                    </tr>
                    <tr>
                        <td><strong>SCAFFOLD</strong></td>
                        <td>Corrects client drift using control variates</td>
                        <td>Improves convergence on heterogeneous data</td>
                    </tr>
                    <tr>
                        <td><strong>FedNova</strong></td>
                        <td>Normalizes local updates by number of steps</td>
                        <td>Handles variable local epochs</td>
                    </tr>
                    <tr>
                        <td><strong>FedOpt</strong> (e.g., FedAdam)</td>
                        <td>Server-side adaptive optimizers (Adam, Yogi)</td>
                        <td>Faster convergence on complex models</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Convergence Considerations</h3>
        <ul>
            <li><strong>Non-IID data slows convergence</strong> → requires more rounds.</li>
            <li><strong>Client sampling strategy</strong> matters:
            <ul>
                <li><strong>Uniform</strong>: Simple but may miss rare clients</li>
                <li><strong>Importance sampling</strong>: Weight by data size or loss</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            📉 <strong>Rule of Thumb</strong>:<br>
            Start with <strong>FedAvg</strong>, then switch to <strong>FedProx/SCAFFOLD</strong> if non-IID performance degrades.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Communication Efficiency in Federated Learning</h2>
    <div class="content">
        <h3>Why Communication is the Bottleneck</h3>
        <p>Uploading/downloading model weights (e.g., 100 MB for ResNet) over slow/mobile networks.</p>
        <p>Millions of devices → massive bandwidth cost.</p>
        
        <h3>Efficiency Techniques</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Mechanism</th>
                        <th>Compression Ratio</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Model Compression</strong></td>
                        <td>Quantize weights (FP32 → INT8) before upload</td>
                        <td>2–4×</td>
                    </tr>
                    <tr>
                        <td><strong>Sparsification</strong></td>
                        <td>Send only top-*k*% important weights (e.g., magnitude-based)</td>
                        <td>10–100×</td>
                    </tr>
                    <tr>
                        <td><strong>Structured Updates</strong></td>
                        <td>Restrict updates to low-rank matrices or specific layers</td>
                        <td>5–50×</td>
                    </tr>
                    <tr>
                        <td><strong>Client Subsampling</strong></td>
                        <td>Train on only 1–10% of clients per round</td>
                        <td>Proportional reduction</td>
                    </tr>
                    <tr>
                        <td><strong>Gradient Compression</strong></td>
                        <td>Use error feedback to accumulate skipped updates</td>
                        <td>Maintains accuracy</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Advanced: Over-the-Air Computation (AirComp)</h3>
        <ul>
            <li><strong>Analog aggregation</strong>: Devices transmit simultaneously; server receives superposition \( \sum \mathbf{w}_k \) in one shot.</li>
            <li><strong>Benefit</strong>: Reduces rounds from <em>K</em> to 1.</li>
            <li><strong>Challenge</strong>: Requires synchronized wireless channels.</li>
        </ul>
        
        <div class="definition">
            📶 <strong>Real-World Impact</strong>:<br>
            Google reduced Gboard FL communication by <strong>99%</strong> using quantization + sparsification.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Security Challenges in Federated Learning</h2>
    <div class="content">
        <h3>Threat Model</h3>
        <ul>
            <li><strong>Honest-but-curious server</strong>: Follows protocol but tries to infer client data.</li>
            <li><strong>Malicious clients</strong>: Send poisoned updates to corrupt global model.</li>
            <li><strong>Eavesdroppers</strong>: Intercept updates on network.</li>
        </ul>
        
        <h3>Key Attacks</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Attack</th>
                        <th>Goal</th>
                        <th>Defense</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Model Poisoning</strong></td>
                        <td>Inject backdoors or degrade accuracy</td>
                        <td>Robust aggregation (e.g., <strong>Krum</strong>, <strong>Median</strong>)</td>
                    </tr>
                    <tr>
                        <td><strong>Membership Inference</strong></td>
                        <td>Determine if a sample was in client's data</td>
                        <td>DP, SecAgg</td>
                    </tr>
                    <tr>
                        <td><strong>Model Inversion</strong></td>
                        <td>Reconstruct client's private data from updates</td>
                        <td>Gradient clipping, DP</td>
                    </tr>
                    <tr>
                        <td><strong>Free-Rider Attacks</strong></td>
                        <td>Malicious clients send fake updates to save compute</td>
                        <td>Authentication, reputation systems</td>
                    </tr>
                    <tr>
                        <td><strong>Sybil Attacks</strong></td>
                        <td>Attacker controls many fake clients</td>
                        <td>Client identity verification</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Robust Aggregation Methods</h3>
        <ul>
            <li><strong>Krum</strong>: Selects update closest to others (resistant to < 50% malicious clients).</li>
            <li><strong>Median / Trimmed Mean</strong>: Discard outliers before averaging.</li>
            <li><strong>FoolsGold</strong>: Detects sybils by analyzing update similarity.</li>
        </ul>
        
        <h3>System-Level Defenses</h3>
        <ul>
            <li><strong>Client authentication</strong>: Ensure only legitimate devices participate.</li>
            <li><strong>Update validation</strong>: Check for norm bounds, anomaly detection on updates.</li>
            <li><strong>Blockchain-based FL</strong>: Immutable audit trail for updates (research stage).</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Critical Insight</strong>:<br>
            <strong>FL shifts attack surface from data theft to model corruption</strong> security must evolve accordingly.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Federated Learning Landscape</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Key Challenge</th>
                        <th>State-of-the-Art Solutions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Privacy</strong></td>
                        <td>Update leakage</td>
                        <td>DP + Secure Aggregation</td>
                    </tr>
                    <tr>
                        <td><strong>Optimization</strong></td>
                        <td>Non-IID data</td>
                        <td>FedProx, SCAFFOLD</td>
                    </tr>
                    <tr>
                        <td><strong>Communication</strong></td>
                        <td>Bandwidth cost</td>
                        <td>Sparsification, quantization</td>
                    </tr>
                    <tr>
                        <td><strong>Security</strong></td>
                        <td>Model poisoning</td>
                        <td>Robust aggregation (Krum, Median)</td>
                    </tr>
                    <tr>
                        <td><strong>Deployment</strong></td>
                        <td>Device heterogeneity</td>
                        <td>Cross-device vs. cross-silo architectures</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>FL enables privacy-preserving collaboration</strong> but <strong>is not private by default</strong>.</li>
            <li><strong>Non-IID data is the #1 challenge</strong> use FedProx/SCAFFOLD for stability.</li>
            <li><strong>Communication efficiency is critical</strong> compress, sparsify, and subsample.</li>
            <li><strong>Security ≠ Privacy</strong>: Defend against both <strong>data leakage</strong> and <strong>model poisoning</strong>.</li>
            <li><strong>Real-world FL requires co-design</strong> of algorithms, systems, and cryptography.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Future Directions</strong>:<br>
            - <strong>Personalized FL</strong>: Global model + local fine-tuning<br>
            - <strong>Federated Learning with LLMs</strong>: Training/fine-tuning large models on-device<br>
            - <strong>Regulatory-compliant FL</strong>: Meeting GDPR, HIPAA via PETs
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Machine Learning (QML)</h2>
    <div class="content">
        <p><em>Harnessing Quantum Mechanics for Next-Generation AI</em></p>
        
        <div class="definition">
            <strong>Core Vision</strong>:<br>
            Leverage <strong>quantum superposition, entanglement, and interference</strong> to accelerate or enhance machine learning tasks either through <strong>quantum speedups</strong> or <strong>novel model expressivity</strong>.
        </div>
        
        <div class="definition">
            <strong>Reality Check</strong>:<br>
            Most QML algorithms require <strong>fault-tolerant quantum computers</strong> (not yet available). Current work focuses on <strong>Noisy Intermediate-Scale Quantum (NISQ)</strong> devices and <strong>quantum-classical hybrids</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Neural Networks (QNNs)</h2>
    <div class="content">
        <h3>What is a QNN?</h3>
        <ul>
            <li>A parameterized quantum circuit that mimics classical neural networks.</li>
            <li><strong>Input</strong>: Classical data encoded into quantum state (e.g., via amplitude or angle encoding).</li>
            <li><strong>Processing</strong>: Layers of parameterized quantum gates (e.g., rotation gates \( R_X(\theta), R_Y(\theta) \)).</li>
            <li><strong>Output</strong>: Measurement probabilities used as predictions.</li>
        </ul>
        
        <h3>Architecture</h3>
        <ul>
            <li><strong>Encoding Layer</strong>: Map classical vector \( \mathbf{x} \in \mathbb{R}^d \) to quantum state \( |\psi(\mathbf{x})\rangle \).
            <ul>
                <li><em>Angle encoding</em>: \( |\psi(\mathbf{x})\rangle = \bigotimes_{i=1}^n R_Y(x_i)|0\rangle \)</li>
            </ul>
            </li>
            <li><strong>Ansatz (Variational Circuit)</strong>: Alternating layers of entangling gates (e.g., CNOT) and parameterized rotations.</li>
            <li><strong>Measurement</strong>: Expectation value \( \langle \psi(\mathbf{x}) | H | \psi(\mathbf{x}) \rangle \) as output (e.g., \( H = Z^{\otimes n} \)).</li>
        </ul>
        
        <h3>Training</h3>
        <ul>
            <li><strong>Loss function</strong>: Classical (e.g., MSE, cross-entropy).</li>
            <li><strong>Optimization</strong>: Classical optimizer (e.g., Adam) updates parameters via <strong>parameter-shift rule</strong> (quantum gradient estimation).</li>
        </ul>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Barren plateaus</strong>: Gradients vanish exponentially with qubit count.</li>
            <li><strong>Expressivity vs. trainability trade-off</strong>: Highly expressive ansätze are hard to train.</li>
            <li><strong>Data encoding bottleneck</strong>: Classical data loading may negate quantum advantage.</li>
        </ul>
        
        <div class="definition">
            🧪 <strong>Use Case</strong>:<br>
            Small-scale classification (e.g., Iris dataset) on IBM Quantum, Rigetti.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Support Vector Machines (QSVM)</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Use <strong>quantum kernels</strong> to compute high-dimensional feature maps <strong>intractable classically</strong>.</p>
        
        <h3>How It Works</h3>
        <ol>
            <li><strong>Quantum Feature Map</strong>:<br>
            Encode classical data \( \mathbf{x} \) into quantum state \( |\phi(\mathbf{x})\rangle \) via parameterized circuit.</li>
            <li><strong>Quantum Kernel</strong>:<br>
            Compute kernel as <strong>state overlap</strong>:<br>
            \[ k(\mathbf{x}, \mathbf{x}') = |\langle \phi(\mathbf{x}) | \phi(\mathbf{x}') \rangle|^2 \]</li>
            <li><strong>Classical SVM</strong>:<br>
            Train SVM using quantum kernel matrix (computed on quantum hardware).</li>
        </ol>
        
        <h3>Potential Advantage</h3>
        <ul>
            <li>Kernel corresponds to <strong>exponentially large feature space</strong> (e.g., \( 2^n \)-dimensional for \( n \) qubits).</li>
            <li>Could classify data <strong>not linearly separable</strong> in classical space.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Kernel estimation requires many circuit executions</strong> → costly on NISQ devices.</li>
            <li><strong>No proven exponential speedup</strong> for real-world datasets.</li>
            <li><strong>Classical simulation</strong> often matches quantum kernel performance (e.g., using random Fourier features).</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Status</strong>:<br>
            Primarily a <strong>proof-of-concept</strong>; outperformed by classical kernels on most benchmarks.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Generative Adversarial Networks (QGANs)</h2>
    <div class="content">
        <h3>Concept</h3>
        <p>Quantum version of GANs:</p>
        <ul>
            <li><strong>Generator (G)</strong>: Parameterized quantum circuit → produces quantum state \( \rho_G \).</li>
            <li><strong>Discriminator (D)</strong>: Quantum or classical model → distinguishes \( \rho_G \) from real data \( \rho_{\text{real}} \).</li>
        </ul>
        
        <h3>Training Objective</h3>
        <div class="equation">
            \[ \min_G \max_D \mathbb{E}_{\text{real}}[\log D(\rho_{\text{real}})] + \mathbb{E}_{\text{fake}}[\log(1 - D(\rho_G))] \]
        </div>
        
        <h3>Implementations</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Generator</th>
                        <th>Discriminator</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Fully Quantum</strong></td>
                        <td>Quantum circuit</td>
                        <td>Quantum circuit</td>
                        <td>Generate quantum states (e.g., for physics)</td>
                    </tr>
                    <tr>
                        <td><strong>Hybrid</strong></td>
                        <td>Quantum circuit</td>
                        <td>Classical NN</td>
                        <td>Generate classical data (e.g., images)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Quantum data</strong>: Simulate ground states of Hamiltonians.</li>
            <li><strong>Classical data</strong>: Generate synthetic financial time series or small images.</li>
            <li><strong>Loading distributions</strong>: Prepare quantum states encoding classical probability distributions.</li>
        </ul>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Mode collapse</strong>: Common in GANs; exacerbated by quantum noise.</li>
            <li><strong>Measurement cost</strong>: Estimating loss requires many shots.</li>
            <li><strong>Limited qubit count</strong>: Cannot generate high-resolution data.</li>
        </ul>
        
        <div class="definition">
            🎨 <strong>Example</strong>:<br>
            IBM demonstrated QGAN generating <strong>2-qubit Bell states</strong> and <strong>3-qubit W states</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Reinforcement Learning (QRL)</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Use quantum agents to <strong>learn optimal policies</strong> in environments faster than classical agents.</p>
        
        <h3>Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Quantum Advantage Claim</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Quantum Speedup in MDPs</strong></td>
                        <td>Quadratic speedup in policy evaluation (via quantum sampling)</td>
                        <td>Theoretical (requires fault-tolerant QC)</td>
                    </tr>
                    <tr>
                        <td><strong>Quantum Policy Representation</strong></td>
                        <td>Represent policy as quantum state → explore action space in superposition</td>
                        <td>NISQ-era experiments (e.g., grid-world tasks)</td>
                    </tr>
                    <tr>
                        <td><strong>Quantum Reward Processing</strong></td>
                        <td>Use quantum amplitude estimation for faster reward estimation</td>
                        <td>Requires oracle access</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>NISQ-Era QRL</h3>
        <ul>
            <li><strong>Hybrid framework</strong>:
            <ul>
                <li>Quantum circuit encodes policy \( \pi(a|s; \theta) \).</li>
                <li>Classical environment provides rewards.</li>
                <li>Classical optimizer updates \( \theta \).</li>
            </ul>
            </li>
            <li><strong>Demonstrated on</strong>:
            <ul>
                <li>Simple grid worlds</li>
                <li>Portfolio optimization</li>
                <li>Quantum control tasks</li>
            </ul>
            </li>
        </ul>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Environment interaction is classical</strong> → limits quantum advantage.</li>
            <li><strong>Credit assignment problem</strong>: Hard to isolate quantum contribution.</li>
            <li><strong>Scalability</strong>: No evidence of advantage on complex RL tasks (e.g., Atari).</li>
        </ul>
        
        <div class="definition">
            🤖 <strong>Outlook</strong>:<br>
            QRL is promising for <strong>quantum-native environments</strong> (e.g., controlling quantum hardware).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum-Classical Hybrid Approaches</h2>
    <div class="content">
        <h3>Why Hybrid?</h3>
        <ul>
            <li>NISQ devices lack error correction → cannot run deep quantum circuits.</li>
            <li><strong>Best of both worlds</strong>: Quantum processors handle <strong>specific subroutines</strong>; classical systems manage the rest.</li>
        </ul>
        
        <h3>Key Frameworks</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Framework</th>
                        <th>How It Works</th>
                        <th>Example Algorithms</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Variational Quantum Algorithms (VQA)</strong></td>
                        <td>Quantum circuit evaluates cost; classical optimizer updates parameters</td>
                        <td>VQE (chemistry), QAOA (optimization), QNNs</td>
                    </tr>
                    <tr>
                        <td><strong>Quantum Embedding Models</strong></td>
                        <td>Use quantum circuit as <strong>feature map</strong>; classical ML on top</td>
                        <td>QSVM, quantum kernel PCA</td>
                    </tr>
                    <tr>
                        <td><strong>Quantum Co-Processors</strong></td>
                        <td>Offload specific tasks (e.g., sampling, linear algebra) to quantum hardware</td>
                        <td>HHL algorithm (theoretical), quantum Monte Carlo</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Advantages</h3>
        <ul>
            <li><strong>NISQ-compatible</strong>: Shallow circuits (< 100 gates).</li>
            <li><strong>Modular</strong>: Integrate with existing ML pipelines.</li>
            <li><strong>Flexible</strong>: Tune quantum/classical boundary based on hardware.</li>
        </ul>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Optimization landscape</strong>: Barren plateaus, noise-induced local minima.</li>
            <li><strong>Data encoding</strong>: Efficiently loading classical data remains hard (QRAM not available).</li>
            <li><strong>Benchmarking</strong>: Hard to prove quantum advantage over classical baselines.</li>
        </ul>
        
        <div class="definition">
            🌉 <strong>Real-World Example</strong>:<br>
            <strong>Quantum Boltzmann Machines</strong> (hybrid) used for anomaly detection in finance (D-Wave).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Quantum ML Approaches Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Quantum Component</th>
                        <th>Classical Component</th>
                        <th>NISQ Feasible?</th>
                        <th>Proven Advantage?</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>QNNs</strong></td>
                        <td>Parameterized circuit</td>
                        <td>Optimizer, loss</td>
                        <td>✅ Yes</td>
                        <td>❌ No (so far)</td>
                    </tr>
                    <tr>
                        <td><strong>QSVM</strong></td>
                        <td>Kernel estimation</td>
                        <td>SVM solver</td>
                        <td>✅ Yes</td>
                        <td>❌ No</td>
                    </tr>
                    <tr>
                        <td><strong>QGANs</strong></td>
                        <td>Generator circuit</td>
                        <td>Discriminator, training loop</td>
                        <td>✅ Yes</td>
                        <td>❌ No</td>
                    </tr>
                    <tr>
                        <td><strong>QRL</strong></td>
                        <td>Policy circuit</td>
                        <td>Environment, reward</td>
                        <td>✅ Yes</td>
                        <td>❌ No</td>
                    </tr>
                    <tr>
                        <td><strong>Hybrid (VQA)</strong></td>
                        <td>Cost evaluation</td>
                        <td>Optimization</td>
                        <td>✅ Yes</td>
                        <td>⚠️ Only for quantum-native problems</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>QML is not magic</strong>: Most algorithms offer <strong>no proven speedup</strong> on classical data.</li>
            <li><strong>NISQ limitations dominate</strong>: Noise, qubit count, and connectivity restrict real-world use.</li>
            <li><strong>Hybrid is the present</strong>: Variational methods (QNNs, VQE) are the dominant paradigm.</li>
            <li><strong>Quantum advantage likely requires</strong>:
            <ul>
                <li>Fault-tolerant quantum computers</li>
                <li><strong>Quantum-native data</strong> (e.g., quantum chemistry, quantum sensor data)</li>
            </ul>
            </li>
            <li><strong>Beware of hype</strong>: Many "quantum ML" papers show no advantage over classical methods.</li>
        </ul>
        
        <div class="definition">
            🔮 <strong>Future Outlook</strong>:<br>
            QML will likely first impact <strong>quantum science</strong> (e.g., material discovery) before revolutionizing classical AI.
        </div>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Post-Quantum Cryptography (PQC)</h2>
    <div class="content">
        <p><em>Cryptographic Algorithms Resistant to Quantum Attacks</em></p>
        
        <div class="definition">
            <strong>Why PQC Matters</strong>:<br>
            <strong>Shor's algorithm</strong> (1994) can factor integers and solve discrete logarithms in <strong>polynomial time</strong> on a fault-tolerant quantum computer <strong>breaking all widely used public-key cryptography</strong>.<br>
            <strong>Grover's algorithm</strong> provides only <strong>quadratic speedup</strong> for symmetric crypto so AES-256 remains safe (with minor key size adjustments).
        </div>
        
        <div class="definition">
            <strong>Goal of PQC</strong>:<br>
            Develop <strong>quantum-resistant</strong> cryptographic primitives that run on <strong>classical computers</strong> but resist attacks from <strong>both classical and quantum adversaries</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum-Resistant Cryptographic Algorithms</h2>
    <div class="content">
        <h3>NIST PQC Standardization Process</h3>
        <ul>
            <li>Launched in 2016 to select <strong>quantum-safe public-key algorithms</strong>.</li>
            <li><strong>Finalists (2022–2024)</strong>:
            <ul>
                <li><strong>KEMs (Key Encapsulation Mechanisms)</strong>:
                <ul>
                    <li><strong>CRYSTALS-Kyber</strong> (lattice-based) → <strong>Selected for standardization</strong></li>
                </ul>
                </li>
                <li><strong>Digital Signatures</strong>:
                <ul>
                    <li><strong>CRYSTALS-Dilithium</strong> (lattice-based) → <strong>Primary standard</strong></li>
                    <li><strong>FALCON</strong> (lattice-based) → For memory-constrained devices</li>
                    <li><strong>SPHINCS+</strong> (hash-based) → <strong>Backup standard</strong></li>
                </ul>
                </li>
            </ul>
            </li>
        </ul>
        
        <h3>Security Requirements</h3>
        <ul>
            <li><strong>Classical security</strong>: Resistant to best-known classical attacks.</li>
            <li><strong>Quantum security</strong>: Resistant to attacks using quantum computers (e.g., Grover, Shor, quantum walks).</li>
            <li><strong>Efficiency</strong>: Practical key sizes, signature sizes, and computation time.</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Migration Timeline</strong>:<br>
            NIST recommends <strong>organizations begin PQC migration by 2030</strong> before large-scale quantum computers arrive.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Lattice-Based Cryptography</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Base security on <strong>hard problems in lattice theory</strong>, such as:</p>
        <ul>
            <li><strong>Shortest Vector Problem (SVP)</strong>: Find shortest non-zero vector in lattice.</li>
            <li><strong>Learning With Errors (LWE)</strong>: Distinguish noisy linear equations from random.</li>
        </ul>
        
        <h3>Why Lattices?</h3>
        <ul>
            <li><strong>Strong security reductions</strong>: Breaking LWE ≈ solving worst-case lattice problems.</li>
            <li><strong>Versatile</strong>: Supports <strong>KEMs, signatures, fully homomorphic encryption (FHE)</strong>.</li>
            <li><strong>Efficient</strong>: Relatively small keys and fast operations.</li>
        </ul>
        
        <h3>Key Algorithms</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Type</th>
                        <th>Key Features</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CRYSTALS-Kyber</strong></td>
                        <td>KEM</td>
                        <td>
                            <ul>
                                <li>Based on <strong>Module-LWE</strong></li>
                                <li>CCA-secure</li>
                                <li>Key size: ~1–2 KB</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>CRYSTALS-Dilithium</strong></td>
                        <td>Signature</td>
                        <td>
                            <ul>
                                <li>Based on <strong>Module-LWE + Module-SIS</strong></li>
                                <li>Fast signing/verification</li>
                                <li>Signature size: ~2–4 KB</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>FALCON</strong></td>
                        <td>Signature</td>
                        <td>
                            <ul>
                                <li>Based on <strong>NTRU lattice</strong></li>
                                <li>Smaller signatures (~0.5–1 KB)</li>
                                <li>Requires floating-point arithmetic</li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Implementation complexity</strong>: Constant-time code needed to prevent side-channel attacks.</li>
            <li><strong>Memory usage</strong>: FALCON requires large precomputed tables.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Status</strong>: <strong>Dominant PQC approach</strong> selected for NIST standards due to balance of security, performance, and flexibility.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Hash-Based Signatures</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Use <strong>cryptographic hash functions</strong> (assumed quantum-resistant if sufficiently long) to build signatures.</p>
        <p>Security relies on <strong>collision resistance</strong> and <strong>preimage resistance</strong> of the hash function.</p>
        
        <h3>Types</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>How It Works</th>
                        <th>Limitation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Stateful (e.g., XMSS)</strong></td>
                        <td>One-time signatures (OTS) chained via Merkle tree; <strong>must track state</strong></td>
                        <td>Risk of reuse if state lost → <strong>not suitable for all systems</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Stateless (e.g., SPHINCS+)</strong></td>
                        <td>Hierarchical trees of OTS; <strong>no state tracking needed</strong></td>
                        <td>Larger signatures (~8–49 KB), slower</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>SPHINCS+ (NIST Backup Standard)</h3>
        <ul>
            <li><strong>Structure</strong>: Forest of Merkle trees layered over FORS (Few-Time Signatures).</li>
            <li><strong>Parameters</strong>:
            <ul>
                <li><strong>Security level</strong>: 128-bit (SHA-256)</li>
                <li><strong>Signature size</strong>: 8 KB (fast) to 49 KB (small)</li>
                <li><strong>Public key</strong>: ~1 KB</li>
            </ul>
            </li>
            <li><strong>Advantage</strong>: <strong>Only hash-based scheme without state</strong> ideal for embedded systems.</li>
        </ul>
        
        <h3>Why Hash-Based?</h3>
        <ul>
            <li><strong>Minimal assumptions</strong>: Only requires secure hash function.</li>
            <li><strong>Long history</strong>: Merkle trees (1979); proven security.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Limitation</strong>: <strong>Signatures only</strong> cannot build KEMs or encryption from hash functions alone.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Code-Based Cryptography</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Base security on <strong>hard problems in error-correcting codes</strong>, such as:</p>
        <ul>
            <li><strong>Syndrome Decoding Problem</strong>: Given a random linear code and syndrome, find low-weight error vector.</li>
        </ul>
        
        <h3>Flagship Algorithm: Classic McEliece</h3>
        <ul>
            <li><strong>Type</strong>: KEM (submitted to NIST as <strong>Classic McEliece</strong>)</li>
            <li><strong>How it works</strong>:
            <ul>
                <li>Public key: Scrambled generator matrix of Goppa code.</li>
                <li>Encryption: Add random errors to message; only holder of private key can decode.</li>
            </ul>
            </li>
            <li><strong>Parameters (NIST Level 1)</strong>:
            <ul>
                <li><strong>Public key size</strong>: <strong>~250 KB</strong> (very large)</li>
                <li><strong>Ciphertext size</strong>: ~200 bytes</li>
                <li><strong>Security</strong>: Based on 60+ years of cryptanalysis</li>
            </ul>
            </li>
        </ul>
        
        <h3>Pros and Cons</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Pros</th>
                        <th>Cons</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>
                            <ul>
                                <li>✅ Extremely conservative security</li>
                                <li>✅ Fast decryption</li>
                                <li>✅ Resistant to all known attacks</li>
                            </ul>
                        </td>
                        <td>
                            <ul>
                                <li>❌ Huge public keys (impractical for TLS, IoT)</li>
                                <li>❌ Not suitable for signatures</li>
                                <li>❌ Key size doesn't scale well</li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            📦 <strong>Use Case</strong>:<br>
            Best for <strong>long-term archival encryption</strong> where key size is acceptable (e.g., government, healthcare records).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Multivariate Polynomial Cryptography</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Base security on <strong>hardness of solving systems of multivariate quadratic equations</strong> over finite fields.</p>
        <p><strong>MQ Problem</strong>: Given \( \mathbf{y} = \mathcal{P}(\mathbf{x}) \), find \( \mathbf{x} \) (where \( \mathcal{P} \) is quadratic map).</p>
        
        <h3>Key Algorithms</h3>
        <ul>
            <li><strong>Rainbow</strong>: Signature scheme based on <strong>Oil and Vinegar</strong> construction.
            <ul>
                <li><strong>Status</strong>: <strong>Broken in 2022</strong> (Beullens attack) → <strong>not selected by NIST</strong>.</li>
            </ul>
            </li>
            <li><strong>GeMSS</strong>, <strong>MQDSS</strong>: Other multivariate schemes (less efficient or broken).</li>
        </ul>
        
        <h3>Why It's Struggling</h3>
        <ul>
            <li><strong>Small keys and fast operations</strong> but <strong>repeatedly broken</strong> by algebraic attacks.</li>
            <li><strong>No strong security reductions</strong> (unlike lattices).</li>
            <li><strong>NIST eliminated all multivariate finalists</strong> due to security concerns.</li>
        </ul>
        
        <h3>Current Status</h3>
        <ul>
            <li><strong>Not recommended</strong> for new deployments.</li>
            <li>Research continues on <strong>new constructions</strong>, but trust is low.</li>
        </ul>
        
        <div class="definition">
            🚫 <strong>Lesson</strong>:<br>
            Efficiency without <strong>provable security</strong> is dangerous in cryptography.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: PQC Approaches Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Best For</th>
                        <th>Key Size</th>
                        <th>Signature Size</th>
                        <th>NIST Status</th>
                        <th>Quantum Security Basis</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Lattice-Based</strong></td>
                        <td>KEMs, Signatures, FHE</td>
                        <td>1–2 KB</td>
                        <td>0.5–4 KB</td>
                        <td>✅ <strong>Selected</strong> (Kyber, Dilithium, FALCON)</td>
                        <td>LWE, SIS</td>
                    </tr>
                    <tr>
                        <td><strong>Hash-Based</strong></td>
                        <td>Signatures (stateless)</td>
                        <td>~1 KB</td>
                        <td>8–49 KB</td>
                        <td>✅ <strong>Backup</strong> (SPHINCS+)</td>
                        <td>Hash function security</td>
                    </tr>
                    <tr>
                        <td><strong>Code-Based</strong></td>
                        <td>KEMs (long-term)</td>
                        <td>~250 KB</td>
                        <td>N/A</td>
                        <td>⚠️ Finalist (Classic McEliece)</td>
                        <td>Syndrome decoding</td>
                    </tr>
                    <tr>
                        <td><strong>Multivariate</strong></td>
                        <td>Signatures (historical)</td>
                        <td><1 KB</td>
                        <td>1–5 KB</td>
                        <td>❌ <strong>Eliminated</strong></td>
                        <td>MQ problem (broken)</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Lattice-based cryptography is the future</strong>: NIST's primary choice for both KEMs and signatures.</li>
            <li><strong>Hash-based signatures are the safe fallback</strong>: SPHINCS+ requires no new assumptions.</li>
            <li><strong>Code-based is niche</strong>: Only where massive key sizes are acceptable.</li>
            <li><strong>Multivariate is deprecated</strong>: Avoid due to repeated breaks.</li>
            <li><strong>Hybrid deployments are critical</strong>: Combine <strong>classical + PQC</strong> during transition (e.g., ECDH + Kyber).</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Migration Strategy</strong>:<br>
            1. <strong>Inventory</strong> systems using RSA/ECC.<br>
            2. <strong>Prioritize</strong> long-life data (e.g., root CAs, encrypted archives).<br>
            3. <strong>Test</strong> PQC libraries (Open Quantum Safe, liboqs).<br>
            4. <strong>Deploy hybrid</strong> solutions now (e.g., TLS 1.3 with Kyber + X25519).
        </div>
    </div>
</section>





        <div class="references">
    <div class="ref-title">References:</div>
    <div class="ref-item">[1] National Institute of Standards and Technology. (2024). Post-Quantum Cryptography Standardization. https://csrc.nist.gov/projects/post-quantum-cryptography</div>
    <div class="ref-item">[2] Bernstein, D. J., & Lange, T. (2017). Post-quantum cryptography. Nature, 549(7671), 188-194.</div>
    <div class="ref-item">[3] Shor, P. W. (1999). Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM Review, 41(2), 303-332.</div>
    <div class="ref-item">[4] Grover, L. K. (1996). A fast quantum mechanical algorithm for database search. Proceedings of the 28th Annual ACM Symposium on Theory of Computing, 212-219.</div>
    <div class="ref-item">[5] Regev, O. (2009). On lattices, learning with errors, random linear codes, and cryptography. Journal of the ACM, 56(6), 1-40.</div>
    <div class="ref-item">[6] Hoffstein, J., Pipher, J., & Silverman, J. H. (2008). An Introduction to Mathematical Cryptography. Springer.</div>
    <div class="ref-item">[7] Buchmann, J., Dahmen, E., & Kremer, F. (2017). Hash-based digital signature schemes. In Post-Quantum Cryptography (pp. 123-152). Springer.</div>
    <div class="ref-item">[8] McEliece, R. J. (1978). A public-key cryptosystem based on algebraic coding theory. Deep Space Network Progress Report, 42(44), 114-116.</div>
    <div class="ref-item">[9] Ding, J., & Yang, B. Y. (2018). Multivariate public key cryptography. In Post-Quantum Cryptography (pp. 153-180). Springer.</div>
    <div class="ref-item">[10] Bernstein, D. J., Buchmann, J., & Dahmen, E. (Eds.). (2009). Post-Quantum Cryptography. Springer.</div>
</div>

<footer>
    <p>© 2025 Post-Quantum Cryptography Research Group | University of Excellence</p>
    <p>For academic and research use only | This document is optimized for printing and reference</p>
    <p>Justin alexia andrew</p>
</footer>
        

    </div>

    <script>
        // This script is included for demonstration purposes
        // In a real implementation, you would add content dynamically here
        document.addEventListener('DOMContentLoaded', function() {
            // Example of how you might add content dynamically
            // This would be replaced with your content insertion logic
            console.log('Document ready for content insertion');
        });
    </script>
</body>
</html>