<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Academic Document</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            padding: 20px;
            counter-reset: section;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
            padding: 40px;
        }

        header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #e0e0e0;
        }

        h1 {
            font-size: 2.5rem;
            color: #2c3e50;
            margin-bottom: 10px;
        }

        .subtitle {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 15px;
        }

        .author-info {
            display: flex;
            justify-content: space-between;
            margin-top: 15px;
            font-size: 0.9rem;
            color: #7f8c8d;
        }

        .section {
            margin-bottom: 30px;
            padding: 20px 0;
            border-bottom: 1px solid #eee;
        }

        .section-title {
            font-size: 1.8rem;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 1px solid #eee;
            color: #2c3e50;
            position: relative;
            counter-increment: section;
        }

        .section-title::before {
            content: counter(section) ". ";
            color: #3498db;
        }

        .content {
            padding: 10px 0;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .equation {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background-color: #f9f9f9;
            border-left: 4px solid #3498db;
            font-size: 1.2rem;
        }

        .table-container {
            overflow-x: auto;
            margin: 20px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 0.9rem;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        .highlight {
            background-color: #e3f2fd;
            padding: 2px 4px;
            border-radius: 3px;
        }

        .definition {
            border-left: 4px solid #27ae60;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #f8fff8;
        }

        .theorem {
            border-left: 4px solid #e74c3c;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #fff8f8;
        }

        .proof {
            border-left: 4px solid #9b59b6;
            padding: 10px 20px;
            margin: 15px 0;
            background-color: #f8f8ff;
            font-style: italic;
        }

        .references {
            margin-top: 30px;
        }

        .ref-title {
            font-weight: bold;
            margin-bottom: 10px;
        }

        .ref-item {
            margin-bottom: 8px;
            padding-left: 20px;
            text-indent: -20px;
        }

        footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            color: #7f8c8d;
            font-size: 0.9rem;
        }

        @media print {
            body {
                background-color: white;
                padding: 0;
            }
            
            .container {
                box-shadow: none;
                padding: 20px;
                max-width: 100%;
            }
            
            button {
                display: none;
            }
        }

        .print-btn {
            display: block;
            margin: 20px auto;
            padding: 12px 30px;
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 1rem;
            transition: background-color 0.3s;
        }

        .print-btn:hover {
            background-color: #2980b9;
        }

        .print-btn i {
            margin-right: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Mathematical Analysis and Applications</h1>
            <div class="subtitle">A Comprehensive Study of Fundamental Concepts</div>
            <div class="author-info">
                <div>Dr. Jane Smith</div>
                <div>Department of Mathematics</div>
                <div>University of Excellence</div>
            </div>
        </header>

        <section class="section">
    <h2 class="section-title">Quantum Bits (Qubits)</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing Fundamentals</em></p>
        
        <h3>Definition and Properties of Qubits</h3>
        <h4>What is a Qubit?</h4>
        <p>A <strong>qubit</strong> (quantum bit) is the fundamental unit of quantum information, analogous to the classical bit in classical computing.</p>
        <p>Unlike a classical bit, which can only be in state <strong>0</strong> or <strong>1</strong>, a qubit can exist in a <strong>superposition</strong> of both states simultaneously.</p>
        
        <h4>Key Properties of Qubits</h4>
        <ol>
            <li><strong>Superposition</strong>: Can be in a linear combination of |0⟩ and |1⟩.</li>
            <li><strong>Entanglement</strong>: Qubits can be correlated in ways that classical bits cannot (though this is more relevant for multi-qubit systems).</li>
            <li><strong>Measurement Collapse</strong>: When measured, the qubit collapses probabilistically to either |0⟩ or |1⟩.</li>
            <li><strong>No-Cloning Theorem</strong>: An unknown quantum state cannot be perfectly copied.</li>
            <li><strong>Unitary Evolution</strong>: Qubit states evolve via reversible, unitary operations (quantum gates).</li>
        </ol>
        
        <h4>Physical Realizations</h4>
        <p>Qubits can be implemented using:</p>
        <ul>
            <li>Electron or nuclear spin (e.g., in NMR or quantum dots)</li>
            <li>Photon polarization</li>
            <li>Superconducting circuits (e.g., transmon qubits)</li>
            <li>Trapped ions</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Superposition Principle and Its Mathematical Representation</h2>
    <div class="content">
        <h3>Superposition Principle</h3>
        <p>A qubit can be in a state that is a <strong>linear combination</strong> of the basis states |0⟩ and |1⟩:</p>
        <div class="equation">
            \[ |\psi\rangle = \alpha |0\rangle + \beta |1\rangle \]
        </div>
        <p>where:</p>
        <ul>
            <li>\(\alpha, \beta \in \mathbb{C}\) (complex numbers),</li>
            <li>\(|\alpha|^2 + |\beta|^2 = 1\) (normalization condition).</li>
        </ul>
        
        <h3>Interpretation</h3>
        <ul>
            <li>\(|\alpha|^2\) = probability of measuring the qubit as <strong>0</strong>.</li>
            <li>\(|\beta|^2\) = probability of measuring the qubit as <strong>1</strong>.</li>
            <li>The relative <strong>phase</strong> between \(\alpha\) and \(\beta\) is physically significant (e.g., for interference).</li>
        </ul>
        
        <h3>Example</h3>
        <p>Equal superposition state:</p>
        <div class="equation">
            \[ |+\rangle = \frac{1}{\sqrt{2}}|0\rangle + \frac{1}{\sqrt{2}}|1\rangle \]
        </div>
        <p>→ 50% chance of measuring 0 or 1.</p>
        
        <p>Another example:</p>
        <div class="equation">
            \[ |\psi\rangle = \frac{\sqrt{3}}{2}|0\rangle + \frac{1}{2}|1\rangle \]
        </div>
        <p>→ Probability of 0: \( \left|\frac{\sqrt{3}}{2}\right|^2 = \frac{3}{4} \),<br>
        Probability of 1: \( \left|\frac{1}{2}\right|^2 = \frac{1}{4} \).</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Bloch Sphere Visualization of Qubit States</h2>
    <div class="content">
        <h3>Why the Bloch Sphere?</h3>
        <p>Any single-qubit pure state can be represented as a point on the surface of a unit sphere called the <strong>Bloch sphere</strong>.</p>
        <p>Provides an intuitive geometric representation of qubit states.</p>
        
        <h3>Mathematical Mapping</h3>
        <p>A general qubit state can be written as:</p>
        <div class="equation">
            \[ |\psi\rangle = \cos\left(\frac{\theta}{2}\right)|0\rangle + e^{i\phi}\sin\left(\frac{\theta}{2}\right)|1\rangle \]
        </div>
        <p>where:</p>
        <ul>
            <li>\(0 \leq \theta \leq \pi\)</li>
            <li>\(0 \leq \phi < 2\pi\)</li>
        </ul>
        
        <h3>Coordinates on the Bloch Sphere</h3>
        <ul>
            <li><strong>North pole</strong>: |0⟩</li>
            <li><strong>South pole</strong>: |1⟩</li>
            <li><strong>Equator</strong>: Superposition states with equal probabilities (e.g., |+⟩, |−⟩, |i⟩, |−i⟩)</li>
            <li><strong>θ (theta)</strong>: Polar angle from z-axis → determines probability amplitudes.</li>
            <li><strong>φ (phi)</strong>: Azimuthal angle in xy-plane → determines relative phase.</li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: Global phase (e.g., multiplying the entire state by \(e^{i\gamma}\)) has no physical effect and is ignored.
        </div>
        
        <h3>Visualization</h3>
        <p>X, Y, Z axes correspond to measurement bases:</p>
        <ul>
            <li>Z-basis: {|0⟩, |1⟩}</li>
            <li>X-basis: {|+⟩, |−⟩}</li>
            <li>Y-basis: {|i⟩, |−i⟩}</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Measurement and Collapse of Quantum States</h2>
    <div class="content">
        <h3>Quantum Measurement Postulate</h3>
        <p>When a qubit in state \(|\psi\rangle = \alpha|0\rangle + \beta|1\rangle\) is measured in the computational (Z) basis:</p>
        <ul>
            <li>Outcome is <strong>0</strong> with probability \(|\alpha|^2\)</li>
            <li>Outcome is <strong>1</strong> with probability \(|\beta|^2\)</li>
        </ul>
        <p><strong>Immediately after measurement</strong>, the state <strong>collapses</strong> to the observed basis state:</p>
        <ul>
            <li>If result is 0 → state becomes |0⟩</li>
            <li>If result is 1 → state becomes |1⟩</li>
        </ul>
        
        <h3>Irreversibility</h3>
        <p>Measurement is <strong>non-unitary</strong> and <strong>irreversible</strong>—information about the original superposition is lost.</p>
        
        <h3>Basis Dependence</h3>
        <p>Measurement can be performed in any orthonormal basis (e.g., X-basis).</p>
        <p>Example: Measuring |+⟩ in X-basis always yields "+", but in Z-basis yields 0 or 1 with 50% probability each.</p>
        
        <h3>Expectation Value</h3>
        <p>For an observable (e.g., Pauli-Z operator), the expectation value is:</p>
        <div class="equation">
            \[ \langle Z \rangle = \langle \psi | Z | \psi \rangle = |\alpha|^2 - |\beta|^2 \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Comparison with Classical Bits</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Classical Bit</th>
                        <th>Qubit</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>State</td>
                        <td>0 <strong>or</strong> 1</td>
                        <td>\(\alpha|0\rangle + \beta|1\rangle\) (superposition)</td>
                    </tr>
                    <tr>
                        <td>State Space</td>
                        <td>Discrete (2 states)</td>
                        <td>Continuous (infinite states on Bloch sphere)</td>
                    </tr>
                    <tr>
                        <td>Measurement</td>
                        <td>Reveals existing value (non-destructive)</td>
                        <td>Probabilistic; <strong>collapses</strong> the state</td>
                    </tr>
                    <tr>
                        <td>Copying</td>
                        <td>Can be copied perfectly</td>
                        <td><strong>Cannot</strong> be copied (No-Cloning Theorem)</td>
                    </tr>
                    <tr>
                        <td>Information Capacity</td>
                        <td>1 bit per bit</td>
                        <td>Infinite info in state, but only <strong>1 bit</strong> extractable per measurement</td>
                    </tr>
                    <tr>
                        <td>Operations</td>
                        <td>Logic gates (AND, OR, NOT)</td>
                        <td>Reversible <strong>unitary gates</strong> (e.g., Hadamard, Pauli)</td>
                    </tr>
                    <tr>
                        <td>Parallelism</td>
                        <td>No inherent parallelism</td>
                        <td><strong>Quantum parallelism</strong>: operations on all states in superposition</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Key Insight</strong>: While a single qubit holds more <em>potential</em> information than a classical bit, <strong>only one classical bit</strong> can be extracted per measurement due to collapse. The power of quantum computing arises from <strong>interference</strong> and <strong>entanglement</strong> across many qubits—not from storing more data per qubit.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <ul>
            <li>A <strong>qubit</strong> generalizes the classical bit using quantum mechanics.</li>
            <li>It leverages <strong>superposition</strong> to process multiple states at once.</li>
            <li>The <strong>Bloch sphere</strong> offers a geometric view of all possible single-qubit states.</li>
            <li><strong>Measurement</strong> is probabilistic and destructive to superposition.</li>
            <li>Qubits are <strong>not just probabilistic bits</strong>—phase and interference are crucial for quantum advantage.</li>
        </ul>
    </div>
</section>
<section class="section">
    <h2 class="section-title">Quantum Entanglement</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Information and Computing</em></p>
        
        <h3>Definition and Properties of Entangled States</h3>
        <h4>What is Quantum Entanglement?</h4>
        <p><strong>Entanglement</strong> is a uniquely quantum phenomenon where two or more particles become <strong>correlated</strong> in such a way that the quantum state of each particle <strong>cannot be described independently</strong> of the others—even when separated by large distances.</p>
        <p>The combined system is described by a <strong>single quantum state</strong>, but individual subsystems <strong>do not have definite states</strong> on their own.</p>
        
        <h4>Formal Definition</h4>
        <p>A pure bipartite state \(|\psi\rangle_{AB}\) is <strong>entangled</strong> if it <strong>cannot</strong> be written as a <strong>product state</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle_{AB} \neq |\phi\rangle_A \otimes |\chi\rangle_B \]
        </div>
        <p>If such a factorization is possible, the state is <strong>separable</strong> (not entangled).</p>
        
        <div class="definition">
            <strong>Example of entangled state</strong>:<br>
            \(|\Phi^+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)\) → cannot be split into individual qubit states.
        </div>
        
        <h4>Key Properties of Entangled States</h4>
        <ol>
            <li><strong>Non-separability</strong>: The whole system's state is global; parts lack individual pure states.</li>
            <li><strong>Non-locality</strong>: Measurement outcomes on one subsystem <strong>instantaneously affect</strong> the other (though no faster-than-light communication is possible).</li>
            <li><strong>Monogamy</strong>: If qubit A is maximally entangled with B, it <strong>cannot</strong> be entangled with C.</li>
            <li><strong>Conservation under local unitary operations</strong>: Entanglement is invariant under local operations (but can be changed by measurements or noise).</li>
            <li><strong>Fragility</strong>: Entanglement is easily destroyed by <strong>decoherence</strong> (interaction with environment).</li>
        </ol>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Bell States and Their Significance</h2>
    <div class="content">
        <h3>What are Bell States?</h3>
        <p>The <strong>Bell states</strong> (or <strong>EPR pairs</strong>) are four specific <strong>maximally entangled</strong> two-qubit states that form an orthonormal basis for the 2-qubit Hilbert space (\(\mathbb{C}^2 \otimes \mathbb{C}^2\)).</p>
        
        <h3>The Four Bell States</h3>
        <div class="equation">
            \[ \begin{aligned}
            |\Phi^+\rangle &= \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle) \\
            |\Phi^-\rangle &= \frac{1}{\sqrt{2}}(|00\rangle - |11\rangle) \\
            |\Psi^+\rangle &= \frac{1}{\sqrt{2}}(|01\rangle + |10\rangle) \\
            |\Psi^-\rangle &= \frac{1}{\sqrt{2}}(|01\rangle - |10\rangle)
            \end{aligned} \]
        </div>
        
        <h3>Significance</h3>
        <ul>
            <li><strong>Maximal entanglement</strong>: Each Bell state has <strong>1 ebit</strong> (entanglement bit) of entanglement.</li>
            <li><strong>Basis for protocols</strong>: Used in quantum teleportation, superdense coding, and entanglement swapping.</li>
            <li><strong>Test of quantum non-locality</strong>: Violate <strong>Bell inequalities</strong>, distinguishing quantum mechanics from local hidden-variable theories.</li>
            <li><strong>Resource states</strong>: Serve as fundamental resources in quantum communication and computation.</li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: \(|\Psi^-\rangle\) is also called the <strong>singlet state</strong>—antisymmetric under particle exchange.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Non-locality and Quantum Correlations</h2>
    <div class="content">
        <h3>What is Non-locality?</h3>
        <p><strong>Quantum non-locality</strong> refers to correlations between distant particles that <strong>cannot be explained</strong> by any <strong>local realistic theory</strong> (i.e., theories assuming:
        <ul>
            <li><strong>Locality</strong>: No influence faster than light.</li>
            <li><strong>Realism</strong>: Physical properties exist prior to measurement.)</li>
        </ul>
        
        <h3>Bell's Theorem (1964)</h3>
        <p>John Bell showed that <strong>any local hidden-variable theory</strong> must obey certain statistical constraints (<strong>Bell inequalities</strong>).</p>
        <p>Quantum mechanics <strong>violates</strong> these inequalities.</p>
        <p>Experiments (e.g., by Aspect, 1982; loophole-free tests, 2015) confirm these violations → <strong>local realism is false</strong>.</p>
        
        <h3>Example: CHSH Inequality</h3>
        <p>For observables \(A_1, A_2\) on qubit A and \(B_1, B_2\) on qubit B:</p>
        <div class="equation">
            \[ S = \langle A_1B_1 \rangle + \langle A_1B_2 \rangle + \langle A_2B_1 \rangle - \langle A_2B_2 \rangle \]
        </div>
        <ul>
            <li><strong>Local hidden-variable theories</strong>: \(|S| \leq 2\)</li>
            <li><strong>Quantum mechanics</strong>: \(|S| \leq 2\sqrt{2} \approx 2.828\) (Tsirelson's bound)</li>
            <li>Achieved using entangled states (e.g., \(|\Phi^+\rangle\)) and specific measurement bases.</li>
        </ul>
        
        <h3>Important Clarifications</h3>
        <ul>
            <li><strong>No faster-than-light communication</strong>: Outcomes are random; only <strong>correlations</strong> are non-local. Cannot transmit information instantaneously (<strong>no-signaling theorem</strong>).</li>
            <li><strong>Correlation ≠ Causation</strong>: Measurement on A doesn't "cause" B's state—it reveals pre-existing (but non-classical) correlation.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Mathematical Representation of Entanglement</h2>
    <div class="content">
        <h3>Pure Bipartite States</h3>
        <p>A state \(|\psi\rangle_{AB}\) is <strong>entangled</strong> iff its <strong>Schmidt rank > 1</strong>.</p>
        <p><strong>Schmidt decomposition</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle_{AB} = \sum_i \lambda_i |u_i\rangle_A \otimes |v_i\rangle_B \]
        </div>
        <p>where \(\lambda_i \geq 0\), \(\sum_i \lambda_i^2 = 1\).</p>
        <p><strong>Entangled</strong> ⇔ more than one non-zero \(\lambda_i\).</p>
        
        <h3>Mixed States and Entanglement Criteria</h3>
        <p>For mixed states (\(\rho_{AB}\)), entanglement is harder to detect:</p>
        <ul>
            <li><strong>Separable state</strong>: \(\rho_{AB} = \sum_k p_k \, \rho_A^{(k)} \otimes \rho_B^{(k)}\), with \(p_k \geq 0\), \(\sum_k p_k = 1\).</li>
            <li><strong>Entangled</strong> ⇔ not separable.</li>
        </ul>
        
        <h4>Entanglement Detection Tools</h4>
        <ol>
            <li><strong>Positive Partial Transpose (PPT) Criterion</strong> (Peres-Horodecki):
            <ul>
                <li>If \(\rho^{T_B}\) (partial transpose over B) has <strong>negative eigenvalues</strong> → state is entangled.</li>
                <li><strong>Necessary and sufficient</strong> for 2×2 and 2×3 systems.</li>
            </ul>
            </li>
            <li><strong>Entanglement Witnesses</strong>: Hermitian operators \(W\) such that \(\text{Tr}(W\sigma) \geq 0\) for all separable \(\sigma\), but \(\text{Tr}(W\rho) < 0\) for some entangled \(\rho\).</li>
        </ol>
        
        <h3>Quantifying Entanglement (Pure States)</h3>
        <ul>
            <li><strong>Entanglement entropy</strong>: \(S(\rho_A) = -\text{Tr}(\rho_A \log_2 \rho_A)\), where \(\rho_A = \text{Tr}_B(|\psi\rangle\langle\psi|)\).
            <ul>
                <li>\(S = 0\) → separable</li>
                <li>\(S = 1\) → maximally entangled (e.g., Bell states)</li>
            </ul>
            </li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications in Quantum Information Processing</h2>
    <div class="content">
        <h3>1. Quantum Teleportation</h3>
        <ul>
            <li><strong>Goal</strong>: Transmit an unknown quantum state using classical communication and shared entanglement.</li>
            <li><strong>Resource</strong>: One shared Bell pair (e.g., \(|\Phi^+\rangle\)) + 2 classical bits.</li>
            <li><strong>Process</strong>:
                <ol>
                    <li>Alice performs Bell measurement on her qubit (state to send) and her half of Bell pair.</li>
                    <li>Sends 2 classical bits to Bob.</li>
                    <li>Bob applies correction based on bits → recovers original state.</li>
                </ol>
            </li>
            <li><strong>Key point</strong>: No cloning; original state is destroyed.</li>
        </ul>
        
        <h3>2. Superdense Coding</h3>
        <ul>
            <li><strong>Goal</strong>: Send <strong>two classical bits</strong> by transmitting <strong>one qubit</strong>.</li>
            <li><strong>Resource</strong>: One shared Bell pair.</li>
            <li><strong>Process</strong>:
                <ul>
                    <li>Alice applies one of four unitary operations to her qubit (encoding 00, 01, 10, 11).</li>
                    <li>Sends her qubit to Bob.</li>
                    <li>Bob performs Bell measurement → decodes 2 bits.</li>
                </ul>
            </li>
            <li><strong>Demonstrates</strong>: Entanglement enhances classical communication capacity.</li>
        </ul>
        
        <h3>3. Quantum Key Distribution (QKD)</h3>
        <ul>
            <li><strong>Example</strong>: E91 protocol (Ekert, 1991)
            <ul>
                <li>Uses entangled photon pairs.</li>
                <li>Security based on <strong>violation of Bell inequalities</strong>—eavesdropping disturbs entanglement and is detectable.</li>
            </ul>
            </li>
        </ul>
        
        <h3>4. Quantum Computing</h3>
        <ul>
            <li><strong>Entangling gates</strong> (e.g., CNOT) create entanglement between qubits.</li>
            <li>Essential for quantum speedup (e.g., in Shor's algorithm, Grover's search).</li>
            <li><strong>Measurement-based quantum computing</strong> (MBQC): Computation driven by measurements on a highly entangled "cluster state".</li>
        </ul>
        
        <h3>5. Quantum Metrology & Sensing</h3>
        <p>Entangled states (e.g., NOON states) enable <strong>Heisenberg-limited precision</strong>, surpassing classical limits in interferometry and imaging.</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Takeaway</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Entanglement</td>
                        <td>Non-separable quantum correlations with no classical analog.</td>
                    </tr>
                    <tr>
                        <td>Bell States</td>
                        <td>Maximally entangled 2-qubit states; foundational for protocols.</td>
                    </tr>
                    <tr>
                        <td>Non-locality</td>
                        <td>Proven via Bell inequality violations; rules out local hidden variables.</td>
                    </tr>
                    <tr>
                        <td>Math Representation</td>
                        <td>Schmidt decomposition (pure), PPT criterion (mixed), entanglement entropy.</td>
                    </tr>
                    <tr>
                        <td>Applications</td>
                        <td>Teleportation, superdense coding, QKD, quantum computing, enhanced sensing.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Remember</strong>: Entanglement is a <strong>resource</strong>, not just a curiosity—it enables tasks impossible classically.
        </div>
    </div>
</section>

        <section class="section">
    <h2 class="section-title">Quantum Gates and Circuits</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing</em></p>
        
        <h3>Single-Qubit Gates</h3>
        <p>Single-qubit gates are <strong>unitary operators</strong> acting on a single qubit. They correspond to rotations on the <strong>Bloch sphere</strong>.</p>
        
        <h4>Pauli Gates</h4>
        <p>These are fundamental and form the Pauli group.</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Gate</th>
                        <th>Matrix</th>
                        <th>Action on Basis States</th>
                        <th>Bloch Sphere Rotation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>X (NOT)</strong></td>
                        <td>\(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\)</td>
                        <td>\(X|0\rangle = |1\rangle\), \(X|1\rangle = |0\rangle\)</td>
                        <td>180° about <strong>x-axis</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Y</strong></td>
                        <td>\(\begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix}\)</td>
                        <td>\(Y|0\rangle = i|1\rangle\), \(Y|1\rangle = -i|0\rangle\)</td>
                        <td>180° about <strong>y-axis</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Z (Phase-flip)</strong></td>
                        <td>\(\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}\)</td>
                        <td>\(Z|0\rangle = |0\rangle\), \(Z|1\rangle = -|1\rangle\)</td>
                        <td>180° about <strong>z-axis</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Note</strong>: \(X, Y, Z\) are <strong>Hermitian</strong> and <strong>unitary</strong>: \(X^\dagger = X\), \(X^2 = I\), etc.
        </div>
        
        <h4>Hadamard Gate (H)</h4>
        <p>Creates superposition from computational basis:</p>
        <div class="equation">
            \[ H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \]
        </div>
        <p>Action:</p>
        <div class="equation">
            \[ H|0\rangle = |+\rangle = \frac{|0\rangle + |1\rangle}{\sqrt{2}}, \quad
            H|1\rangle = |-\rangle = \frac{|0\rangle - |1\rangle}{\sqrt{2}} \]
        </div>
        <ul>
            <li>Also maps \(|+\rangle \to |0\rangle\), \(|-\rangle \to |1\rangle\) → <strong>basis change</strong> between Z and X.</li>
            <li>Bloch sphere: Rotation by 180° about axis \((\hat{x} + \hat{z})/\sqrt{2}\).</li>
        </ul>
        
        <h4>Phase Gate (S) and T Gate</h4>
        <p><strong>Phase (S) gate</strong>:</p>
        <div class="equation">
            \[ S = \begin{pmatrix} 1 & 0 \\ 0 & i \end{pmatrix}, \quad S^2 = Z \]
        </div>
        <p>Adds a <strong>+90° phase</strong> to |1⟩: \(S|1\rangle = i|1\rangle\)</p>
        
        <p><strong>T gate (π/8 gate)</strong>:</p>
        <div class="equation">
            \[ T = \begin{pmatrix} 1 & 0 \\ 0 & e^{i\pi/4} \end{pmatrix}, \quad T^2 = S \]
        </div>
        <p>Adds <strong>+45° phase</strong> to |1⟩.</p>
        
        <div class="definition">
            These are <strong>non-Clifford</strong> gates (T is especially important for universality).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Multi-Qubit Gates</h2>
    <div class="content">
        <p>Multi-qubit gates enable <strong>entanglement</strong> and <strong>conditional operations</strong>.</p>
        
        <h3>CNOT (Controlled-NOT) Gate</h3>
        <ul>
            <li><strong>2-qubit gate</strong>: flips target qubit if control is |1⟩.</li>
            <li>Matrix (in basis |00⟩, |01⟩, |10⟩, |11⟩):</li>
        </ul>
        <div class="equation">
            \[ \text{CNOT} = 
            \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 1 & 0
            \end{pmatrix} \]
        </div>
        <ul>
            <li>Action:</li>
        </ul>
        <div class="equation">
            \[ \text{CNOT}|a,b\rangle = |a, a \oplus b\rangle \]
        </div>
        <ul>
            <li><strong>Creates entanglement</strong>:<br>
            \( \text{CNOT}(H \otimes I)|00\rangle = \text{CNOT}|+\!0\rangle = \frac{|00\rangle + |11\rangle}{\sqrt{2}} = |\Phi^+\rangle \)</li>
        </ul>
        
        <h3>Toffoli Gate (CCNOT)</h3>
        <ul>
            <li><strong>3-qubit gate</strong>: flips target if <strong>both controls</strong> are |1⟩.</li>
            <li>Universal for <strong>classical reversible computing</strong>.</li>
            <li>Can implement AND, OR, etc.</li>
            <li>Quantum universality: With single-qubit gates, Toffoli enables universal quantum computation.</li>
            <li>Not natively available on all hardware; often <strong>decomposed</strong> into CNOTs and T gates.</li>
        </ul>
        
        <h3>SWAP Gate</h3>
        <p>Exchanges states of two qubits:</p>
        <div class="equation">
            \[ \text{SWAP}|a,b\rangle = |b,a\rangle \]
        </div>
        <p>Matrix:</p>
        <div class="equation">
            \[ \text{SWAP} =
            \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1
            \end{pmatrix} \]
        </div>
        <p>Can be built from <strong>3 CNOTs</strong>:</p>
        <div class="equation">
            \[ \text{SWAP} = \text{CNOT}_{12} \cdot \text{CNOT}_{21} \cdot \text{CNOT}_{12} \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Universal Gate Sets</h2>
    <div class="content">
        <p>A set of quantum gates is <strong>universal</strong> if it can approximate <strong>any unitary operation</strong> on \(n\) qubits to arbitrary precision.</p>
        
        <h3>Common Universal Sets</h3>
        <ol>
            <li><strong>{CNOT, H, T}</strong>
            <ul>
                <li>Most widely used in fault-tolerant quantum computing.</li>
                <li>T is non-Clifford; enables approximation of any rotation via <strong>Solovay-Kitaev theorem</strong>.</li>
            </ul>
            </li>
            <li><strong>{CNOT, single-qubit rotations}</strong>
            <ul>
                <li>Any single-qubit unitary can be decomposed into rotations: \(R_x(\theta), R_y(\phi), R_z(\lambda)\).</li>
            </ul>
            </li>
            <li><strong>{Toffoli, Hadamard}</strong>
            <ul>
                <li>Sufficient for universal quantum computation (though less efficient).</li>
            </ul>
            </li>
        </ol>
        
        <div class="definition">
            <strong>Key Insight</strong>: You need <strong>at least one non-Clifford gate</strong> (like T) to achieve universality. The Clifford group (generated by H, S, CNOT) alone is <strong>not universal</strong> (Gottesman-Knill theorem).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Circuit Diagrams and Notation</h2>
    <div class="content">
        <p>Quantum circuits provide a <strong>visual representation</strong> of quantum algorithms.</p>
        
        <h3>Basic Conventions</h3>
        <ul>
            <li><strong>Horizontal lines</strong>: Qubits (top to bottom = qubit 0 to qubit \(n-1\)).</li>
            <li><strong>Time flows left to right</strong>.</li>
            <li><strong>Gates</strong> are boxes or symbols applied to qubit lines.</li>
            <li><strong>Multi-qubit gates</strong> connect control and target with dots and ⊕.</li>
        </ul>
        
        <h3>Examples</h3>
        <ul>
            <li><strong>Hadamard on qubit 0</strong>:
            <pre>q0: ──H──</pre>
            </li>
            <li><strong>CNOT (q0 control, q1 target)</strong>:
            <pre>q0: ──●──
      │
q1: ──⊕──</pre>
            </li>
            <li><strong>Toffoli (q0,q1 control; q2 target)</strong>:
            <pre>q0: ──●──
      │
q1: ──●──
      │
q2: ──⊕──</pre>
            </li>
            <li><strong>Measurement</strong>:
            <pre>q0: ──M──</pre>
            </li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: Classical bits (from measurement) are often shown as double lines.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Circuit Depth and Complexity</h2>
    <div class="content">
        <h3>Circuit Depth</h3>
        <ul>
            <li><strong>Definition</strong>: The <strong>longest path</strong> (in time steps) from input to output, considering gates that can be applied <strong>in parallel</strong>.</li>
            <li><strong>Why it matters</strong>: Depth correlates with <strong>execution time</strong> and <strong>susceptibility to noise</strong> (decoherence).</li>
            <li>Example:
            <pre>q0: ──H──●──────
         │
q1: ─────⊕──H──</pre>
            - Depth = 2 (H and CNOT cannot be parallelized; second H on q1 comes after CNOT).</li>
        </ul>
        
        <h3>Circuit Size</h3>
        <p>Total number of gates (often counts CNOTs separately due to higher error rates).</p>
        
        <h3>Complexity Classes</h3>
        <ul>
            <li><strong>BQP (Bounded-error Quantum Polynomial time)</strong>: Problems solvable by quantum circuits of <strong>polynomial size and depth</strong> with bounded error.</li>
            <li><strong>Quantum advantage</strong> often hinges on <strong>exponentially smaller depth/size</strong> vs. classical counterparts (e.g., Shor's algorithm).</li>
        </ul>
        
        <h3>Optimization Goals</h3>
        <ul>
            <li>Minimize <strong>CNOT count</strong> (most error-prone gate on NISQ devices).</li>
            <li>Reduce <strong>depth</strong> to stay within coherence time.</li>
            <li>Use <strong>gate decomposition</strong> to map to hardware-native gates.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Key Quantum Gates</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Gate</th>
                        <th>Type</th>
                        <th>Matrix / Action</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>X, Y, Z</strong></td>
                        <td>Single-qubit</td>
                        <td>Pauli matrices</td>
                        <td>Bit/phase flips, rotations</td>
                    </tr>
                    <tr>
                        <td><strong>H</strong></td>
                        <td>Single-qubit</td>
                        <td>\(\frac{1}{\sqrt{2}}\begin{bmatrix}1&1\\1&-1\end{bmatrix}\)</td>
                        <td>Superposition, basis change</td>
                    </tr>
                    <tr>
                        <td><strong>S, T</strong></td>
                        <td>Single-qubit</td>
                        <td>Phase rotations</td>
                        <td>Add complex phases; T enables universality</td>
                    </tr>
                    <tr>
                        <td><strong>CNOT</strong></td>
                        <td>2-qubit</td>
                        <td>\(|a,b\rangle \to |a, a\oplus b\rangle\)</td>
                        <td>Entanglement, conditional logic</td>
                    </tr>
                    <tr>
                        <td><strong>Toffoli</strong></td>
                        <td>3-qubit</td>
                        <td>CCNOT</td>
                        <td>Classical logic, universality</td>
                    </tr>
                    <tr>
                        <td><strong>SWAP</strong></td>
                        <td>2-qubit</td>
                        <td>\(|a,b\rangle \to |b,a\rangle\)</td>
                        <td>Qubit exchange</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li>Single-qubit gates manipulate state on the Bloch sphere.</li>
            <li>Multi-qubit gates (especially CNOT) <strong>generate entanglement</strong>.</li>
            <li><strong>{H, T, CNOT}</strong> is a standard universal gate set.</li>
            <li>Circuit <strong>depth</strong> is critical for near-term hardware.</li>
            <li>Quantum circuits are the <strong>"assembly language"</strong> of quantum algorithms.</li>
        </ul>
    </div>
</section>

       <section class="section">
    <h2 class="section-title">Quantum Algorithms and Complexity</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Computing</em></p>
        
        <h3>Grover's Search Algorithm</h3>
        <h4>Problem Statement</h4>
        <p>Given an unstructured database of \(N = 2^n\) items and a <strong>black-box oracle</strong> \(f(x)\) such that:</p>
        <div class="equation">
            \[ f(x) = 
            \begin{cases}
            1 & \text{if } x = x_0 \text{ (target item)} \\
            0 & \text{otherwise}
            \end{cases} \]
        </div>
        <p>Goal: Find \(x_0\) with as few queries to \(f\) as possible.</p>
        
        <h4>Classical vs. Quantum</h4>
        <ul>
            <li><strong>Classical</strong>: Requires \(O(N)\) queries (on average \(N/2\)).</li>
            <li><strong>Grover's algorithm</strong>: Requires only \(O(\sqrt{N})\) queries → <strong>quadratic speedup</strong>.</li>
        </ul>
        
        <h4>Key Principles</h4>
        <ol>
            <li><strong>Amplitude Amplification</strong>:
            <ul>
                <li>Start with uniform superposition:<br>
                \[ |\psi\rangle = H^{\otimes n}|0\rangle^{\otimes n} = \frac{1}{\sqrt{N}} \sum_{x=0}^{N-1} |x\rangle \]
                </li>
                <li>Repeatedly apply the <strong>Grover iteration</strong> \(G = (2|\psi\rangle\langle\psi| - I) \cdot O_f\), where:
                <ul>
                    <li>\(O_f\) = oracle (flips sign of target: \(|x_0\rangle \to -|x_0\rangle\))</li>
                    <li>\(2|\psi\rangle\langle\psi| - I\) = <strong>diffusion operator</strong> (inverts amplitudes about mean)</li>
                </ul>
                </li>
            </ul>
            </li>
            <li><strong>Geometric Interpretation</strong>:
            <ul>
                <li>State vector rotates in 2D plane spanned by \(|x_0\rangle\) and \(|\psi_{\perp}\rangle\) (uniform superposition of non-targets).</li>
                <li>Each Grover iteration rotates by angle \(2\theta\), where \(\sin\theta = 1/\sqrt{N}\).</li>
                <li>Optimal number of iterations: \(R \approx \frac{\pi}{4} \sqrt{N}\)</li>
            </ul>
            </li>
        </ol>
        
        <h4>Applications</h4>
        <ul>
            <li><strong>Unstructured search</strong>: Database search, collision detection.</li>
            <li><strong>Optimization</strong>: Speeds up brute-force search in NP problems (e.g., SAT solvers).</li>
            <li><strong>Amplitude estimation</strong>: Core subroutine in quantum Monte Carlo methods.</li>
            <li><strong>Cryptanalysis</strong>: Reduces effective key length (e.g., AES-128 → ~64-bit security).</li>
        </ul>
        
        <div class="definition">
            <strong>Limitation</strong>: Only provides <strong>quadratic speedup</strong>—not exponential.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Fourier Transform (QFT)</h2>
    <div class="content">
        <h3>Definition</h3>
        <p>The <strong>QFT</strong> is the quantum analog of the classical Discrete Fourier Transform (DFT).</p>
        <p>Maps computational basis state \(|j\rangle\) to superposition:</p>
        <div class="equation">
            \[ \text{QFT}|j\rangle = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} e^{2\pi i j k / N} |k\rangle, \quad N = 2^n \]
        </div>
        
        <h3>Circuit Implementation</h3>
        <ul>
            <li>Efficiently implemented using <strong>Hadamard</strong> and <strong>controlled-phase</strong> gates.</li>
            <li><strong>Circuit depth</strong>: \(O(n^2)\) (can be reduced to \(O(n \log n)\) with approximations).</li>
            <li><strong>Structure</strong>:
            <ul>
                <li>Apply Hadamard to qubit 0.</li>
                <li>Apply controlled-\(R_k\) gates (phase rotations) from higher qubits.</li>
                <li>Repeat for each qubit.</li>
                <li>Reverse qubit order at the end (swap gates).</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Properties</h3>
        <ul>
            <li><strong>Unitary</strong>: QFT\(^{-1}\) = QFT\(^{\dagger}\)</li>
            <li><strong>Exponentially faster</strong> than classical FFT for preparing Fourier state (but <strong>output is not directly readable</strong> due to measurement collapse).</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Core subroutine</strong> in:
            <ul>
                <li><strong>Shor's algorithm</strong> (period finding)</li>
                <li><strong>Phase estimation</strong></li>
                <li><strong>Hidden subgroup problems</strong></li>
                <li><strong>Quantum simulations</strong> (e.g., solving linear systems via HHL algorithm)</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            <strong>Note</strong>: QFT does <strong>not</strong> provide exponential speedup for classical FFT tasks because extracting all Fourier coefficients requires exponential measurements.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Shor's Algorithm for Integer Factorization</h2>
    <div class="content">
        <h3>Problem</h3>
        <p>Given composite integer \(N\), find non-trivial factors.</p>
        <ul>
            <li><strong>Classically</strong>: Best known algorithm (GNFS) runs in <strong>sub-exponential</strong> time → infeasible for large \(N\) (basis of RSA cryptography).</li>
            <li><strong>Shor's algorithm</strong>: Solves in <strong>polynomial time</strong> → breaks RSA.</li>
        </ul>
        
        <h3>Algorithm Overview</h3>
        <ol>
            <li><strong>Classical reduction</strong>:
            <ul>
                <li>Pick random \(a < N\), compute \(\gcd(a, N)\). If >1, done.</li>
                <li>Else, find <strong>order \(r\)</strong> of \(a \mod N\): smallest \(r\) such that \(a^r \equiv 1 \mod N\).</li>
            </ul>
            </li>
            <li><strong>Quantum subroutine: Period Finding</strong>
            <ul>
                <li>Use <strong>quantum phase estimation</strong> or <strong>QFT-based period finding</strong>:
                <ul>
                    <li>Prepare state: \(\frac{1}{\sqrt{Q}} \sum_{x=0}^{Q-1} |x\rangle |a^x \mod N\rangle\)</li>
                    <li>Measure second register → collapses first to periodic superposition with period \(r\).</li>
                    <li>Apply <strong>QFT</strong> to first register → peaks at multiples of \(Q/r\).</li>
                    <li>Classical post-processing (continued fractions) extracts \(r\).</li>
                </ul>
                </li>
            </ul>
            </li>
            <li><strong>Classical post-processing</strong>:
            <ul>
                <li>If \(r\) even and \(a^{r/2} \not\equiv -1 \mod N\), then<br>
                \(\gcd(a^{r/2} \pm 1, N)\) yields non-trivial factors.</li>
            </ul>
            </li>
        </ol>
        
        <h3>Complexity</h3>
        <ul>
            <li><strong>Time</strong>: \(O((\log N)^3)\) → <strong>exponential speedup</strong> over classical.</li>
            <li><strong>Qubits</strong>: \(O(\log N)\)</li>
        </ul>
        
        <h3>Significance</h3>
        <ul>
            <li>First <strong>exponentially faster</strong> quantum algorithm for a practical problem.</li>
            <li>Motivated global investment in <strong>post-quantum cryptography</strong>.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Walks</h2>
    <div class="content">
        <h3>What is a Quantum Walk?</h3>
        <p>Quantum analog of classical random walks.</p>
        <p>Two main types:</p>
        <ol>
            <li><strong>Discrete-time quantum walk (DTQW)</strong></li>
            <li><strong>Continuous-time quantum walk (CTQW)</strong></li>
        </ol>
        
        <h3>Discrete-Time Quantum Walk (DTQW)</h3>
        <ul>
            <li>Requires <strong>coin</strong> and <strong>position</strong> registers.</li>
            <li>Evolution:<br>
            \(|\psi(t+1)\rangle = S \cdot (C \otimes I) |\psi(t)\rangle\)
            <ul>
                <li>\(C\): <strong>Coin operator</strong> (e.g., Hadamard) → creates superposition of directions.</li>
                <li>\(S\): <strong>Shift operator</strong> → moves particle based on coin state.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Properties</h3>
        <ul>
            <li><strong>Quadratic speedup</strong> in hitting time vs. classical walks (e.g., on a line: spreads as \(t\) vs. \(\sqrt{t}\)).</li>
            <li><strong>Interference and entanglement</strong> between coin and position.</li>
            <li><strong>Universal for quantum computation</strong>: Any quantum circuit can be simulated by a quantum walk.</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li><strong>Search algorithms</strong>: Spatial search on graphs (e.g., finding marked node on hypercube).</li>
            <li><strong>Graph isomorphism</strong>, <strong>element distinctness</strong>.</li>
            <li><strong>Quantum simulation</strong> of physical processes (e.g., energy transport).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Complexity Classes</h2>
    <div class="content">
        <h3>BQP (Bounded-error Quantum Polynomial Time)</h3>
        <ul>
            <li><strong>Definition</strong>: Class of decision problems solvable by a quantum computer in <strong>polynomial time</strong>, with error probability ≤ 1/3.</li>
            <li><strong>Formally</strong>:<br>
            \(L \in \text{BQP}\) if ∃ uniform family of quantum circuits \(\{C_n\}\) such that:
            <ul>
                <li>Size of \(C_n\) = poly(n)</li>
                <li>For \(x \in L\): \(\Pr[C_n(x) = 1] \geq 2/3\)</li>
                <li>For \(x \notin L\): \(\Pr[C_n(x) = 0] \geq 2/3\)</li>
            </ul>
            </li>
            <li><strong>Contains</strong>: P, BPP</li>
            <li><strong>Contained in</strong>: PSPACE</li>
            <li><strong>Believed not to contain</strong>: NP-complete problems (but not proven)</li>
            <li><strong>Key problems in BQP</strong>:
            <ul>
                <li>Factoring (Shor)</li>
                <li>Discrete log</li>
                <li>Simulation of quantum systems</li>
            </ul>
            </li>
        </ul>
        
        <h3>QMA (Quantum Merlin-Arthur)</h3>
        <ul>
            <li><strong>Quantum analog of NP</strong>.</li>
            <li><strong>Definition</strong>: A problem is in QMA if, for every "yes" instance, there exists a <strong>polynomial-size quantum witness</strong> \(|\psi\rangle\) that a <strong>polynomial-time quantum verifier</strong> accepts with high probability; for "no" instances, no such witness exists.</li>
            <li><strong>Formally</strong>:<br>
            \(L \in \text{QMA}\) if ∃ quantum verifier \(V\) (poly-time) such that:
            <ul>
                <li>If \(x \in L\): ∃ \(|\psi\rangle\) with \(\Pr[V(x, |\psi\rangle) = 1] \geq 2/3\)</li>
                <li>If \(x \notin L\): ∀ \(|\psi\rangle\), \(\Pr[V(x, |\psi\rangle) = 1] \leq 1/3\)</li>
            </ul>
            </li>
            <li><strong>Complete problem</strong>: <strong>k-local Hamiltonian problem</strong> (quantum analog of SAT).</li>
            <li><strong>Contains</strong>: BQP, NP</li>
            <li><strong>Contained in</strong>: PP</li>
        </ul>
        
        <div class="definition">
            <strong>Analogy</strong>:<br>
            - <strong>NP</strong>: Merlin sends classical proof → Arthur verifies classically.<br>
            - <strong>QMA</strong>: Merlin sends <strong>quantum proof</strong> → Arthur verifies <strong>quantumly</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm/Concept</th>
                        <th>Speedup</th>
                        <th>Key Idea</th>
                        <th>Application</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Grover's Search</strong></td>
                        <td>Quadratic (\(O(\sqrt{N})\))</td>
                        <td>Amplitude amplification</td>
                        <td>Unstructured search, optimization</td>
                    </tr>
                    <tr>
                        <td><strong>QFT</strong></td>
                        <td>Exponential state prep</td>
                        <td>Quantum Fourier basis</td>
                        <td>Shor's, phase estimation</td>
                    </tr>
                    <tr>
                        <td><strong>Shor's Algorithm</strong></td>
                        <td>Exponential</td>
                        <td>Period finding via QFT</td>
                        <td>Integer factorization, RSA break</td>
                    </tr>
                    <tr>
                        <td><strong>Quantum Walks</strong></td>
                        <td>Quadratic (hitting time)</td>
                        <td>Coherent walk with interference</td>
                        <td>Graph search, simulation</td>
                    </tr>
                    <tr>
                        <td><strong>BQP</strong></td>
                        <td>—</td>
                        <td>Efficient quantum decision problems</td>
                        <td>Captures power of quantum computers</td>
                    </tr>
                    <tr>
                        <td><strong>QMA</strong></td>
                        <td>—</td>
                        <td>Quantum proofs + verification</td>
                        <td>Quantum satisfiability, physics</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Grover</strong>: Quadratic speedup for search—broadly applicable but not exponential.</li>
            <li><strong>Shor + QFT</strong>: Exponential speedup for structured problems (factoring, period finding).</li>
            <li><strong>Quantum walks</strong>: Offer alternative algorithmic framework with speedups on graphs.</li>
            <li><strong>BQP ≠ NP</strong>: Quantum computers likely <strong>cannot solve NP-complete problems efficiently</strong>.</li>
            <li><strong>QMA</strong>: Natural class for quantum many-body problems and verification.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum-Inspired Algorithms</h2>
    <div class="content">
        <p><em>Classical Simulation of Quantum Phenomena</em></p>
        
        <h3>Simulating Quantum Phenomena: Overview</h3>
        <h4>What Are Quantum-Inspired Algorithms?</h4>
        <p><strong>Definition</strong>: Classical algorithms that borrow ideas from quantum mechanics (e.g., superposition, interference, entanglement) to solve problems more efficiently—<strong>without requiring a quantum computer</strong>.</p>
        <p><strong>Goal</strong>: Leverage quantum-like structures to gain speedups or new insights on classical hardware.</p>
        <p><strong>Important distinction</strong>:</p>
        <ul>
            <li><strong>Quantum simulation</strong>: Simulating a quantum system <em>on a classical computer</em>.</li>
            <li><strong>Quantum-inspired</strong>: Using quantum concepts to design <em>better classical algorithms</em>.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Note</strong>: True quantum advantage (e.g., exponential speedup) generally <strong>cannot</strong> be replicated classically for arbitrary quantum systems—due to the exponential cost of representing quantum states.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Classical Simulation of Superposition</h2>
    <div class="content">
        <h3>How Superposition Is Represented Classically</h3>
        <p>A single qubit in superposition:</p>
        <div class="equation">
            \[ |\psi\rangle = \alpha|0\rangle + \beta|1\rangle, \quad |\alpha|^2 + |\beta|^2 = 1 \]
        </div>
        <p><strong>Classical representation</strong>: Store complex amplitudes \(\alpha, \beta\) as floating-point numbers → <strong>2 complex numbers</strong> per qubit.</p>
        
        <h3>Scaling Challenge</h3>
        <ul>
            <li>For \(n\) qubits, the state vector has \(2^n\) complex amplitudes.</li>
            <li><strong>Memory requirement</strong>: \(O(2^n)\) → becomes infeasible beyond ~45–50 qubits on supercomputers.</li>
        </ul>
        
        <h3>Simulation Techniques</h3>
        <ol>
            <li><strong>State-vector simulation</strong>:
            <ul>
                <li>Exact simulation using full \(2^n\)-dimensional vector.</li>
                <li>Used in simulators like Qiskit Aer, QuTiP.</li>
            </ul>
            </li>
            <li><strong>Sparse state representation</strong>:
            <ul>
                <li>If the state has few non-zero amplitudes (e.g., after few gates), store only non-zero entries.</li>
            </ul>
            </li>
            <li><strong>Tensor network methods</strong> (see Section 5): Avoid full state vector when entanglement is limited.</li>
        </ol>
        
        <div class="definition">
            ✅ <strong>Feasible for</strong>: Small circuits, educational purposes, verification of quantum hardware.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Simulating Entanglement on Classical Hardware</h2>
    <div class="content">
        <h3>The Core Challenge</h3>
        <ul>
            <li>Entanglement implies <strong>non-separability</strong>: The full \(2^n\)-dimensional state cannot be factored.</li>
            <li>Classical simulation must track <strong>global correlations</strong>, which grow exponentially with qubit count.</li>
        </ul>
        
        <h3>Approaches to Simulate Entanglement</h3>
        <h4>(a) Full State-Vector Simulation</h4>
        <ul>
            <li>Stores entire wavefunction → captures all entanglement exactly.</li>
            <li><strong>Cost</strong>: \(O(2^n)\) memory and \(O(2^n)\) time per gate (for dense gates like CNOT).</li>
        </ul>
        
        <h4>(b) Stabilizer Formalism (Gottesman-Knill Theorem)</h4>
        <ul>
            <li>Efficiently simulates circuits using only <strong>Clifford gates</strong> (H, S, CNOT).</li>
            <li>Represents state via <strong>stabilizer generators</strong> (not amplitudes).</li>
            <li><strong>Complexity</strong>: \(O(n^2)\) per gate for \(n\) qubits.</li>
            <li><strong>Limitation</strong>: Cannot simulate non-Clifford gates (e.g., T gate) efficiently → not universal.</li>
        </ul>
        
        <h4>(c) Matrix Product States (MPS) / Tensor Networks</h4>
        <p>Represents state as a chain of tensors:</p>
        <div class="equation">
            \[ |\psi\rangle = \sum_{i_1,\dots,i_n} A^{[1]}_{i_1} A^{[2]}_{i_2} \cdots A^{[n]}_{i_n} |i_1 i_2 \dots i_n\rangle \]
        </div>
        <ul>
            <li><strong>Efficient when entanglement is low</strong> (e.g., 1D systems with area law).</li>
            <li><strong>Bond dimension \(\chi\)</strong> controls accuracy and cost: memory = \(O(n \chi^2)\).</li>
            <li>Used in DMRG (Density Matrix Renormalization Group) for condensed matter physics.</li>
        </ul>
        
        <div class="definition">
            🔑 <strong>Key Insight</strong>: Entanglement entropy limits classical simulability. High entanglement → classical simulation becomes intractable.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quantum Interference Simulation Techniques</h2>
    <div class="content">
        <h3>What Is Quantum Interference?</h3>
        <p>Amplitudes (complex numbers) can <strong>constructively or destructively interfere</strong>.</p>
        <p>Critical for quantum speedups (e.g., in Grover's or Shor's algorithms).</p>
        
        <h3>Classical Simulation Methods</h3>
        <ol>
            <li><strong>Path Integral / Sum-over-Paths</strong>:
            <ul>
                <li>Compute final amplitude as sum over all computational paths.</li>
                <li>Each path contributes a complex phase.</li>
                <li><strong>Problem</strong>: \(2^{\text{#gates}}\) paths → exponential in circuit depth.</li>
            </ul>
            </li>
            <li><strong>Feynman's Path Simulator</strong>:
            <ul>
                <li>For a circuit with \(m\) gates, simulate by summing over all possible intermediate measurement outcomes.</li>
                <li>Time complexity: \(O(4^m)\) → only feasible for shallow circuits.</li>
            </ul>
            </li>
            <li><strong>Monte Carlo with Sign Problem</strong>:
            <ul>
                <li>Sample paths probabilistically.</li>
                <li><strong>Obstacle</strong>: Negative/complex weights → <strong>sign problem</strong> → high variance, inefficient.</li>
            </ul>
            </li>
            <li><strong>Stabilizer Rank Decomposition</strong> (for non-Clifford circuits):
            <ul>
                <li>Approximate a state as sum of \(R\) stabilizer states:<br>
                \[ |\psi\rangle \approx \sum_{k=1}^R c_k |\phi_k\rangle \]
                </li>
                <li>Simulate each \(|\phi_k\rangle\) efficiently (via Gottesman-Knill), then combine.</li>
                <li>Cost scales with <strong>stabilizer rank \(R\)</strong> (e.g., \(R = 2^t\) for \(t\) T-gates).</li>
            </ul>
            </li>
        </ol>
        
        <div class="definition">
            🌊 <strong>Interference is hard to simulate classically</strong> because it requires tracking <strong>global phase relationships</strong> across exponentially many states.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Approximation Methods for Quantum States</h2>
    <div class="content">
        <p>Exact simulation is often impossible → use <strong>approximations</strong>.</p>
        
        <h3>(a) Tensor Networks</h3>
        <ul>
            <li><strong>MPS (1D)</strong>, <strong>PEPS (2D)</strong>, <strong>MERA</strong>: Represent states with limited entanglement.</li>
            <li><strong>Accuracy controlled by bond dimension</strong>.</li>
            <li>Widely used in quantum chemistry and condensed matter.</li>
        </ul>
        
        <h3>(b) Variational Methods</h3>
        <ul>
            <li><strong>Quantum-inspired classical ansatz</strong>: Use parameterized classical models (e.g., neural networks) to approximate quantum states.
            <ul>
                <li>Example: <strong>Restricted Boltzmann Machines (RBMs)</strong> to represent wavefunctions.</li>
            </ul>
            </li>
            <li>Train via energy minimization (e.g., for ground states).</li>
        </ul>
        
        <h3>(c) Low-Rank Approximations</h3>
        <ul>
            <li>Assume density matrix \(\rho\) has low rank → store via SVD.</li>
            <li>Useful for <strong>mixed states</strong> with limited entanglement.</li>
        </ul>
        
        <h3>(d) Clifford+T Approximation</h3>
        <ul>
            <li>Decompose arbitrary gates into Clifford + T.</li>
            <li>Simulate using <strong>stabilizer decomposition</strong> (as above).</li>
            <li>Error controlled by number of T gates.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Computational Trade-offs in Simulation</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Memory</th>
                        <th>Time</th>
                        <th>Accuracy</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>State-vector</strong></td>
                        <td>\(O(2^n)\)</td>
                        <td>\(O(2^n)\)/gate</td>
                        <td>Exact</td>
                        <td>Small \(n\) (< 45 qubits)</td>
                    </tr>
                    <tr>
                        <td><strong>Stabilizer (Clifford)</strong></td>
                        <td>\(O(n^2)\)</td>
                        <td>\(O(n^2)\)/gate</td>
                        <td>Exact (Clifford only)</td>
                        <td>Clifford circuits</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Networks (MPS)</strong></td>
                        <td>\(O(n \chi^2)\)</td>
                        <td>\(O(n \chi^3)\)</td>
                        <td>Approximate</td>
                        <td>Low-entanglement 1D systems</td>
                    </tr>
                    <tr>
                        <td><strong>Path Integral</strong></td>
                        <td>\(O(1)\)</td>
                        <td>\(O(4^m)\)</td>
                        <td>Exact</td>
                        <td>Shallow circuits (small \(m\))</td>
                    </tr>
                    <tr>
                        <td><strong>Stabilizer Rank</strong></td>
                        <td>\(O(R n^2)\)</td>
                        <td>\(O(R n^2)\)</td>
                        <td>Approximate</td>
                        <td>Circuits with few T gates</td>
                    </tr>
                    <tr>
                        <td><strong>Neural Quantum States</strong></td>
                        <td>\(O(\text{params})\)</td>
                        <td>Training cost</td>
                        <td>Approximate</td>
                        <td>Ground states, optimization</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Trade-offs</h3>
        <ul>
            <li><strong>Accuracy vs. Scalability</strong>: Exact methods don't scale; approximations introduce error.</li>
            <li><strong>Entanglement vs. Efficiency</strong>: High entanglement → tensor networks fail; state-vector needed.</li>
            <li><strong>Circuit Depth vs. Path Methods</strong>: Deep circuits → path integral infeasible.</li>
            <li><strong>Hardware Limits</strong>: Even with compression, simulating 50+ qubits with high entanglement requires exascale computing.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Rule of Thumb</strong>:<br>
            If a quantum circuit can be simulated efficiently classically, it likely <strong>does not provide quantum advantage</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary</h2>
    <div class="content">
        <ul>
            <li><strong>Superposition</strong> and <strong>interference</strong> require tracking complex amplitudes → exponential classical cost.</li>
            <li><strong>Entanglement</strong> is the main barrier to efficient classical simulation.</li>
            <li><strong>Special cases</strong> (Clifford circuits, low entanglement) can be simulated efficiently.</li>
            <li><strong>Approximation methods</strong> (tensor networks, stabilizer decompositions) enable simulation of larger systems at the cost of accuracy.</li>
            <li><strong>Quantum-inspired algorithms</strong> may offer practical speedups for specific problems (e.g., linear algebra, optimization), but <strong>do not replicate exponential quantum advantage</strong>.</li>
        </ul>
        
        <div class="definition">
            🌐 <strong>Reality Check</strong>: Classical simulation is crucial for <strong>verifying quantum hardware</strong>, <strong>developing algorithms</strong>, and <strong>exploring quantum-classical boundaries</strong>—but it cannot replace scalable, fault-tolerant quantum computers for truly hard problems.
        </div>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Tensor Networks</h2>
    <div class="content">
        <p><em>Study Notes for Quantum Simulation and High-Dimensional Data Representation</em></p>
        
        <h3>Tensor Algebra Fundamentals</h3>
        <h4>What is a Tensor?</h4>
        <p>A <strong>tensor</strong> is a multi-dimensional array of numbers that generalizes scalars (0D), vectors (1D), and matrices (2D).</p>
        <ul>
            <li><strong>Order (or rank)</strong>: Number of indices (e.g., a 3D tensor \(T_{ijk}\) has order 3).</li>
            <li><strong>Dimensions (modes)</strong>: Size along each index (e.g., \(T \in \mathbb{C}^{d_1 \times d_2 \times \cdots \times d_n}\)).</li>
        </ul>
        
        <h4>Key Operations</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Operation</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Tensor product</strong> (\(\otimes\))</td>
                        <td>Combines tensors: \((A \otimes B)_{i,j,k,l} = A_{i,j} B_{k,l}\)</td>
                    </tr>
                    <tr>
                        <td><strong>Contraction</strong></td>
                        <td>Sum over shared indices (generalizes matrix multiplication).<br>Example: \(C_{ik} = \sum_j A_{ij} B_{jk}\)</td>
                    </tr>
                    <tr>
                        <td><strong>Reshaping</strong></td>
                        <td>Reinterpret tensor as different shape (e.g., vector ↔ matrix) without changing data.</td>
                    </tr>
                    <tr>
                        <td><strong>Trace</strong></td>
                        <td>Contraction of a tensor with itself over one or more pairs of indices.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h4>Einstein Notation (Implicit Summation)</h4>
        <p>Repeated indices are summed over:<br>
        \(C_{ik} = A_{ij} B_{jk}\) means \(\sum_j A_{ij} B_{jk}\).</p>
        <p><strong>Crucial for tensor network diagrams</strong>.</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Tensor Decomposition Methods</h2>
    <div class="content">
        <p>Decomposing high-order tensors into simpler components reduces storage and enables efficient computation.</p>
        
        <h3>(a) CANDECOMP/PARAFAC (CP) Decomposition</h3>
        <p>Expresses a tensor as a <strong>sum of rank-1 tensors</strong>:</p>
        <div class="equation">
            \[ \mathcal{T} \approx \sum_{r=1}^R \lambda_r \, a^{(1)}_r \circ a^{(2)}_r \circ \cdots \circ a^{(N)}_r \]
        </div>
        <p>where \(\circ\) denotes outer product.</p>
        <ul>
            <li><strong>Parameters</strong>: \(R \cdot \sum_{n=1}^N d_n\) (vs. \(\prod d_n\) for full tensor).</li>
            <li><strong>Pros</strong>: Compact, interpretable.</li>
            <li><strong>Cons</strong>: Rank \(R\) hard to choose; decomposition not always unique; ill-conditioned.</li>
        </ul>
        
        <div class="definition">
            📌 Used in chemometrics, signal processing.
        </div>
        
        <h3>(b) Tucker Decomposition</h3>
        <p>Generalization of SVD to higher orders:</p>
        <div class="equation">
            \[ \mathcal{T} \approx \mathcal{G} \times_1 A^{(1)} \times_2 A^{(2)} \cdots \times_N A^{(N)} \]
        </div>
        <ul>
            <li>\(\mathcal{G}\): <strong>Core tensor</strong> (size \(r_1 \times r_2 \times \cdots \times r_N\))</li>
            <li>\(A^{(n)}\): Factor matrices (\(d_n \times r_n\))</li>
        </ul>
        <ul>
            <li><strong>Storage</strong>: \(\prod r_n + \sum d_n r_n\)</li>
            <li><strong>Pros</strong>: Flexible; captures multi-linear structure.</li>
            <li><strong>Cons</strong>: Core tensor still exponential in \(N\) if all \(r_n\) large → <strong>curse of dimensionality</strong>.</li>
        </ul>
        
        <div class="definition">
            📌 Basis for <strong>Higher-Order SVD (HOSVD)</strong>.
        </div>
        
        <h3>(c) Tensor Train (TT) / Matrix Product State (MPS)</h3>
        <p><strong>Decomposes tensor into a chain of 3D cores</strong>:</p>
        <div class="equation">
            \[ \mathcal{T}_{i_1 i_2 \dots i_N} = \sum_{\alpha_1,\dots,\alpha_{N-1}} 
            G^{(1)}_{i_1,\alpha_1} 
            G^{(2)}_{\alpha_1,i_2,\alpha_2} 
            \cdots 
            G^{(N)}_{\alpha_{N-1},i_N} \]
        </div>
        <ul>
            <li><strong>Bond dimensions</strong> \(\{\alpha_k\}\) control approximation rank.</li>
            <li><strong>Storage</strong>: \(O(N d r^2)\) for uniform physical dimension \(d\) and bond dimension \(r\).</li>
            <li><strong>Avoids exponential scaling</strong> if \(r\) is small (e.g., for weakly entangled states).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Key insight</strong>: TT/MPS is <strong>exact</strong> for any tensor (with \(r\) up to \(d^{N/2}\)), but <strong>efficient</strong> only when \(r \ll d^{N/2}\).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Matrix Product States (MPS) Representation</h2>
    <div class="content">
        <h3>Origin</h3>
        <p>MPS is the <strong>tensor network form of a quantum many-body wavefunction</strong>:</p>
        <div class="equation">
            \[ |\psi\rangle = \sum_{s_1,\dots,s_N} c_{s_1 \dots s_N} |s_1 s_2 \dots s_N\rangle \]
        </div>
        <p>where \(c_{s_1 \dots s_N}\) is represented as a <strong>Tensor Train</strong>.</p>
        
        <h3>MPS Structure</h3>
        <p>Each site \(k\) has a <strong>core tensor</strong> \(A^{[k]}_{s_k}\) of shape \((\chi_{k-1}, d, \chi_k)\), where:</p>
        <ul>
            <li>\(d\): local Hilbert space dimension (e.g., \(d=2\) for qubits)</li>
            <li>\(\chi_k\): <strong>bond dimension</strong> (entanglement capacity)</li>
        </ul>
        <p><strong>Boundary conditions</strong>: \(\chi_0 = \chi_N = 1\) (open boundary)</p>
        
        <h3>Gauge Freedom</h3>
        <ul>
            <li>MPS is <strong>not unique</strong>: Can insert \(X X^{-1} = I\) between cores.</li>
            <li>Common gauges:
            <ul>
                <li><strong>Left-canonical</strong>: \(\sum_{s_k} (A^{[k]}_{s_k})^\dagger A^{[k]}_{s_k} = I\)</li>
                <li><strong>Right-canonical</strong>: \(\sum_{s_k} A^{[k]}_{s_k} (A^{[k]}_{s_k})^\dagger = I\)</li>
            </ul>
            </li>
            <li>Enables efficient local operations (e.g., in DMRG).</li>
        </ul>
        
        <h3>Entanglement and Bond Dimension</h3>
        <ul>
            <li>For a bipartition at bond \(k\), <strong>entanglement entropy</strong> \(S \leq \log_2 \chi_k\).</li>
            <li><strong>Area law</strong>: Ground states of 1D gapped Hamiltonians have <strong>constant</strong> \(\chi\) → MPS is efficient.</li>
        </ul>
        
        <div class="definition">
            🌟 MPS is the foundation of <strong>Density Matrix Renormalization Group (DMRG)</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Tensor Network Diagrams and Notation</h2>
    <div class="content">
        <h3>Diagrammatic Rules</h3>
        <ul>
            <li><strong>Node</strong>: Tensor (shape indicates order).</li>
            <li><strong>Edge</strong>: Index (labeled or unlabeled).</li>
            <li><strong>Connected edges</strong>: Contracted (summed over).</li>
            <li><strong>Open edges</strong>: Free indices (output of network).</li>
        </ul>
        
        <h3>Examples</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Object</th>
                        <th>Diagram</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Vector \(v_i\)</td>
                        <td>○——</td>
                    </tr>
                    <tr>
                        <td>Matrix \(M_{ij}\)</td>
                        <td>○——○</td>
                    </tr>
                    <tr>
                        <td>Order-3 tensor \(T_{ijk}\)</td>
                        <td>○ with 3 legs</td>
                    </tr>
                    <tr>
                        <td>Matrix multiplication \(C = AB\)</td>
                        <td>○——○——○ ⇒ ○————○</td>
                    </tr>
                    <tr>
                        <td>MPS (4 sites)</td>
                        <td>○—○—○—○ (each node has one physical leg down, bonds horizontal)</td>
                    </tr>
                    <tr>
                        <td>Contraction of two tensors</td>
                        <td>Two nodes connected by an edge</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Advantages of Diagrams</h3>
        <ul>
            <li>Visualize complex contractions.</li>
            <li>Reveal computational structure (e.g., sequential vs. parallel).</li>
            <li>Simplify derivation of algorithms (e.g., DMRG, TEBD).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Contraction Algorithms and Complexity</h2>
    <div class="content">
        <h3>What is Contraction?</h3>
        <p>Evaluate a tensor network by summing over all internal (bond) indices.</p>
        <p>Goal: Compute scalar (e.g., \(\langle \psi | \phi \rangle\)) or reduced tensor (e.g., reduced density matrix).</p>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Order matters</strong>: Different contraction sequences have vastly different costs.</li>
            <li><strong>Optimal contraction is NP-hard</strong> (equivalent to graph partitioning).</li>
        </ul>
        
        <h3>Contraction Strategies</h3>
        <h4>(a) Sequential (Naive) Contraction</h4>
        <ul>
            <li>Contract one pair at a time.</li>
            <li><strong>Cost</strong>: Can be exponential if done poorly.</li>
        </ul>
        
        <h4>(b) Optimal Tree Contraction (for Trees)</h4>
        <ul>
            <li>For <strong>tree-structured networks</strong> (no cycles), optimal order is linear in size.</li>
            <li>Use dynamic programming to find min-cost order.</li>
        </ul>
        
        <h4>(c) Heuristic Methods (for General Networks)</h4>
        <ul>
            <li><strong>Greedy</strong>: At each step, contract pair minimizing cost (e.g., `opt_einsum` in Python).</li>
            <li><strong>Graph-based</strong>: Model as <strong>line graph</strong>; use graph partitioning (e.g., METIS).</li>
            <li><strong>Tree decomposition</strong>: For networks with small <strong>treewidth</strong>.</li>
        </ul>
        
        <h3>Complexity Example: MPS Overlap</h3>
        <p>Compute \(\langle \psi | \phi \rangle\) for two MPS with bond dimensions \(\chi_\psi, \chi_\phi\).</p>
        <p>Contract from left to right:</p>
        <ul>
            <li>Each step: matrix multiplication of size \(\chi \times \chi\)</li>
            <li><strong>Total cost</strong>: \(O(N d \chi^3)\)</li>
        </ul>
        
        <h3>Memory vs. Time Trade-off</h3>
        <p><strong>Intermediate tensors</strong> can be large.</p>
        <p><strong>Slicing (index splitting)</strong>: Fix some indices to reduce memory at cost of repeated contractions.</p>
        
        <div class="definition">
            💡 <strong>Rule of thumb</strong>:<br>
            Contraction cost ≈ \(O(\chi^k)\), where \(k\) = number of bonds meeting at a vertex in the contraction tree.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Idea</th>
                        <th>Complexity/Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CP Decomposition</strong></td>
                        <td>Sum of rank-1 tensors</td>
                        <td>Compact but unstable</td>
                    </tr>
                    <tr>
                        <td><strong>Tucker</strong></td>
                        <td>Core + factor matrices</td>
                        <td>Flexible; core still large</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Train (MPS)</strong></td>
                        <td>Chain of cores; bond dimensions control entanglement</td>
                        <td>\(O(N d r^2)\); ideal for 1D quantum states</td>
                    </tr>
                    <tr>
                        <td><strong>MPS Gauge</strong></td>
                        <td>Left/right canonical forms enable local updates</td>
                        <td>Essential for DMRG</td>
                    </tr>
                    <tr>
                        <td><strong>Tensor Diagrams</strong></td>
                        <td>Visual language for indices and contractions</td>
                        <td>Intuitive algorithm design</td>
                    </tr>
                    <tr>
                        <td><strong>Contraction</strong></td>
                        <td>Order drastically affects cost; optimal is NP-hard</td>
                        <td>Heuristics used in practice (e.g., `opt_einsum`)</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li>Tensor networks <strong>tame the curse of dimensionality</strong> by exploiting <strong>structure</strong> (e.g., limited entanglement).</li>
            <li><strong>MPS</strong> is the workhorse for 1D quantum systems and underlies <strong>DMRG</strong>.</li>
            <li><strong>Contraction order</strong> is critical—small networks can be evaluated exactly; large ones require smart heuristics.</li>
            <li>Diagrams are not just illustrative—they are <strong>computational tools</strong>.</li>
            <li>Applications span <strong>quantum physics</strong>, <strong>machine learning</strong> (e.g., tensor completion), and <strong>data compression</strong>.</li>
        </ul>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Quantum-Inspired Search</h2>
    <div class="content">
        <p><em>Classical Approximations and Applications of Grover-like Techniques</em></p>
        
        <h3>Classical Approximations of Grover's Algorithm</h3>
        <h4>Can Grover's Algorithm Be Simulated Classically?</h4>
        <p><strong>Yes—but not efficiently</strong> for large unstructured search spaces.</p>
        <p>The <strong>quadratic speedup</strong> (\(O(\sqrt{N})\) vs. \(O(N)\)) arises from <strong>quantum parallelism + interference</strong>, which classical systems cannot replicate <em>exactly</em> without exponential overhead.</p>
        
        <h4>Why Exact Simulation Fails at Scale</h4>
        <ul>
            <li>To simulate Grover's state evolution, a classical computer must store and update a <strong>superposition of \(N = 2^n\) amplitudes</strong>.</li>
            <li><strong>Memory and time cost</strong>: \(O(N)\)—same as brute-force search.</li>
            <li>Thus, <strong>no asymptotic speedup</strong> from exact simulation.</li>
        </ul>
        
        <h4>Approximate Classical Emulations</h4>
        <p>Instead of simulating the full quantum state, classical algorithms borrow <strong>structural ideas</strong> from Grover:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Idea</th>
                        <th>Limitation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random sampling with boosting</strong></td>
                        <td>Repeatedly sample and "boost" likelihood of promising candidates</td>
                        <td>No guaranteed speedup; heuristic</td>
                    </tr>
                    <tr>
                        <td><strong>Amplitude-inspired weighting</strong></td>
                        <td>Assign weights to items; iteratively increase weight of marked items</td>
                        <td>Requires oracle feedback per iteration</td>
                    </tr>
                    <tr>
                        <td><strong>Monte Carlo with bias</strong></td>
                        <td>Bias random walks toward marked elements using oracle hints</td>
                        <td>Still \(O(N)\) in worst case</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Key insight</strong>: Classical systems can <strong>mimic the iterative refinement</strong> of Grover, but <strong>without interference</strong>, they cannot achieve true \(O(\sqrt{N})\) scaling.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Amplitude Amplification Techniques (Classical Analogues)</h2>
    <div class="content">
        <h3>What is Amplitude Amplification? (Quantum)</h3>
        <p>Generalization of Grover's algorithm.</p>
        <p>Given a subspace of "good" states, it <strong>rotates</strong> the state vector toward that subspace using:</p>
        <ul>
            <li>An <strong>oracle</strong> \(O\) that marks good states.</li>
            <li>A <strong>diffusion operator</strong> \(D = 2|\psi\rangle\langle\psi| - I\) that inverts amplitudes about the mean.</li>
        </ul>
        
        <h3>Classical Analogues</h3>
        <p>While true amplitude amplification is quantum, classical algorithms use <strong>similar iterative boosting</strong>:</p>
        
        <h4>(a) Multiplicative Weight Updates (MWU)</h4>
        <ul>
            <li>Maintain a probability distribution over items.</li>
            <li>After querying the oracle, <strong>increase weight</strong> of marked items, <strong>decrease</strong> others.</li>
            <li>Normalize to form new distribution.</li>
            <li>After \(O(\sqrt{N})\) rounds, marked item dominates <strong>in expectation</strong>—but <strong>not guaranteed</strong>.</li>
        </ul>
        
        <h4>(b) Boosting in Machine Learning</h4>
        <ul>
            <li>Algorithms like <strong>AdaBoost</strong> iteratively adjust weights of misclassified samples.</li>
            <li><strong>Analogy</strong>: Oracle = weak learner; amplification = focusing on hard examples.</li>
            <li>Not a search algorithm per se, but shares the <strong>iterative refinement</strong> philosophy.</li>
        </ul>
        
        <h4>(c) Stochastic Search with Feedback</h4>
        <ul>
            <li>Use oracle to guide a <strong>biased random search</strong>:
            <ul>
                <li>Start with uniform distribution.</li>
                <li>After each query, update probabilities using Bayesian inference or reinforcement learning.</li>
            </ul>
            </li>
            <li>Can reduce expected queries—but worst-case remains \(O(N)\).</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Critical difference</strong>: Quantum amplitude amplification uses <strong>coherent interference</strong> to <em>cancel</em> bad paths; classical methods only <em>reweight</em>—no destructive interference.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Oracle Design for Classical Search</h2>
    <div class="content">
        <h3>What is an Oracle?</h3>
        <p>A <strong>black-box function</strong> \(f: \{0,1\}^n \to \{0,1\}\) such that \(f(x) = 1\) iff \(x\) is a solution.</p>
        <p>In quantum algorithms, the oracle is applied <strong>coherently</strong> to superpositions.</p>
        
        <h3>Classical Oracle Usage</h3>
        <p>In classical search, the oracle is simply a <strong>predicate function</strong> called on individual inputs.</p>
        <p><strong>Design considerations</strong>:</p>
        <ul>
            <li><strong>Cost per query</strong>: Should be \(O(1)\) or low for speedup to matter.</li>
            <li><strong>Side information</strong>: Can the oracle return more than yes/no? (e.g., gradient, distance)</li>
            <li><strong>Parallelizability</strong>: Can multiple queries be made simultaneously?</li>
        </ul>
        
        <h3>Quantum-Inspired Oracle Strategies</h3>
        <ol>
            <li><strong>Batch querying</strong>: Evaluate oracle on a <strong>subset</strong> of candidates in parallel (e.g., GPU).</li>
            <li><strong>Probabilistic oracles</strong>: Return approximate answers (e.g., in noisy settings).</li>
            <li><strong>Structured oracles</strong>: Exploit partial structure (e.g., locality, smoothness) to guide search—blurring line between structured and unstructured search.</li>
        </ol>
        
        <div class="definition">
            🔍 <strong>Note</strong>: True unstructured search assumes <strong>no side information</strong>—only yes/no answers. Most real-world problems have <em>some</em> structure, enabling better-than-Grover classical heuristics.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Quadratic Speedup Approximations</h2>
    <div class="content">
        <h3>Can Classical Algorithms Achieve \(O(\sqrt{N})\) Queries?</h3>
        <p><strong>No—not in the worst case for unstructured search</strong> (proven by Bennett et al., 1997).</p>
        <p><strong>Lower bound</strong>: Any classical randomized algorithm requires \(\Omega(N)\) queries to find a unique marked item with constant success probability.</p>
        
        <h3>When Can We *Approximate* Quadratic Speedup?</h3>
        <p>Only under <strong>relaxed assumptions</strong>:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Approximate Speedup</th>
                        <th>Mechanism</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Multiple solutions</strong> (\(M\) marked items)</td>
                        <td>\(O(N/M)\) classically vs. \(O(\sqrt{N/M})\) quantum</td>
                        <td>Classical: random sampling finds solution in \(O(N/M)\)</td>
                    </tr>
                    <tr>
                        <td><strong>Approximate search</strong></td>
                        <td>Find <em>near</em>-solution faster</td>
                        <td>Use locality, embeddings, or hashing (e.g., LSH)</td>
                    </tr>
                    <tr>
                        <td><strong>Parallel classical queries</strong></td>
                        <td>\(O(\sqrt{N})\) time with \(O(\sqrt{N})\) processors</td>
                        <td>Not query-efficient; trades time for hardware</td>
                    </tr>
                    <tr>
                        <td><strong>Probabilistic success</strong></td>
                        <td>High probability in fewer than \(N\) steps</td>
                        <td>Still \(\Omega(N)\) expected queries</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Quantum-Inspired Heuristics with Empirical Speedup</h3>
        <ul>
            <li><strong>Grover-inspired local search</strong>: Use amplitude-like weighting in combinatorial optimization.</li>
            <li><strong>Quantum walk-inspired search</strong>: Classical random walks with memory or restarts mimic hitting-time improvements.</li>
            <li><strong>Amplitude estimation via Monte Carlo</strong>: Estimate \(M/N\) (fraction of solutions) using classical sampling—used in finance, but with \(O(1/\epsilon^2)\) vs. quantum \(O(1/\epsilon)\).</li>
        </ul>
        
        <div class="definition">
            📉 <strong>Bottom line</strong>: Classical algorithms <strong>cannot match</strong> Grover's asymptotic query complexity for unstructured search—but can <strong>approach it</strong> in practice for structured or noisy variants.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications to Unstructured Search Problems</h2>
    <div class="content">
        <p>While true unstructured search is rare, many real-world problems <strong>approximate</strong> it:</p>
        
        <h3>(a) Cryptanalysis</h3>
        <ul>
            <li><strong>Brute-force key search</strong>: Find key \(k\) such that \(\text{Decrypt}_k(c) = m\).
            <ul>
                <li>Classical: \(O(2^n)\)</li>
                <li>Grover: \(O(2^{n/2})\) → reduces AES-128 security to ~64 bits.</li>
                <li><strong>Classical approximation</strong>: Use rainbow tables, side-channel info, or partial key guesses—<strong>not unstructured</strong>.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(b) Database Search</h3>
        <ul>
            <li><strong>Idealized</strong>: Find record with property \(P\).
            <ul>
                <li>Quantum: \(O(\sqrt{N})\) with quantum RAM (QRAM)—<strong>not yet feasible</strong>.</li>
                <li>Classical: Indexing (e.g., B-trees) makes it <strong>structured</strong> → \(O(\log N)\).</li>
                <li><strong>Unindexed databases</strong>: Linear scan = \(O(N)\); no classical quadratic speedup.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(c) NP Problem Solving</h3>
        <ul>
            <li><strong>SAT, Graph Coloring, etc.</strong>: Search space is unstructured in worst case.
            <ul>
                <li>Quantum: Grover gives \(O(\sqrt{2^n}) = O(1.414^n)\) vs. classical \(O(2^n)\).</li>
                <li>Classical heuristics (DPLL, CDCL): Exploit <strong>problem structure</strong> → often much faster than \(O(2^n)\), but <strong>exponential in worst case</strong>.</li>
                <li><strong>Quantum-inspired SAT solvers</strong>: Use amplitude-like scoring to prioritize variable assignments—modest empirical gains.</li>
            </ul>
            </li>
        </ul>
        
        <h3>(d) Machine Learning & Optimization</h3>
        <ul>
            <li><strong>Finding optimal parameters</strong> in high-dimensional space.
            <ul>
                <li>Quantum: Amplitude amplification for minimization.</li>
                <li>Classical: <strong>Simulated annealing</strong>, <strong>genetic algorithms</strong>, <strong>Bayesian optimization</strong>—use feedback to guide search.</li>
                <li><strong>Quantum-inspired</strong>: Use Grover-like iteration in <strong>combinatorial optimization</strong> (e.g., MaxSAT), but no proven speedup.</li>
            </ul>
            </li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Quantum (Grover)</th>
                        <th>Classical Approximation</th>
                        <th>Achievable Speedup?</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Query complexity</strong></td>
                        <td>\(O(\sqrt{N})\)</td>
                        <td>\(\Omega(N)\) (worst-case lower bound)</td>
                        <td>❌ No</td>
                    </tr>
                    <tr>
                        <td><strong>Amplitude amplification</strong></td>
                        <td>Coherent interference</td>
                        <td>Weighted sampling / boosting</td>
                        <td>❌ (No interference)</td>
                    </tr>
                    <tr>
                        <td><strong>Oracle usage</strong></td>
                        <td>Applied to superposition</td>
                        <td>Applied to single inputs</td>
                        <td>—</td>
                    </tr>
                    <tr>
                        <td><strong>Multiple solutions (\(M\))</strong></td>
                        <td>\(O(\sqrt{N/M})\)</td>
                        <td>\(O(N/M)\) (random sampling)</td>
                        <td>✅ Quadratic gap remains</td>
                    </tr>
                    <tr>
                        <td><strong>Real-world applicability</strong></td>
                        <td>Limited by QRAM, noise</td>
                        <td>Widely used heuristics</td>
                        <td>✅ Practical gains</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>True quadratic speedup is uniquely quantum</strong> for unstructured search—<strong>not replicable classically</strong>.</li>
            <li><strong>Quantum-inspired classical algorithms</strong> borrow <em>ideas</em> (iterative boosting, weighting) but <strong>lack interference</strong>.</li>
            <li><strong>Oracle design</strong> and <strong>problem structure</strong> dominate real-world performance—pure unstructured search is rare.</li>
            <li><strong>Applications</strong> benefit more from <strong>hybrid approaches</strong> (e.g., classical pre-processing + quantum search) than pure emulation.</li>
            <li><strong>No free lunch</strong>: Classical methods can't beat information-theoretic lower bounds—but can exploit structure Grover ignores.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Final Thought</strong>: Quantum-inspired search is valuable for <strong>algorithm design intuition</strong> and <strong>heuristic development</strong>, but it does <strong>not challenge the fundamental quantum advantage</strong> of Grover's algorithm in its native setting.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Machine Learning Fundamentals</h2>
    <div class="content">
        <h3>Supervised Learning</h3>
        <div class="definition">
            <strong>Definition</strong>: Learning a mapping from input features \( \mathbf{x} \in \mathbb{R}^d \) to output labels \( y \) using a <strong>labeled training dataset</strong> \( \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^n \).
        </div>
        
        <p>Two main types:</p>
        <ul>
            <li><strong>Classification</strong>: \( y \) is <strong>categorical</strong> (e.g., spam/not spam).</li>
            <li><strong>Regression</strong>: \( y \) is <strong>continuous</strong> (e.g., house price).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Classification and Regression Principles</h2>
    <div class="content">
        <h3>Goal</h3>
        <ul>
            <li>Learn a <strong>hypothesis function</strong> \( h_\theta(\mathbf{x}) \) that approximates the true relationship \( y \approx f(\mathbf{x}) \).</li>
            <li>Minimize <strong>prediction error</strong> on unseen data (<strong>generalization</strong>).</li>
        </ul>
        
        <h3>Loss Functions</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Common Loss Function</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Regression</strong></td>
                        <td>Mean Squared Error (MSE): \( \frac{1}{n}\sum (y_i - \hat{y}_i)^2 \)</td>
                        <td>Penalizes large errors quadratically</td>
                    </tr>
                    <tr>
                        <td><strong>Classification</strong></td>
                        <td>Cross-Entropy Loss: \( -\sum y_i \log(\hat{y}_i) \)</td>
                        <td>Measures divergence between true and predicted probabilities</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Bias-Variance Tradeoff</h3>
        <ul>
            <li><strong>High bias</strong> → underfitting (model too simple).</li>
            <li><strong>High variance</strong> → overfitting (model too complex).</li>
            <li>Goal: Find model complexity that balances both.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Decision Trees and Ensemble Methods</h2>
    <div class="content">
        <h3>Decision Trees</h3>
        <ul>
            <li><strong>Structure</strong>: Tree of <strong>if-else rules</strong> based on feature thresholds.</li>
            <li><strong>Splitting criterion</strong>:
            <ul>
                <li><strong>Classification</strong>: Information gain (based on entropy) or Gini impurity.<br>
                \[ \text{Gini} = 1 - \sum_{k} p_k^2 \]</li>
                <li><strong>Regression</strong>: Minimize MSE after split.</li>
            </ul>
            </li>
            <li><strong>Pros</strong>: Interpretable, handles mixed data types, no feature scaling needed.</li>
            <li><strong>Cons</strong>: Prone to overfitting, unstable to data changes.</li>
        </ul>
        
        <h3>Ensemble Methods</h3>
        <p>Combine multiple models to improve performance and robustness.</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Key Properties</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Bagging</strong> (e.g., <strong>Random Forest</strong>)</td>
                        <td>Train many trees on <strong>bootstrap samples</strong>; average predictions (regression) or vote (classification)</td>
                        <td>Reduces variance; decorrelates trees by random feature subsets</td>
                    </tr>
                    <tr>
                        <td><strong>Boosting</strong> (e.g., <strong>AdaBoost, XGBoost</strong>)</td>
                        <td>Sequentially train weak learners; each focuses on <strong>previous errors</strong></td>
                        <td>Reduces bias; highly accurate; sensitive to noise</td>
                    </tr>
                    <tr>
                        <td><strong>Stacking</strong></td>
                        <td>Use predictions of base models as input to a <strong>meta-learner</strong></td>
                        <td>Leverages strengths of diverse models</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Random Forest</strong>: Often a strong baseline—robust, parallelizable, handles missing data.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Support Vector Machines (SVM)</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Find the <strong>optimal separating hyperplane</strong> that maximizes the <strong>margin</strong> (distance to nearest points).</p>
        
        <h3>Mathematical Formulation</h3>
        <p>For linearly separable data:</p>
        <div class="equation">
            \[ \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{subject to} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \]
        </div>
        <p><strong>Support vectors</strong>: Data points on the margin (only these affect the solution).</p>
        
        <h3>Non-linear SVM: Kernel Trick</h3>
        <ul>
            <li>Map data to higher-dimensional space via <strong>kernel function</strong> \( K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j) \).</li>
            <li>Common kernels:
            <ul>
                <li><strong>Linear</strong>: \( K = \mathbf{x}_i^\top \mathbf{x}_j \)</li>
                <li><strong>Polynomial</strong>: \( K = (\gamma \mathbf{x}_i^\top \mathbf{x}_j + r)^d \)</li>
                <li><strong>RBF (Gaussian)</strong>: \( K = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2) \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Soft Margin (for noisy data)</h3>
        <ul>
            <li>Introduce <strong>slack variables</strong> \( \xi_i \) to allow misclassifications.</li>
            <li>Controlled by <strong>regularization parameter \( C \)</strong>:
            <ul>
                <li>Large \( C \) → hard margin (low bias, high variance)</li>
                <li>Small \( C \) → soft margin (high bias, low variance)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Pros & Cons</h3>
        <ul>
            <li>✅ Effective in high dimensions, memory efficient (uses support vectors only).</li>
            <li>❌ Poor performance on large datasets; requires careful kernel and \( C \) tuning.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Neural Networks and Backpropagation</h2>
    <div class="content">
        <h3>Neural Network Structure</h3>
        <ul>
            <li><strong>Layers</strong>: Input → hidden layers → output.</li>
            <li><strong>Neuron</strong>: \( z = \mathbf{w}^\top \mathbf{x} + b \), \( a = \sigma(z) \)</li>
            <li><strong>Activation functions</strong>:
            <ul>
                <li><strong>Sigmoid</strong>: \( \sigma(z) = \frac{1}{1 + e^{-z}} \) → outputs probabilities (classification)</li>
                <li><strong>ReLU</strong>: \( \max(0, z) \) → avoids vanishing gradient; standard in hidden layers</li>
                <li><strong>Softmax</strong>: For multi-class output: \( \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}} \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Backpropagation</h3>
        <ul>
            <li><strong>Goal</strong>: Compute gradients of loss \( \mathcal{L} \) w.r.t. all weights for gradient descent.</li>
            <li><strong>Algorithm</strong>:
            <ol>
                <li><strong>Forward pass</strong>: Compute predictions \( \hat{y} \).</li>
                <li><strong>Compute loss</strong> \( \mathcal{L}(y, \hat{y}) \).</li>
                <li><strong>Backward pass</strong>: Apply chain rule from output to input:<br>
                \[ \frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \frac{\partial \mathcal{L}}{\partial a_j^{(l)}} \cdot \frac{\partial a_j^{(l)}}{\partial z_j^{(l)}} \cdot \frac{\partial z_j^{(l)}}{\partial w_{ij}^{(l)}} \]</li>
                <li>Update weights: \( w \leftarrow w - \eta \nabla_w \mathcal{L} \) (\(\eta\) = learning rate)</li>
            </ol>
            </li>
        </ul>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>Universal approximation theorem</strong>: A NN with 1 hidden layer can approximate any continuous function (given enough neurons).</li>
            <li><strong>Deep learning</strong>: Multiple hidden layers learn hierarchical features (e.g., edges → shapes → objects in images).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Model Evaluation Metrics</h2>
    <div class="content">
        <h3>For Classification</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Formula</th>
                        <th>When to Use</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Accuracy</strong></td>
                        <td>\( \frac{TP + TN}{TP + TN + FP + FN} \)</td>
                        <td>Balanced datasets</td>
                    </tr>
                    <tr>
                        <td><strong>Precision</strong></td>
                        <td>\( \frac{TP}{TP + FP} \)</td>
                        <td>Minimize false positives (e.g., spam detection)</td>
                    </tr>
                    <tr>
                        <td><strong>Recall (Sensitivity)</strong></td>
                        <td>\( \frac{TP}{TP + FN} \)</td>
                        <td>Minimize false negatives (e.g., disease screening)</td>
                    </tr>
                    <tr>
                        <td><strong>F1-Score</strong></td>
                        <td>\( 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \)</td>
                        <td>Imbalanced datasets; harmonic mean of P & R</td>
                    </tr>
                    <tr>
                        <td><strong>ROC-AUC</strong></td>
                        <td>Area under ROC curve (TPR vs. FPR)</td>
                        <td>Threshold-invariant performance</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            <strong>Confusion Matrix</strong>:<br>
            <pre>               Predicted
              +       -
Actual   +   TP      FN
         -   FP      TN</pre>
        </div>
        
        <h3>For Regression</h3>
        <ul>
            <li><strong>Mean Absolute Error (MAE)</strong>: \( \frac{1}{n}\sum |y_i - \hat{y}_i| \) → robust to outliers.</li>
            <li><strong>Mean Squared Error (MSE)</strong>: \( \frac{1}{n}\sum (y_i - \hat{y}_i)^2 \) → penalizes large errors.</li>
            <li><strong>R² (Coefficient of Determination)</strong>:<br>
            \[ R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} \]
            <ul>
                <li>\( R^2 = 1 \): perfect fit; \( R^2 = 0 \): no better than mean predictor.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Cross-Validation</h3>
        <ul>
            <li><strong>k-Fold CV</strong>: Split data into \(k\) folds; train on \(k-1\), validate on 1; repeat \(k\) times.</li>
            <li>Provides <strong>unbiased estimate</strong> of generalization performance.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Supervised Learning Algorithms</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Type</th>
                        <th>Strengths</th>
                        <th>Weaknesses</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Decision Tree</strong></td>
                        <td>Both</td>
                        <td>Interpretable, fast</td>
                        <td>Overfits</td>
                        <td>Baseline, explainability</td>
                    </tr>
                    <tr>
                        <td><strong>Random Forest</strong></td>
                        <td>Both</td>
                        <td>Robust, handles noise</td>
                        <td>Less interpretable</td>
                        <td>General-purpose</td>
                    </tr>
                    <tr>
                        <td><strong>SVM</strong></td>
                        <td>Both</td>
                        <td>Effective in high-D, memory efficient</td>
                        <td>Slow on large data</td>
                        <td>Text classification, small/medium datasets</td>
                    </tr>
                    <tr>
                        <td><strong>Neural Network</strong></td>
                        <td>Both</td>
                        <td>Learns complex patterns</td>
                        <td>Data-hungry, black-box</td>
                        <td>Images, speech, large datasets</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Supervised learning</strong> requires labeled data and aims to generalize from examples.</li>
            <li><strong>No free lunch</strong>: Algorithm choice depends on data size, dimensionality, noise, and interpretability needs.</li>
            <li><strong>Evaluation metrics must match the problem</strong>: Accuracy fails on imbalanced data; use F1 or AUC instead.</li>
            <li><strong>Ensembles</strong> (e.g., Random Forest, XGBoost) often outperform single models.</li>
            <li><strong>Neural networks</strong> excel with large data but require careful tuning and validation.</li>
        </ul>
    </div>
</section>



<section class="section">
    <h2 class="section-title">Unsupervised Learning</h2>
    <div class="content">
        <p><em>Learning patterns from unlabeled data</em></p>
        
        <div class="definition">
            <strong>Definition</strong>: Discover hidden structures, patterns, or relationships in data <strong>without labeled outputs</strong>.<br>
            <strong>Goal</strong>: Summarize, compress, or reveal intrinsic properties of the data.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Clustering Algorithms</h2>
    <div class="content">
        <p>Clustering groups similar data points together based on feature similarity.</p>
        
        <h3>(a) K-Means Clustering</h3>
        <ul>
            <li><strong>Objective</strong>: Partition \(n\) points into \(k\) clusters to minimize <strong>within-cluster sum of squares (WCSS)</strong>:<br>
            \[ \min_{C_1,\dots,C_k} \sum_{i=1}^k \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2 \]
            where \(\boldsymbol{\mu}_i\) = centroid of cluster \(i\).</li>
            
            <li><strong>Algorithm</strong> (Lloyd's):
            <ol>
                <li>Initialize \(k\) centroids randomly.</li>
                <li><strong>Assign</strong>: Assign each point to nearest centroid.</li>
                <li><strong>Update</strong>: Recompute centroids as mean of assigned points.</li>
                <li>Repeat until convergence.</li>
            </ol>
            </li>
            
            <li><strong>Pros</strong>: Simple, fast, scalable.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li>Requires pre-specifying \(k\).</li>
                <li>Sensitive to initialization and outliers.</li>
                <li>Assumes spherical, equally sized clusters.</li>
            </ul>
            </li>
            
            <li><strong>Choosing \(k\)</strong>: Use <strong>elbow method</strong> (plot WCSS vs. \(k\)) or <strong>silhouette score</strong>.</li>
        </ul>
        
        <h3>(b) Hierarchical Clustering</h3>
        <ul>
            <li>Builds a <strong>tree of clusters</strong> (dendrogram).</li>
            <li>Two approaches:
            <ul>
                <li><strong>Agglomerative</strong> (bottom-up): Start with each point as a cluster; merge closest pairs.</li>
                <li><strong>Divisive</strong> (top-down): Start with one cluster; recursively split.</li>
            </ul>
            </li>
            
            <li><strong>Linkage criteria</strong> (define distance between clusters):
            <ul>
                <li><strong>Single</strong>: min distance → chaining effect.</li>
                <li><strong>Complete</strong>: max distance → compact clusters.</li>
                <li><strong>Average</strong>: mean distance → balanced.</li>
                <li><strong>Ward</strong>: minimizes WCSS → similar to K-means.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>: No need to specify \(k\); dendrogram gives full hierarchy.</li>
            <li><strong>Cons</strong>: \(O(n^3)\) time; not scalable to large datasets.</li>
        </ul>
        
        <h3>(c) DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h3>
        <ul>
            <li><strong>Core idea</strong>: Cluster points in <strong>dense regions</strong>, mark sparse points as <strong>noise</strong>.</li>
            <li><strong>Parameters</strong>:
            <ul>
                <li>\(\varepsilon\): Radius of neighborhood.</li>
                <li>MinPts: Minimum points in \(\varepsilon\)-neighborhood to be a <strong>core point</strong>.</li>
            </ul>
            </li>
            
            <li><strong>Point types</strong>:
            <ul>
                <li><strong>Core</strong>: ≥ MinPts in \(\varepsilon\)-neighborhood.</li>
                <li><strong>Border</strong>: In neighborhood of core, but not core itself.</li>
                <li><strong>Noise</strong>: Neither core nor border.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>:
            <ul>
                <li>Finds arbitrarily shaped clusters.</li>
                <li>Robust to outliers.</li>
                <li>No need to specify \(k\).</li>
            </ul>
            </li>
            <li><strong>Cons</strong>: Struggles with varying densities; sensitive to \(\varepsilon\) and MinPts.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Spatial data, anomaly detection, irregular cluster shapes.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Dimensionality Reduction</h2>
    <div class="content">
        <p>Reduce number of features while preserving essential structure.</p>
        
        <h3>(a) Principal Component Analysis (PCA)</h3>
        <ul>
            <li><strong>Goal</strong>: Project data onto orthogonal axes (<strong>principal components</strong>) that capture <strong>maximum variance</strong>.</li>
            <li><strong>Steps</strong>:
            <ol>
                <li>Standardize data (zero mean, unit variance).</li>
                <li>Compute covariance matrix.</li>
                <li>Eigendecomposition: Find eigenvectors (PCs) and eigenvalues (variance explained).</li>
                <li>Project data onto top \(k\) PCs.</li>
            </ol>
            </li>
            
            <li><strong>Mathematically</strong>:<br>
            \[ \mathbf{Z} = \mathbf{X} \mathbf{W}_k \]
            where \(\mathbf{W}_k\) = top \(k\) eigenvectors.</li>
            
            <li><strong>Pros</strong>: Linear, fast, removes multicollinearity.</li>
            <li><strong>Cons</strong>: Only captures linear relationships; global structure only.</li>
            <li><strong>Use cases</strong>: Visualization (2D/3D), noise reduction, preprocessing for ML.</li>
        </ul>
        
        <h3>(b) t-SNE (t-Distributed Stochastic Neighbor Embedding)</h3>
        <ul>
            <li><strong>Goal</strong>: Preserve <strong>local similarities</strong> in low-dimensional embedding (typically 2D/3D).</li>
            <li><strong>Key idea</strong>:
            <ul>
                <li>Convert high-D Euclidean distances to <strong>probabilities</strong> (similar points → high probability).</li>
                <li>Minimize <strong>KL divergence</strong> between high-D and low-D probability distributions.</li>
            </ul>
            </li>
            
            <li><strong>Pros</strong>: Excellent for <strong>visualization</strong>; reveals clusters and manifolds.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li>Not linear; <strong>not for preprocessing</strong> (distorts global structure).</li>
                <li>Computationally expensive (\(O(n^2)\)).</li>
                <li>Results vary with perplexity (key hyperparameter).</li>
            </ul>
            </li>
        </ul>
        
        <div class="definition">
            📌 <strong>Rule</strong>: Use <strong>PCA first</strong> (to reduce to 50D), then <strong>t-SNE</strong> for visualization.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Anomaly Detection Methods</h2>
    <div class="content">
        <p>Identify rare items, events, or observations that differ significantly from the majority.</p>
        
        <h3>Common Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Statistical</strong></td>
                        <td>Assume data follows distribution (e.g., Gaussian); flag points with low likelihood</td>
                        <td>Univariate data, known distribution</td>
                    </tr>
                    <tr>
                        <td><strong>Distance-based</strong></td>
                        <td>Points far from neighbors are anomalies (e.g., k-NN distance)</td>
                        <td>Multivariate, no distribution assumption</td>
                    </tr>
                    <tr>
                        <td><strong>Density-based</strong></td>
                        <td>Low-density regions = anomalies (e.g., LOF: Local Outlier Factor)</td>
                        <td>Varying densities</td>
                    </tr>
                    <tr>
                        <td><strong>Clustering-based</strong></td>
                        <td>Points not in any cluster (or in small clusters) are anomalies</td>
                        <td>Works with DBSCAN, K-means</td>
                    </tr>
                    <tr>
                        <td><strong>Reconstruction-based</strong></td>
                        <td>Use autoencoders (see below); high reconstruction error = anomaly</td>
                        <td>High-dimensional, complex data</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Evaluation</h3>
        <ul>
            <li>Often <strong>semi-supervised</strong>: Assume most data is normal.</li>
            <li>Metrics: Precision@K, ROC-AUC (if labels available).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Autoencoders and Their Variants</h2>
    <div class="content">
        <p>Neural networks that learn efficient data <strong>encodings</strong> in an unsupervised manner.</p>
        
        <h3>Basic Autoencoder</h3>
        <ul>
            <li><strong>Architecture</strong>:<br>
            <strong>Encoder</strong>: \( \mathbf{z} = f(\mathbf{x}) = \sigma(\mathbf{W}_e \mathbf{x} + \mathbf{b}_e) \)<br>
            <strong>Decoder</strong>: \( \hat{\mathbf{x}} = g(\mathbf{z}) = \sigma(\mathbf{W}_d \mathbf{z} + \mathbf{b}_d) \)</li>
            <li><strong>Loss</strong>: Reconstruction error (e.g., MSE): \( \mathcal{L} = \|\mathbf{x} - \hat{\mathbf{x}}\|^2 \)</li>
            <li><strong>Bottleneck</strong>: Latent dimension \( < \) input dimension → forces compression.</li>
        </ul>
        
        <h3>Key Variants</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Variant</th>
                        <th>Modification</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Denoising Autoencoder (DAE)</strong></td>
                        <td>Input: corrupted \(\tilde{\mathbf{x}}\); target: clean \(\mathbf{x}\)</td>
                        <td>Learn robust features; prevent identity mapping</td>
                    </tr>
                    <tr>
                        <td><strong>Sparse Autoencoder</strong></td>
                        <td>Add sparsity penalty on latent code (e.g., KL divergence on activations)</td>
                        <td>Learn part-based representations</td>
                    </tr>
                    <tr>
                        <td><strong>Variational Autoencoder (VAE)</strong></td>
                        <td>Latent code sampled from learned distribution \(q(z|x)\); regularized via KL divergence to prior \(p(z)\)</td>
                        <td>Generative model; smooth latent space</td>
                    </tr>
                    <tr>
                        <td><strong>Convolutional Autoencoder</strong></td>
                        <td>Use CNN layers in encoder/decoder</td>
                        <td>Handle image data; preserve spatial structure</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Applications</h3>
        <ul>
            <li>Dimensionality reduction (non-linear alternative to PCA)</li>
            <li>Anomaly detection (high reconstruction error = anomaly)</li>
            <li>Denoising, image inpainting</li>
            <li>Pretraining for deep networks</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Association Rule Learning</h2>
    <div class="content">
        <p>Discover <strong>interesting relationships</strong> between variables in large datasets (e.g., market basket analysis).</p>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>Itemset</strong>: Collection of items (e.g., {milk, bread}).</li>
            <li><strong>Transaction</strong>: A single record (e.g., customer purchase).</li>
            <li><strong>Rule</strong>: \( X \Rightarrow Y \) (e.g., {milk} ⇒ {bread})</li>
        </ul>
        
        <h3>Metrics</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Formula</th>
                        <th>Interpretation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Support</strong></td>
                        <td>\( \frac{\text{# transactions containing } X \cup Y}{\text{total transactions}} \)</td>
                        <td>How frequent is the rule?</td>
                    </tr>
                    <tr>
                        <td><strong>Confidence</strong></td>
                        <td>\( \frac{\text{support}(X \cup Y)}{\text{support}(X)} \)</td>
                        <td>How often does Y occur when X occurs?</td>
                    </tr>
                    <tr>
                        <td><strong>Lift</strong></td>
                        <td>\( \frac{\text{confidence}(X \Rightarrow Y)}{\text{support}(Y)} \)</td>
                        <td>>1: positive correlation; =1: independent; <1: negative</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Apriori Algorithm</h3>
        <ul>
            <li><strong>Principle</strong>: If an itemset is frequent, all its subsets are frequent (<strong>anti-monotonicity</strong>).</li>
            <li><strong>Steps</strong>:
            <ol>
                <li>Find all frequent 1-itemsets (support ≥ min_support).</li>
                <li>Generate candidate \(k\)-itemsets from frequent \((k-1)\)-itemsets.</li>
                <li>Prune candidates with infrequent subsets.</li>
                <li>Repeat until no more frequent itemsets.</li>
            </ol>
            </li>
            <li><strong>Pros</strong>: Simple, guarantees completeness.</li>
            <li><strong>Cons</strong>: Computationally expensive for large itemsets.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Modern alternatives</strong>: FP-Growth (uses frequent pattern tree; faster, no candidate generation).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Type</th>
                        <th>Key Strength</th>
                        <th>Limitation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>K-Means</strong></td>
                        <td>Clustering</td>
                        <td>Fast, simple</td>
                        <td>Spherical clusters only</td>
                    </tr>
                    <tr>
                        <td><strong>DBSCAN</strong></td>
                        <td>Clustering</td>
                        <td>Finds arbitrary shapes, handles noise</td>
                        <td>Struggles with density variation</td>
                    </tr>
                    <tr>
                        <td><strong>PCA</strong></td>
                        <td>Dim. Reduction</td>
                        <td>Linear, fast, interpretable</td>
                        <td>Misses non-linear structure</td>
                    </tr>
                    <tr>
                        <td><strong>t-SNE</strong></td>
                        <td>Dim. Reduction</td>
                        <td>Excellent visualization</td>
                        <td>Not for preprocessing; slow</td>
                    </tr>
                    <tr>
                        <td><strong>Autoencoder</strong></td>
                        <td>Representation</td>
                        <td>Non-linear, flexible</td>
                        <td>Requires tuning; black-box</td>
                    </tr>
                    <tr>
                        <td><strong>Association Rules</strong></td>
                        <td>Pattern Mining</td>
                        <td>Interpretable business rules</td>
                        <td>Combinatorial explosion</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Clustering</strong> reveals group structure; choose algorithm based on shape, noise, and scalability.</li>
            <li><strong>Dimensionality reduction</strong>: Use <strong>PCA</strong> for linear/global structure, <strong>t-SNE</strong> for visualization.</li>
            <li><strong>Anomaly detection</strong> is critical in fraud, security, and quality control—method depends on data assumptions.</li>
            <li><strong>Autoencoders</strong> provide powerful non-linear compression and are foundational for deep generative models.</li>
            <li><strong>Association rules</strong> uncover actionable insights in transactional data—but require careful thresholding.</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Remember</strong>: Unsupervised learning is <strong>exploratory</strong>—results must be validated with domain knowledge!
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Deep Learning</h2>
    <div class="content">
        <p><em>Advanced Neural Network Architectures and Training Methods</em></p>
        
        <h3>Convolutional Neural Networks (CNNs)</h3>
        <h4>Core Idea</h4>
        <ul>
            <li>Designed for <strong>grid-like data</strong> (e.g., images, audio spectrograms).</li>
            <li>Exploit <strong>spatial locality</strong> and <strong>translation invariance</strong> via <strong>convolutional layers</strong>.</li>
        </ul>
        
        <h4>Key Components</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Function</th>
                        <th>Parameters</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Convolutional</strong></td>
                        <td>Apply learnable filters (kernels) to detect local features (edges, textures)</td>
                        <td>Filter size (e.g., 3×3), #filters, stride, padding</td>
                    </tr>
                    <tr>
                        <td><strong>Activation</strong></td>
                        <td>Introduce non-linearity (e.g., ReLU: \( \max(0, x) \))</td>
                        <td>None</td>
                    </tr>
                    <tr>
                        <td><strong>Pooling</strong></td>
                        <td>Downsample feature maps (reduce spatial size, retain key info)</td>
                        <td>Max-pooling (most common), average-pooling</td>
                    </tr>
                    <tr>
                        <td><strong>Fully Connected (FC)</strong></td>
                        <td>Final layers for classification/regression</td>
                        <td>Standard dense layers</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h4>Why CNNs Work</h4>
        <ul>
            <li><strong>Parameter sharing</strong>: Same filter applied across entire input → fewer parameters.</li>
            <li><strong>Hierarchical feature learning</strong>:<br>
            Early layers → edges → mid layers → shapes → late layers → objects.</li>
            <li><strong>Translation equivariance</strong>: Shift in input → shift in feature map (not output class).</li>
        </ul>
        
        <h4>Common Architectures</h4>
        <ul>
            <li><strong>LeNet-5</strong> (1998): First CNN for digit recognition.</li>
            <li><strong>AlexNet</strong> (2012): Deep CNN with ReLU, dropout; won ImageNet.</li>
            <li><strong>VGG</strong>: Uniform 3×3 convolutions; deeper = better.</li>
            <li><strong>ResNet</strong>: Uses <strong>skip connections</strong> to solve vanishing gradients → trains 1000+ layers.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Image classification, object detection, segmentation, medical imaging.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Recurrent Neural Networks (RNNs) and LSTMs</h2>
    <div class="content">
        <h3>RNNs: Processing Sequences</h3>
        <ul>
            <li><strong>Idea</strong>: Maintain a <strong>hidden state</strong> \( h_t \) that captures information from past inputs.</li>
            <li><strong>Update rule</strong>:<br>
            \[ h_t = \sigma(W_h h_{t-1} + W_x x_t + b) \]
            \( y_t = W_y h_t + b_y \)</li>
            
            <li><strong>Pros</strong>: Handles variable-length sequences; shares parameters across time.</li>
            <li><strong>Cons</strong>:
            <ul>
                <li><strong>Vanishing/exploding gradients</strong> → struggles with long-term dependencies.</li>
                <li>Sequential computation → not parallelizable.</li>
            </ul>
            </li>
        </ul>
        
        <h3>LSTM (Long Short-Term Memory)</h3>
        <ul>
            <li>Solves vanishing gradient with <strong>gating mechanisms</strong>:
            <ul>
                <li><strong>Forget gate</strong> \( f_t \): Decide what to discard from cell state.</li>
                <li><strong>Input gate</strong> \( i_t \): Decide what new info to store.</li>
                <li><strong>Output gate</strong> \( o_t \): Decide what to output.</li>
            </ul>
            </li>
            
            <li><strong>Cell state update</strong>:<br>
            \[ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t, \quad h_t = o_t \odot \tanh(C_t) \]</li>
            
            <li><strong>GRU (Gated Recurrent Unit)</strong>: Simpler variant (2 gates); often comparable to LSTM.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Time series forecasting, speech recognition, machine translation (pre-Transformer).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Transformer Architectures and Attention Mechanisms</h2>
    <div class="content">
        <h3>Limitations of RNNs/CNNs for Sequences</h3>
        <ul>
            <li>RNNs: Not parallelizable; slow training.</li>
            <li>CNNs: Fixed receptive field; need stacking for long-range dependencies.</li>
        </ul>
        
        <h3>Attention Mechanism (Core Idea)</h3>
        <ul>
            <li>Compute <strong>weighted sum</strong> of values, where weights are based on <strong>compatibility</strong> between query and keys.</li>
            <li><strong>Scaled Dot-Product Attention</strong>:<br>
            \[ \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V \]
            <ul>
                <li>\( Q \): queries, \( K \): keys, \( V \): values</li>
                <li>\( d_k \): dimension of keys (scaling prevents softmax saturation)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Transformer Architecture (Vaswani et al., 2017)</h3>
        <ul>
            <li><strong>Encoder-Decoder</strong> structure (for seq2seq tasks like translation).</li>
            <li><strong>Each block contains</strong>:
            <ul>
                <li><strong>Multi-Head Attention</strong>: Concatenate attention from multiple subspaces → captures different relationships.</li>
                <li><strong>Position-wise Feed-Forward Network</strong>: Two linear layers with ReLU.</li>
                <li><strong>Residual connections + LayerNorm</strong>: Stabilize training.</li>
            </ul>
            </li>
            
            <li><strong>Positional Encoding</strong>: Inject sequence order info (since no recurrence):<br>
            \[ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad
            PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right) \]</li>
        </ul>
        
        <h3>Impact</h3>
        <ul>
            <li><strong>Parallelizable</strong>: All tokens processed simultaneously → faster training.</li>
            <li><strong>State-of-the-art</strong>: Foundation of BERT, GPT, T5, and most modern NLP models.</li>
            <li><strong>Beyond NLP</strong>: Vision Transformers (ViT), speech, protein folding (AlphaFold).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: NLP, multimodal learning, any task with long-range dependencies.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Deep Learning Optimization Techniques</h2>
    <div class="content">
        <p>Training deep networks is challenging due to non-convexity, saddle points, and ill-conditioning.</p>
        
        <h3>Gradient Descent Variants</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Optimizer</th>
                        <th>Update Rule</th>
                        <th>Advantages</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>SGD</strong></td>
                        <td>\( \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L} \)</td>
                        <td>Simple, but oscillates</td>
                    </tr>
                    <tr>
                        <td><strong>SGD + Momentum</strong></td>
                        <td>\( v \leftarrow \beta v + \nabla \mathcal{L},\ \theta \leftarrow \theta - \eta v \)</td>
                        <td>Smoother convergence</td>
                    </tr>
                    <tr>
                        <td><strong>RMSProp</strong></td>
                        <td>\( s \leftarrow \beta s + (1-\beta)(\nabla \mathcal{L})^2,\ \theta \leftarrow \theta - \eta \frac{\nabla \mathcal{L}}{\sqrt{s} + \epsilon} \)</td>
                        <td>Adapts per-parameter learning rate</td>
                    </tr>
                    <tr>
                        <td><strong>Adam</strong></td>
                        <td>Combines momentum + RMSProp</td>
                        <td>Most popular; robust default</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Learning Rate Scheduling</h3>
        <ul>
            <li><strong>Step decay</strong>: Reduce LR at fixed epochs.</li>
            <li><strong>Cosine annealing</strong>: Smoothly decrease LR to 0.</li>
            <li><strong>Warmup</strong>: Gradually increase LR at start (critical for Transformers).</li>
        </ul>
        
        <h3>Gradient Clipping</h3>
        <p>Cap gradient norm to prevent exploding gradients (essential for RNNs).</p>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Regularization Methods</h2>
    <div class="content">
        <p>Prevent overfitting in high-capacity deep networks.</p>
        
        <h3>(a) Dropout</h3>
        <ul>
            <li><strong>Idea</strong>: Randomly set a fraction \( p \) of activations to zero during training.</li>
            <li><strong>Effect</strong>: Forces network to not rely on specific neurons → ensemble-like behavior.</li>
            <li><strong>At test time</strong>: Scale activations by \( (1 - p) \) (or use inverted dropout).</li>
            <li><strong>Typical rates</strong>: 0.2–0.5 in hidden layers; not used in RNN hidden states (use <strong>variational dropout</strong> instead).</li>
        </ul>
        
        <h3>(b) Batch Normalization (BatchNorm)</h3>
        <ul>
            <li><strong>Idea</strong>: Normalize layer inputs to have zero mean and unit variance <strong>per batch</strong>:<br>
            \[ \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta \]
            where \( \gamma, \beta \) are learnable scale/shift parameters.</li>
            
            <li><strong>Benefits</strong>:
            <ul>
                <li>Reduces internal covariate shift.</li>
                <li>Allows higher learning rates.</li>
                <li>Acts as a regularizer (due to batch noise).</li>
            </ul>
            </li>
            
            <li><strong>Placement</strong>: After linear layer, before activation (common in CNNs).</li>
            <li><strong>Alternatives</strong>:
            <ul>
                <li><strong>LayerNorm</strong>: Normalize across features (used in Transformers).</li>
                <li><strong>GroupNorm</strong>: Between BatchNorm and LayerNorm; works well for small batches.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Other Regularization Techniques</h3>
        <ul>
            <li><strong>Weight decay (L2 regularization)</strong>: Penalize large weights.</li>
            <li><strong>Data augmentation</strong>: Artificially expand training set (e.g., rotate/flipping images).</li>
            <li><strong>Early stopping</strong>: Halt training when validation loss stops improving.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Key Innovation</th>
                        <th>Primary Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CNN</strong></td>
                        <td>Local receptive fields, weight sharing</td>
                        <td>Images, spatial data</td>
                    </tr>
                    <tr>
                        <td><strong>RNN/LSTM</strong></td>
                        <td>Hidden state for memory</td>
                        <td>Sequential data (time series, text)</td>
                    </tr>
                    <tr>
                        <td><strong>Transformer</strong></td>
                        <td>Self-attention, parallelization</td>
                        <td>NLP, long-range dependencies</td>
                    </tr>
                    <tr>
                        <td><strong>Adam</strong></td>
                        <td>Adaptive per-parameter learning rates</td>
                        <td>Default optimizer for most tasks</td>
                    </tr>
                    <tr>
                        <td><strong>Dropout</strong></td>
                        <td>Random neuron deactivation</td>
                        <td>General-purpose regularization</td>
                    </tr>
                    <tr>
                        <td><strong>BatchNorm</strong></td>
                        <td>Normalize per batch</td>
                        <td>Stabilize and accelerate CNN training</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>CNNs</strong> dominate <strong>spatial data</strong>; <strong>Transformers</strong> dominate <strong>sequential data</strong> (replacing RNNs).</li>
            <li><strong>Attention</strong> is a universal mechanism—now used in vision, audio, and biology.</li>
            <li><strong>Optimization</strong> is as important as architecture: Adam + learning rate scheduling is standard.</li>
            <li><strong>Regularization</strong> is essential: Combine dropout, BatchNorm, and data augmentation.</li>
            <li><strong>Modern deep learning</strong> = <strong>modular design</strong>: Mix and match components (e.g., CNN + Transformer for video).</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>: For new projects, start with a <strong>Transformer</strong> (for sequences) or <strong>ResNet/ViT</strong> (for images)—they are strong baselines.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Neural Networks</h2>
    <div class="content">
        <h3>Graph Representation Learning</h3>
        <div class="definition">
            <strong>Goal</strong>: Learn low-dimensional, continuous vector representations (<strong>embeddings</strong>) of nodes, edges, or entire graphs that preserve structural and feature information for downstream tasks (e.g., classification, link prediction, clustering).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Theory Fundamentals</h2>
    <div class="content">
        <h3>Basic Definitions</h3>
        <ul>
            <li><strong>Graph</strong> \( G = (V, E) \):
            <ul>
                <li>\( V \): Set of <strong>nodes</strong> (vertices), \( |V| = n \)</li>
                <li>\( E \subseteq V \times V \): Set of <strong>edges</strong> (links)</li>
            </ul>
            </li>
            <li><strong>Types</strong>:
            <ul>
                <li><strong>Undirected</strong>: \( (u,v) \in E \Rightarrow (v,u) \in E \)</li>
                <li><strong>Directed</strong>: Edges have direction (e.g., web graphs)</li>
                <li><strong>Weighted</strong>: Edges have weights \( w_{uv} \)</li>
                <li><strong>Attributed</strong>: Nodes/edges have features \( \mathbf{x}_v \in \mathbb{R}^d \)</li>
            </ul>
            </li>
        </ul>
        
        <h3>Key Concepts</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Term</th>
                        <th>Definition</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Adjacency matrix</strong> \( A \)</td>
                        <td>\( A_{ij} = 1 \) if edge between \( i,j \); 0 otherwise</td>
                    </tr>
                    <tr>
                        <td><strong>Degree matrix</strong> \( D \)</td>
                        <td>Diagonal matrix: \( D_{ii} = \sum_j A_{ij} \)</td>
                    </tr>
                    <tr>
                        <td><strong>Laplacian</strong> \( L \)</td>
                        <td>\( L = D - A \) (captures smoothness on graphs)</td>
                    </tr>
                    <tr>
                        <td><strong>Path / Walk</strong></td>
                        <td>Sequence of connected nodes</td>
                    </tr>
                    <tr>
                        <td><strong>Neighborhood</strong> \( \mathcal{N}(v) \)</td>
                        <td>Set of nodes adjacent to \( v \)</td>
                    </tr>
                    <tr>
                        <td><strong>Connected component</strong></td>
                        <td>Maximal subgraph where all nodes are reachable</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Why Graphs Matter</h3>
        <ul>
            <li>Model relational data: social networks, molecules, knowledge graphs, traffic systems.</li>
            <li>Structure encodes semantics: “You are who your friends are” (homophily).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Embeddings and Node Representations</h2>
    <div class="content">
        <h3>What is a Graph Embedding?</h3>
        <p>Map each node \( v \) to a vector \( \mathbf{z}_v \in \mathbb{R}^k \) (\( k \ll n \)) such that <strong>graph proximity ≈ vector similarity</strong>.</p>
        
        <h3>Desired Properties</h3>
        <ol>
            <li><strong>Structural equivalence</strong>: Nodes with similar roles (e.g., hubs) have similar embeddings.</li>
            <li><strong>Homophily</strong>: Connected nodes have similar embeddings.</li>
            <li><strong>Scalability</strong>: Efficient for large graphs.</li>
            <li><strong>Inductive capability</strong>: Generalize to unseen nodes.</li>
        </ol>
        
        <h3>Categories of Methods</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Approach</th>
                        <th>Examples</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Matrix factorization</strong></td>
                        <td>Factorize graph matrices (e.g., \( A \), \( L \))</td>
                        <td>Laplacian Eigenmaps, GraRep</td>
                    </tr>
                    <tr>
                        <td><strong>Random walk-based</strong></td>
                        <td>Use walk statistics as context</td>
                        <td>DeepWalk, node2vec</td>
                    </tr>
                    <tr>
                        <td><strong>Neural network-based</strong></td>
                        <td>Learn via message passing</td>
                        <td>GCN, GAT, GraphSAGE</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Random Walk Techniques on Graphs</h2>
    <div class="content">
        <h3>Core Idea</h3>
        <p>Treat node neighborhoods as “sentences” and use <strong>language modeling</strong> (e.g., Word2Vec) to learn embeddings.</p>
        
        <h3>(a) DeepWalk (2014)</h3>
        <ul>
            <li><strong>Steps</strong>:
            <ol>
                <li>Generate <strong>random walks</strong> (length \( l \)) from each node.</li>
                <li>Treat each walk as a “sentence” of nodes.</li>
                <li>Apply <strong>Skip-gram</strong> to predict context nodes given center node.</li>
            </ol>
            </li>
            <li><strong>Optimization</strong>:<br>
            \[ \max_{\mathbf{Z}} \sum_{v \in V} \sum_{u \in \mathcal{N}_R(v)} \log P(u | v; \mathbf{Z}) \]
            where \( \mathcal{N}_R(v) \) = nodes in random walks starting at \( v \).</li>
        </ul>
        
        <h3>(b) node2vec (2016)</h3>
        <ul>
            <li><strong>Improvement</strong>: Biased random walks controlled by two parameters:
            <ul>
                <li><strong>Return parameter \( p \)</strong>: Likelihood to return to previous node.</li>
                <li><strong>In-out parameter \( q \)</strong>: Explore locally (\( q < 1 \)) vs. globally (\( q > 1 \)).</li>
            </ul>
            </li>
            <li><strong>Flexible</strong>: Interpolates between <strong>BFS</strong> (structural equivalence) and <strong>DFS</strong> (homophily).</li>
        </ul>
        
        <h3>Pros & Cons</h3>
        <ul>
            <li>✅ Simple, scalable, no node features needed.</li>
            <li>❌ Transductive only (can't embed new nodes without retraining).</li>
            <li>❌ Ignores edge weights/direction in basic form.</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Kernels and Similarity Measures</h2>
    <div class="content">
        <h3>What is a Graph Kernel?</h3>
        <p>A function \( \kappa(G_1, G_2) \) that measures similarity between two <strong>entire graphs</strong> by comparing substructures.</p>
        
        <h3>Common Graph Kernels</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Kernel</th>
                        <th>Substructure Compared</th>
                        <th>Complexity</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random Walk Kernel</strong></td>
                        <td>Counts matching walks</td>
                        <td>\( O(n^3) \); may diverge</td>
                    </tr>
                    <tr>
                        <td><strong>Shortest-Path Kernel</strong></td>
                        <td>Distribution of shortest-path lengths</td>
                        <td>\( O(n^2) \)</td>
                    </tr>
                    <tr>
                        <td><strong>Graphlet Kernel</strong></td>
                        <td>Counts of small connected subgraphs (e.g., triangles)</td>
                        <td>Exponential in graphlet size</td>
                    </tr>
                    <tr>
                        <td><strong>Weisfeiler-Lehman (WL) Kernel</strong></td>
                        <td>Refines node labels via neighborhood aggregation</td>
                        <td>\( O(h n) \), \( h \)=iterations</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>WL Kernel (Most Practical)</h3>
        <ul>
            <li>Simulates <strong>WL graph isomorphism test</strong>:
            <ol>
                <li>Initialize node labels (e.g., degree).</li>
                <li>Iteratively update label of \( v \):<br>
                \( l_v^{(k)} = \text{hash}(l_v^{(k-1)}, \{l_u^{(k-1)} : u \in \mathcal{N}(v)\}) \)</li>
                <li>Compare label distributions between graphs.</li>
            </ol>
            </li>
        </ul>
        
        <div class="definition">
            🔑 <strong>Connection to GNNs</strong>: WL test ≈ 1-layer GNN; expressive power of GNNs bounded by WL test.
        </div>
        
        <h3>Limitations</h3>
        <ul>
            <li>Kernels are <strong>computationally expensive</strong> for large graphs.</li>
            <li>Not end-to-end trainable with neural networks (unlike GNNs).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Applications of Graph Representation Learning</h2>
    <div class="content">
        <h3>(a) Node-Level Tasks</h3>
        <ul>
            <li><strong>Node classification</strong>: Predict labels (e.g., user interests in social networks).
            <ul>
                <li><em>Methods</em>: GCN, GAT, GraphSAGE.</li>
            </ul>
            </li>
            <li><strong>Link prediction</strong>: Predict missing/future edges (e.g., friend recommendation).
            <ul>
                <li><em>Approach</em>: Embed nodes → score pairs via dot product or MLP.</li>
            </ul>
            </li>
            <li><strong>Anomaly detection</strong>: Identify unusual nodes (e.g., fraud in transaction graphs).</li>
        </ul>
        
        <h3>(b) Graph-Level Tasks</h3>
        <ul>
            <li><strong>Graph classification</strong>: Predict property of entire graph (e.g., molecule toxicity).
            <ul>
                <li><em>Methods</em>: Graph kernels, GNNs with <strong>readout</strong> (e.g., sum/mean pooling).</li>
            </ul>
            </li>
            <li><strong>Graph similarity</strong>: Measure similarity between graphs (e.g., chemical compounds).</li>
        </ul>
        
        <h3>(c) Real-World Domains</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Domain</th>
                        <th>Application</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Social Networks</strong></td>
                        <td>Community detection, recommendation</td>
                        <td>Facebook friend suggestions</td>
                    </tr>
                    <tr>
                        <td><strong>Bioinformatics</strong></td>
                        <td>Protein function prediction, drug discovery</td>
                        <td>Predicting protein-protein interactions</td>
                    </tr>
                    <tr>
                        <td><strong>Knowledge Graphs</strong></td>
                        <td>Entity linking, question answering</td>
                        <td>Google Knowledge Graph</td>
                    </tr>
                    <tr>
                        <td><strong>Computer Vision</strong></td>
                        <td>Scene graph generation</td>
                        <td>Understanding object relationships in images</td>
                    </tr>
                    <tr>
                        <td><strong>Traffic/Transport</strong></td>
                        <td>Traffic forecasting</td>
                        <td>Predicting congestion using road networks</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>(d) Emerging Trends</h3>
        <ul>
            <li><strong>Heterogeneous graphs</strong>: Multiple node/edge types (e.g., academic graphs: authors, papers, venues).</li>
            <li><strong>Dynamic graphs</strong>: Evolving structure over time (e.g., Twitter retweets).</li>
            <li><strong>Large-scale GNNs</strong>: Scaling to billion-edge graphs (e.g., PinSage at Pinterest).</li>
        </ul>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Key Idea</th>
                        <th>Strengths</th>
                        <th>Limitations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Random Walks (DeepWalk/node2vec)</strong></td>
                        <td>Use walk context + Skip-gram</td>
                        <td>Scalable, no features needed</td>
                        <td>Transductive, ignores features</td>
                    </tr>
                    <tr>
                        <td><strong>Graph Kernels</strong></td>
                        <td>Compare substructures via kernel function</td>
                        <td>Theoretically grounded</td>
                        <td>Not differentiable, slow</td>
                    </tr>
                    <tr>
                        <td><strong>GNNs (e.g., GCN)</strong></td>
                        <td>Message passing + neural networks</td>
                        <td>Inductive, uses features, end-to-end</td>
                        <td>Over-smoothing, scalability</td>
                    </tr>
                    <tr>
                        <td><strong>WL Kernel</strong></td>
                        <td>Simulate graph isomorphism test</td>
                        <td>Strong theoretical link to GNNs</td>
                        <td>Fixed representation</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Graph representation learning</strong> bridges graph theory and deep learning.</li>
            <li><strong>Random walks</strong> enable NLP-inspired embedding methods for graphs.</li>
            <li><strong>Graph kernels</strong> provide powerful similarity measures but lack trainability.</li>
            <li><strong>GNNs</strong> unify structure and features in an end-to-end framework—dominant in modern applications.</li>
            <li><strong>Applications span</strong> social science, biology, recommendation, and AI reasoning.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Rule of Thumb</strong>:<br>
            - Use <strong>node2vec</strong> for quick, feature-free embeddings.<br>
            - Use <strong>GNNs</strong> when node features exist or inductive learning is needed.<br>
            - Use <strong>graph kernels</strong> for small graphs with strong theoretical guarantees.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Neural Network Architectures</h2>
    <div class="content">
        <p><em>Message-Passing Frameworks for Relational Data</em></p>
        
        <div class="definition">
            <strong>Core Paradigm</strong>: <strong>Message Passing</strong><br>
            Each node aggregates information from its neighbors to update its representation:<br>
            \[ \mathbf{h}_v^{(k)} = \text{UPDATE}^{(k)} \left( \mathbf{h}_v^{(k-1)}, \, \text{AGGREGATE}^{(k)} \left( \{ \mathbf{h}_u^{(k-1)} : u \in \mathcal{N}(v) \} \right) \right) \]
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Convolutional Networks (GCNs)</h2>
    <div class="content">
        <p><em>(Kipf & Welling, ICLR 2017)</em></p>
        
        <h3>Key Idea</h3>
        <p>Generalize <strong>convolution</strong> from grids (images) to <strong>arbitrary graphs</strong> using <strong>spectral graph theory</strong> (simplified to spatial domain).</p>
        
        <h3>Layer-wise Propagation Rule</h3>
        <div class="equation">
            \[ \mathbf{H}^{(l+1)} = \sigma \left( \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1/2} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right) \]
        </div>
        <ul>
            <li>\( \tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I} \): Adjacency matrix with self-loops</li>
            <li>\( \tilde{\mathbf{D}} \): Degree matrix of \( \tilde{\mathbf{A}} \)</li>
            <li>\( \mathbf{W}^{(l)} \): Learnable weight matrix</li>
            <li>\( \sigma \): Non-linearity (e.g., ReLU)</li>
        </ul>
        
        <h3>Intuition</h3>
        <ul>
            <li><strong>Normalized adjacency</strong>: \( \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-1/2} \) ensures stable aggregation (avoids numerical instability).</li>
            <li>Each node takes a <strong>weighted average</strong> of its own and neighbors' features.</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
            <li>Simple, effective, end-to-end trainable.</li>
            <li>Works well on <strong>homophilic graphs</strong> (connected nodes are similar).</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li><strong>Transductive</strong>: Requires full graph at training (can't generalize to unseen nodes).</li>
            <li><strong>Fixed aggregation</strong>: All neighbors weighted equally (no attention).</li>
            <li><strong>Over-smoothing</strong>: Node representations become indistinguishable after many layers.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Semi-supervised node classification on citation networks (e.g., Cora, PubMed).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Attention Networks (GATs)</h2>
    <div class="content">
        <p><em>(Veličković et al., ICLR 2018)</em></p>
        
        <h3>Key Idea</h3>
        <p>Replace fixed aggregation in GCN with <strong>attention mechanisms</strong> → learn <strong>importance weights</strong> for neighbors.</p>
        
        <h3>Attention Mechanism</h3>
        <ol>
            <li><strong>Compute attention coefficients</strong> between node \(i\) and neighbor \(j\):<br>
            \[ e_{ij} = \mathbf{a}^\top [\mathbf{W} \mathbf{h}_i \, \| \, \mathbf{W} \mathbf{h}_j] \]
            where \( \| \) = concatenation, \( \mathbf{a} \) = attention vector.</li>
            
            <li><strong>Apply softmax</strong> over neighbors:<br>
            \[ \alpha_{ij} = \text{softmax}_j(e_{ij}) = \frac{\exp(\text{LeakyReLU}(e_{ij}))}{\sum_{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(e_{ik}))} \]</li>
            
            <li><strong>Aggregate</strong>:<br>
            \[ \mathbf{h}_i' = \sigma \left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W} \mathbf{h}_j \right) \]</li>
        </ol>
        
        <h3>Multi-Head Attention</h3>
        <p>Use \( K \) independent attention heads → concatenate or average outputs:<br>
        \[ \mathbf{h}_i' = \underset{k=1}{\overset{K}{\|}} \sigma \left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k \mathbf{h}_j \right) \]</p>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>Inductive</strong>: Can generalize to unseen graphs/nodes.</li>
            <li><strong>Interpretable</strong>: Attention weights reveal important neighbors.</li>
            <li>Handles <strong>heterophilic graphs</strong> better than GCN (not all neighbors are equal).</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li>Computationally heavier than GCN (due to attention computation).</li>
            <li>May overfit on small graphs.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Tasks requiring interpretability or handling diverse neighbor importance (e.g., recommendation, knowledge graphs).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">GraphSAGE and Inductive Learning</h2>
    <div class="content">
        <p><em>(Hamilton et al., NeurIPS 2017)</em></p>
        
        <h3>Key Idea</h3>
        <p><strong>Inductive framework</strong>: Learn <strong>aggregator functions</strong> that generalize to <strong>unseen nodes</strong> (no need for full graph at training).</p>
        
        <h3>Algorithm</h3>
        <ol>
            <li>Sample fixed-size neighborhood \( \mathcal{N}(v) \) (e.g., 10 neighbors).</li>
            <li><strong>Aggregate</strong> neighbor features using a learnable function:
            <ul>
                <li><strong>Mean</strong>: \( \text{AGG} = \text{mean}(\{ \mathbf{h}_u : u \in \mathcal{N}(v) \}) \)</li>
                <li><strong>LSTM</strong>: Order-sensitive aggregation</li>
                <li><strong>Pooling</strong>: \( \text{AGG} = \max( \{ \sigma(\mathbf{W} \mathbf{h}_u + \mathbf{b}) \} ) \)</li>
            </ul>
            </li>
            <li><strong>Combine</strong> with node's own features:<br>
            \[ \mathbf{h}_v^{(k)} = \sigma \left( \mathbf{W}^{(k)} \cdot \text{CONCAT}( \mathbf{h}_v^{(k-1)}, \, \text{AGG}^{(k)} ) \right) \]</li>
        </ol>
        
        <h3>Why Inductive?</h3>
        <p>Aggregators are <strong>parameterized functions</strong>, not graph-dependent → can embed <strong>new nodes</strong> at inference.</p>
        
        <h3>Strengths</h3>
        <ul>
            <li>Scales to <strong>large graphs</strong> (via neighbor sampling).</li>
            <li>Truly <strong>inductive</strong>: Works on dynamic or evolving graphs.</li>
            <li>Flexible aggregator design.</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li>Sampling introduces stochasticity → higher variance.</li>
            <li>Fixed neighborhood size may miss long-range dependencies.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Large-scale industrial applications (e.g., PinSage at Pinterest for recommendation).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Isomorphism Networks (GINs)</h2>
    <div class="content">
        <p><em>(Xu et al., ICLR 2019)</em></p>
        
        <h3>Key Idea</h3>
        <p>Design a GNN as <strong>powerful as the Weisfeiler-Lehman (WL) graph isomorphism test</strong>—the theoretical upper bound for message-passing GNNs.</p>
        
        <h3>WL Test Recap</h3>
        <p>Iteratively refines node labels based on multiset of neighbor labels.</p>
        <p>If two graphs have different label distributions → not isomorphic.</p>
        
        <h3>GIN Layer</h3>
        <div class="equation">
            \[ \mathbf{h}_v^{(k)} = \text{MLP}^{(k)} \left( (1 + \epsilon^{(k)}) \cdot \mathbf{h}_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} \mathbf{h}_u^{(k-1)} \right) \]
        </div>
        <ul>
            <li>\( \epsilon^{(k)} \): Learnable scalar (or fixed)</li>
            <li><strong>Sum aggregation</strong> (not mean/max) → injective over multisets</li>
            <li><strong>MLP</strong> ensures universal approximation</li>
        </ul>
        
        <h3>Why Sum Aggregation?</h3>
        <ul>
            <li>Mean/max are <strong>not injective</strong>: Different multisets can yield same mean/max.</li>
            <li><strong>Sum</strong> preserves full multiset information → matches WL test expressivity.</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>Maximally expressive</strong> among standard GNNs.</li>
            <li>Excellent for <strong>graph-level tasks</strong> (e.g., molecular property prediction).</li>
        </ul>
        
        <h3>Limitations</h3>
        <ul>
            <li>Overkill for simple node classification.</li>
            <li>Still limited by WL test (can't distinguish some non-isomorphic graphs).</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Graph classification where structural fidelity is critical (e.g., quantum chemistry, molecule property prediction).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Temporal Graph Neural Networks (TGNNs)</h2>
    <div class="content">
        <h3>Problem Setting</h3>
        <ul>
            <li>Graph evolves over time: edges/nodes appear/disappear, features change.</li>
            <li><strong>Goal</strong>: Predict future links, node states, or graph properties.</li>
        </ul>
        
        <h3>Key Challenges</h3>
        <ul>
            <li><strong>Temporal dynamics</strong>: How to model time?</li>
            <li><strong>Causality</strong>: Future cannot influence past.</li>
            <li><strong>Irregular timestamps</strong>: Events occur at arbitrary times.</li>
        </ul>
        
        <h3>Architectures</h3>
        <h4>(a) Discrete-Time TGNNs</h4>
        <ul>
            <li><strong>Assumption</strong>: Time is divided into fixed intervals (e.g., days).</li>
            <li><strong>Approach</strong>: Stack GNNs over time slices + RNNs (e.g., GRU) to propagate state:<br>
            \[ \mathbf{h}_v^{(t)} = \text{GRU} \left( \mathbf{h}_v^{(t-1)}, \, \text{GNN}(\mathbf{X}^{(t)}, \mathbf{A}^{(t)}) \right) \]</li>
            <li><strong>Examples</strong>: DySAT, EvolveGCN.</li>
        </ul>
        
        <h4>(b) Continuous-Time TGNNs</h4>
        <ul>
            <li><strong>Model events</strong> (e.g., edge creation) at exact timestamps.</li>
            <li><strong>Approach</strong>: Use <strong>temporal point processes</strong> or <strong>ODEs</strong>.</li>
            <li><strong>Key model</strong>: <strong>TGAT</strong> (Temporal Graph Attention Network):
            <ul>
                <li>Embed time via <strong>time encodings</strong> (like Transformers).</li>
                <li>Use attention over <strong>temporal neighbors</strong> (past interactions).</li>
            </ul>
            </li>
            <li><strong>Other models</strong>: CAW (Causal Anonymous Walks), TGN (Temporal Graph Network).</li>
        </ul>
        
        <h3>Temporal Embedding Techniques</h3>
        <ul>
            <li><strong>Time encoding</strong>: \( \phi(\Delta t) = [\cos(\omega_1 \Delta t), \sin(\omega_1 \Delta t), \dots] \)</li>
            <li><strong>Memory modules</strong>: Store node state in external memory (e.g., TGN).</li>
        </ul>
        
        <h3>Applications</h3>
        <ul>
            <li>Fraud detection (transaction graphs)</li>
            <li>Epidemic forecasting</li>
            <li>Social network evolution</li>
            <li>Traffic prediction</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Best for</strong>: Dynamic systems where timing matters (e.g., financial transactions, communication logs).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Architecture</th>
                        <th>Key Innovation</th>
                        <th>Inductive?</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>GCN</strong></td>
                        <td>Spectral convolution simplified</td>
                        <td>❌ (transductive)</td>
                        <td>Homophilic node classification</td>
                    </tr>
                    <tr>
                        <td><strong>GAT</strong></td>
                        <td>Attention-based neighbor weighting</td>
                        <td>✅</td>
                        <td>Interpretable, heterophilic graphs</td>
                    </tr>
                    <tr>
                        <td><strong>GraphSAGE</strong></td>
                        <td>Neighbor sampling + learnable aggregators</td>
                        <td>✅</td>
                        <td>Large-scale, dynamic graphs</td>
                    </tr>
                    <tr>
                        <td><strong>GIN</strong></td>
                        <td>Maximally expressive (WL-equivalent)</td>
                        <td>✅</td>
                        <td>Graph classification, molecules</td>
                    </tr>
                    <tr>
                        <td><strong>Temporal GNNs</strong></td>
                        <td>Model time + structure jointly</td>
                        <td>✅</td>
                        <td>Evolving graphs, event prediction</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>GCN</strong> is the foundational spatial GNN—but limited by fixed aggregation.</li>
            <li><strong>GAT</strong> adds flexibility and interpretability via attention.</li>
            <li><strong>GraphSAGE</strong> enables <strong>scalable inductive learning</strong>—critical for real-world systems.</li>
            <li><strong>GIN</strong> sets the <strong>theoretical expressivity limit</strong> for message-passing GNNs.</li>
            <li><strong>Temporal GNNs</strong> extend GNNs to <strong>dynamic settings</strong>—essential for real-world applications.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Design Tip</strong>:<br>
            - For <strong>static, small graphs</strong>: GCN or GAT.<br>
            - For <strong>large or evolving graphs</strong>: GraphSAGE or Temporal GNNs.<br>
            - For <strong>graph-level prediction</strong>: GIN.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Graph Learning Applications</h2>
    <div class="content">
        <p><em>From Prediction to Generation on Relational Data</em></p>
        
        <h3>Node Classification and Link Prediction</h3>
        <h4>Node Classification</h4>
        <ul>
            <li><strong>Goal</strong>: Predict labels for <strong>unlabeled nodes</strong> in a graph (e.g., user interests, protein functions).</li>
            <li><strong>Assumption</strong>: <strong>Homophily</strong> — connected nodes tend to share labels.</li>
            <li><strong>Input</strong>: Graph \( G = (V, E) \), node features \( \mathbf{X} \), labels for a subset \( V_{\text{train}} \subset V \).</li>
            <li><strong>Output</strong>: Labels for \( V \setminus V_{\text{train}} \).</li>
        </ul>
        
        <h4>Approaches</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Type</th>
                        <th>Notes</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>GNNs (GCN, GAT, GraphSAGE)</strong></td>
                        <td>End-to-end</td>
                        <td>State-of-the-art; use both structure and features</td>
                    </tr>
                    <tr>
                        <td><strong>Label Propagation</strong></td>
                        <td>Classical</td>
                        <td>Iteratively smooth labels over edges; no features needed</td>
                    </tr>
                    <tr>
                        <td><strong>Random Walks (node2vec + classifier)</strong></td>
                        <td>Embedding-based</td>
                        <td>Transductive; ignores node features</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Social networks: User profiling<br>
            - Biology: Protein function annotation<br>
            - Fraud detection: Flag suspicious accounts
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Link Prediction</h2>
    <div class="content">
        <ul>
            <li><strong>Goal</strong>: Predict <strong>missing or future edges</strong> (e.g., "Will these two users connect?").</li>
            <li><strong>Input</strong>: Observed graph \( G \); possibly node features.</li>
            <li><strong>Output</strong>: Probability of edge existence for node pairs \( (u, v) \notin E \).</li>
        </ul>
        
        <h3>Scoring Functions</h3>
        <p>Given node embeddings \( \mathbf{z}_u, \mathbf{z}_v \):</p>
        <ul>
            <li><strong>Dot product</strong>: \( s(u,v) = \mathbf{z}_u^\top \mathbf{z}_v \)</li>
            <li><strong>Cosine similarity</strong>: \( s(u,v) = \frac{\mathbf{z}_u^\top \mathbf{z}_v}{\|\mathbf{z}_u\| \|\mathbf{z}_v\|} \)</li>
            <li><strong>MLP</strong>: \( s(u,v) = \text{MLP}([\mathbf{z}_u \| \mathbf{z}_v]) \)</li>
        </ul>
        
        <h3>Methods</h3>
        <ul>
            <li><strong>GNN-based</strong>: Use GNN to get embeddings → score pairs.</li>
            <li><strong>Matrix factorization</strong>: Factorize adjacency matrix \( A \approx Z Z^\top \).</li>
            <li><strong>Heuristics</strong>: Common neighbors, Jaccard index, Adamic-Adar.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Recommendation systems (e.g., "People you may know")<br>
            - Knowledge graph completion (e.g., predicting missing facts)<br>
            - Drug-target interaction prediction
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Classification and Regression</h2>
    <div class="content">
        <h3>Graph Classification</h3>
        <ul>
            <li><strong>Goal</strong>: Assign a label to an <strong>entire graph</strong> (e.g., "Is this molecule toxic?").</li>
            <li><strong>Input</strong>: Set of graphs \( \{G_1, G_2, \dots, G_N\} \), each with node/edge features.</li>
            <li><strong>Output</strong>: Class label per graph.</li>
        </ul>
        
        <h4>Pipeline</h4>
        <ol>
            <li><strong>Node representation</strong>: Use GNN layers to get \( \mathbf{h}_v \) for all \( v \in G \).</li>
            <li><strong>Readout (graph pooling)</strong>: Aggregate node embeddings into graph-level vector \( \mathbf{h}_G \):
            <ul>
                <li><strong>Mean/sum pooling</strong>: \( \mathbf{h}_G = \sum_{v \in V} \mathbf{h}_v \)</li>
                <li><strong>Set2Set</strong>, <strong>DiffPool</strong>, <strong>SAGPool</strong>: Learnable pooling</li>
            </ul>
            </li>
            <li><strong>Classifier</strong>: MLP on \( \mathbf{h}_G \) → output label.</li>
        </ol>
        
        <h4>Key Models</h4>
        <ul>
            <li><strong>GIN</strong>: Maximally expressive for graph classification.</li>
            <li><strong>DGCNN</strong>: Sorts node embeddings before pooling.</li>
            <li><strong>Graph kernels</strong>: WL kernel for small graphs.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Chemistry: Molecular property prediction (e.g., solubility, toxicity)<br>
            - Program analysis: Detecting malware in code graphs<br>
            - Social network analysis: Identifying community types
        </div>
        
        <h3>Graph Regression</h3>
        <ul>
            <li>Same as classification, but output is <strong>continuous</strong> (e.g., molecular energy, graph diameter).</li>
            <li>Replace final classifier with <strong>regressor</strong> (e.g., linear layer).</li>
            <li>Loss: Mean Squared Error (MSE) or MAE.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use cases</strong>:<br>
            - Quantum chemistry: Predicting molecular energy (QM9 dataset)<br>
            - Traffic: Estimating total congestion in a road network
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Graph Generation and Synthesis</h2>
    <div class="content">
        <h3>Goal</h3>
        <p>Learn distribution \( p(G) \) over graphs → generate <strong>new, realistic graphs</strong>.</p>
        
        <h3>Applications</h3>
        <ul>
            <li>Drug discovery: Generate novel molecules with desired properties.</li>
            <li>Circuit design: Create efficient chip layouts.</li>
            <li>Privacy: Synthesize anonymized social networks.</li>
        </ul>
        
        <h3>Approaches</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Idea</th>
                        <th>Strengths/Limitations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>VAE-based (GraphVAE)</strong></td>
                        <td>Encode graph → latent vector → decode to adjacency matrix</td>
                        <td>Handles small graphs; struggles with discreteness</td>
                    </tr>
                    <tr>
                        <td><strong>Autoregressive (GraphRNN)</strong></td>
                        <td>Generate nodes/edges sequentially using RNNs</td>
                        <td>Captures structure; slow, order-dependent</td>
                    </tr>
                    <tr>
                        <td><strong>GAN-based (MolGAN)</strong></td>
                        <td>Generator creates graphs; discriminator critiques</td>
                        <td>Mode collapse; hard to train</td>
                    </tr>
                    <tr>
                        <td><strong>Score-based / Diffusion</strong></td>
                        <td>Learn gradient of log-density → sample via Langevin dynamics</td>
                        <td>State-of-the-art quality; computationally heavy</td>
                    </tr>
                    <tr>
                        <td><strong>Rule-based (GrammarVAE)</strong></td>
                        <td>Use formal grammars (e.g., SMILES for molecules)</td>
                        <td>Valid structures guaranteed; limited expressivity</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Challenges</h3>
        <ul>
            <li><strong>Discrete structure</strong>: Gradients don't flow through adjacency matrices.</li>
            <li><strong>Validity</strong>: Generated graphs must obey domain constraints (e.g., chemical valency).</li>
            <li><strong>Evaluation</strong>: No standard metric; use <strong>MMD</strong> (Maximum Mean Discrepancy) on graph statistics.</li>
        </ul>
        
        <div class="definition">
            🔬 <strong>Cutting-edge</strong>: <strong>3D molecular generation</strong> (e.g., GeoDiff) combines graph + geometry.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Dynamic Graph Learning</h2>
    <div class="content">
        <h3>Problem Setting</h3>
        <ul>
            <li>Graph changes over time: nodes/edges appear/disappear, features evolve.</li>
            <li><strong>Two types</strong>:
            <ul>
                <li><strong>Discrete-time</strong>: Snapshots at fixed intervals (e.g., daily social network).</li>
                <li><strong>Continuous-time</strong>: Events at arbitrary timestamps (e.g., financial transactions).</li>
            </ul>
            </li>
        </ul>
        
        <h3>Tasks</h3>
        <ul>
            <li><strong>Temporal node classification</strong>: Predict node label at time \( t \).</li>
            <li><strong>Dynamic link prediction</strong>: Predict future edges.</li>
            <li><strong>Anomaly detection</strong>: Spot unusual activity (e.g., fraud bursts).</li>
        </ul>
        
        <h3>Key Architectures</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Mechanism</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>TGAT</strong></td>
                        <td>Temporal self-attention + time encoding</td>
                        <td>Continuous-time event prediction</td>
                    </tr>
                    <tr>
                        <td><strong>TGN</strong></td>
                        <td>Memory module + message passing</td>
                        <td>Streaming graphs</td>
                    </tr>
                    <tr>
                        <td><strong>DySAT</strong></td>
                        <td>Self-attention over time + structure</td>
                        <td>Discrete snapshots</td>
                    </tr>
                    <tr>
                        <td><strong>CAW</strong></td>
                        <td>Causal anonymous walks</td>
                        <td>Irregular timestamps</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Temporal Encoding</h3>
        <p>Inject time into embeddings:</p>
        <div class="equation">
            \[ \phi(\Delta t) = [\cos(\omega_1 \Delta t), \sin(\omega_1 \Delta t), \dots, \cos(\omega_d \Delta t), \sin(\omega_d \Delta t)] \]
        </div>
        <p>(similar to Transformer positional encoding)</p>
        
        <div class="definition">
            🌐 <strong>Use cases</strong>:<br>
            - Cybersecurity: Detecting attack patterns in network traffic<br>
            - E-commerce: Real-time recommendation on evolving user-item graphs<br>
            - Epidemiology: Modeling disease spread
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Scalability Challenges in Graph Learning</h2>
    <div class="content">
        <h3>Why Graphs Don't Scale Easily</h3>
        <ul>
            <li><strong>Irregular structure</strong>: No fixed grid → hard to batch.</li>
            <li><strong>Neighborhood explosion</strong>: \( k \)-hop neighborhood grows exponentially.</li>
            <li><strong>Memory bottleneck</strong>: Storing full adjacency matrix is \( O(n^2) \).</li>
        </ul>
        
        <h3>Key Challenges</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Challenge</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Full-batch training</strong></td>
                        <td>GCN requires full graph → infeasible for large graphs (e.g., >1M nodes)</td>
                    </tr>
                    <tr>
                        <td><strong>Neighbor explosion</strong></td>
                        <td>Aggregating 2-hop neighbors of a node may include entire graph</td>
                    </tr>
                    <tr>
                        <td><strong>Heterogeneous hardware</strong></td>
                        <td>Sparse operations don't leverage GPUs efficiently</td>
                    </tr>
                    <tr>
                        <td><strong>Dynamic graphs</strong></td>
                        <td>Continual updates require online learning</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Scalability Solutions</h3>
        <h4>(a) Sampling-Based Methods</h4>
        <ul>
            <li><strong>Neighbor sampling</strong> (GraphSAGE): Sample fixed number of neighbors per layer.</li>
            <li><strong>Layer sampling</strong> (FastGCN): Sample nodes per layer independently.</li>
            <li><strong>Subgraph sampling</strong> (Cluster-GCN): Train on graph partitions.</li>
        </ul>
        
        <h4>(b) Precomputation & Compression</h4>
        <ul>
            <li><strong>Precompute embeddings</strong>: Use unsupervised methods (e.g., node2vec) → train classifier separately.</li>
            <li><strong>Graph coarsening</strong>: Merge nodes to create hierarchy (e.g., METIS).</li>
        </ul>
        
        <h4>(c) System Optimizations</h4>
        <ul>
            <li><strong>Sparse tensor libraries</strong>: DGL, PyG optimize sparse-dense ops.</li>
            <li><strong>In-memory graph stores</strong>: Use CSR/CSC formats for fast access.</li>
            <li><strong>Distributed training</strong>: Partition graph across machines (e.g., AliGraph, Euler).</li>
        </ul>
        
        <h4>(d) Inductive Architectures</h4>
        <p>Models like <strong>GraphSAGE</strong> and <strong>GAT</strong> avoid full-graph dependency → naturally scalable.</p>
        
        <div class="definition">
            📊 <strong>Scalability Benchmarks</strong>:<br>
            - <strong>OGB-Large</strong>: Datasets with 100M+ edges.<br>
            - <strong>Twitter Graph</strong>: 1.5B nodes, 35B edges → requires distributed GNNs.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Application</th>
                        <th>Key Task</th>
                        <th>Representative Models</th>
                        <th>Real-World Use</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Node Classification</strong></td>
                        <td>Label nodes</td>
                        <td>GCN, GAT, GraphSAGE</td>
                        <td>User profiling, fraud detection</td>
                    </tr>
                    <tr>
                        <td><strong>Link Prediction</strong></td>
                        <td>Predict edges</td>
                        <td>SEAL, GNN + MLP</td>
                        <td>Recommendation, KG completion</td>
                    </tr>
                    <tr>
                        <td><strong>Graph Classification</strong></td>
                        <td>Label graphs</td>
                        <td>GIN, DiffPool</td>
                        <td>Drug discovery, program analysis</td>
                    </tr>
                    <tr>
                        <td><strong>Graph Generation</strong></td>
                        <td>Create new graphs</td>
                        <td>GraphRNN, MolGAN, Diffusion</td>
                        <td>Molecule design, synthetic data</td>
                    </tr>
                    <tr>
                        <td><strong>Dynamic Learning</strong></td>
                        <td>Model time + graph</td>
                        <td>TGAT, TGN, CAW</td>
                        <td>Financial fraud, traffic forecasting</td>
                    </tr>
                    <tr>
                        <td><strong>Scalability</strong></td>
                        <td>Handle large graphs</td>
                        <td>GraphSAGE, Cluster-GCN</td>
                        <td>Web-scale recommendation</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Node/link tasks</strong> dominate industrial applications (e.g., recommendation).</li>
            <li><strong>Graph classification</strong> is crucial in scientific domains (e.g., chemistry).</li>
            <li><strong>Graph generation</strong> is emerging but faces validity and evaluation hurdles.</li>
            <li><strong>Dynamic graphs</strong> require specialized architectures that respect causality.</li>
            <li><strong>Scalability</strong> is the #1 barrier to real-world deployment—sampling and system co-design are essential.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Future Directions</strong>:<br>
            - <strong>Foundation models for graphs</strong> (e.g., pre-trained GNNs)<br>
            - <strong>3D+temporal graph learning</strong> (e.g., protein folding over time)<br>
            - <strong>Privacy-preserving graph learning</strong> (federated GNNs)
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Cybersecurity Fundamentals</h2>
    <div class="content">
        <h3>Network Security</h3>
        <div class="definition">
            <strong>Goal</strong>: Protect the <strong>confidentiality, integrity, and availability (CIA triad)</strong> of data and resources as they traverse networks.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Network Protocols and Vulnerabilities</h2>
    <div class="content">
        <h3>Key Protocols & Associated Risks</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Protocol</th>
                        <th>Purpose</th>
                        <th>Common Vulnerabilities</th>
                        <th>Mitigations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>TCP/IP</strong></td>
                        <td>Core internet protocol suite</td>
                        <td>IP spoofing, SYN flood (DoS), session hijacking</td>
                        <td>Ingress/egress filtering, SYN cookies</td>
                    </tr>
                    <tr>
                        <td><strong>HTTP</strong></td>
                        <td>Web communication</td>
                        <td>Eavesdropping, cookie theft, XSS</td>
                        <td>Use <strong>HTTPS</strong> (HTTP over TLS)</td>
                    </tr>
                    <tr>
                        <td><strong>DNS</strong></td>
                        <td>Domain name resolution</td>
                        <td>DNS spoofing, cache poisoning, DDoS on DNS servers</td>
                        <td>DNSSEC, DoH (DNS over HTTPS), Anycast DNS</td>
                    </tr>
                    <tr>
                        <td><strong>ARP</strong></td>
                        <td>Map IP → MAC address</td>
                        <td><strong>ARP spoofing</strong> (Man-in-the-Middle)</td>
                        <td>Static ARP entries, DHCP snooping, port security</td>
                    </tr>
                    <tr>
                        <td><strong>SMTP/POP3/IMAP</strong></td>
                        <td>Email transfer/retrieval</td>
                        <td>Credential sniffing, email spoofing</td>
                        <td>Enforce <strong>STARTTLS</strong>, SPF/DKIM/DMARC</td>
                    </tr>
                    <tr>
                        <td><strong>SNMP</strong></td>
                        <td>Network device monitoring</td>
                        <td>Default community strings (e.g., "public"), info leakage</td>
                        <td>Disable v1/v2; use SNMPv3 with auth/encryption</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Common Attack Vectors</h3>
        <ul>
            <li><strong>Man-in-the-Middle (MitM)</strong>: Attacker intercepts/modifies traffic (e.g., via ARP spoofing).</li>
            <li><strong>Denial-of-Service (DoS/DDoS)</strong>: Overwhelm network resources (e.g., SYN flood, UDP flood).</li>
            <li><strong>Reconnaissance</strong>: Scanning (Nmap), banner grabbing → map network for weaknesses.</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Defense Principle</strong>: <strong>Encrypt everything</strong> (TLS 1.2+), disable legacy protocols (SSLv3, Telnet), and apply least privilege.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Firewall Principles and Configurations</h2>
    <div class="content">
        <h3>What is a Firewall?</h3>
        <p>A <strong>network security device</strong> that monitors and filters incoming/outgoing traffic based on <strong>predefined rules</strong>.</p>
        
        <h3>Types of Firewalls</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>How It Works</th>
                        <th>Pros</th>
                        <th>Cons</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Packet-Filtering</strong></td>
                        <td>Filters based on IP, port, protocol</td>
                        <td>Fast, low overhead</td>
                        <td>No state awareness; easily bypassed</td>
                    </tr>
                    <tr>
                        <td><strong>Stateful Inspection</strong></td>
                        <td>Tracks active connections (state table)</td>
                        <td>Blocks unsolicited traffic; more secure</td>
                        <td>Higher resource use</td>
                    </tr>
                    <tr>
                        <td><strong>Proxy Firewall (Application-Level)</strong></td>
                        <td>Acts as intermediary; inspects Layer 7 (app data)</td>
                        <td>Deep inspection; hides internal IPs</td>
                        <td>Latency; app-specific</td>
                    </tr>
                    <tr>
                        <td><strong>Next-Generation Firewall (NGFW)</strong></td>
                        <td>Combines stateful + app awareness + IPS + identity</td>
                        <td>Granular control (user/app-based policies)</td>
                        <td>Complex; expensive</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Firewall Configuration Best Practices</h3>
        <ul>
            <li><strong>Default-deny policy</strong>: Block all traffic unless explicitly allowed.</li>
            <li><strong>Least privilege</strong>: Only open necessary ports (e.g., 443 for HTTPS, not 80).</li>
            <li><strong>Rule ordering</strong>: Place specific rules before general ones.</li>
            <li><strong>Logging & monitoring</strong>: Enable logs for denied/allowed traffic.</li>
            <li><strong>Regular audits</strong>: Remove obsolete rules ("rule bloat" increases risk).</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Example Rule</strong>:<br>
            <code>ALLOW TCP 192.168.1.0/24 → ANY on port 443</code><br>
            <code>DENY ALL</code>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Intrusion Detection/Prevention Systems (IDS/IPS)</h2>
    <div class="content">
        <h3>IDS vs. IPS</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>IDS (Intrusion Detection System)</th>
                        <th>IPS (Intrusion Prevention System)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Action</strong></td>
                        <td><strong>Detects</strong> and alerts</td>
                        <td><strong>Detects + blocks</strong> malicious traffic</td>
                    </tr>
                    <tr>
                        <td><strong>Placement</strong></td>
                        <td>Passive (SPAN/mirror port)</td>
                        <td>Inline (in traffic path)</td>
                    </tr>
                    <tr>
                        <td><strong>Risk</strong></td>
                        <td>No impact on traffic if fails</td>
                        <td>Failure = network downtime</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Monitoring, forensics</td>
                        <td>Real-time threat blocking</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Detection Methods</h3>
        <ul>
            <li><strong>Signature-Based</strong>: Matches known attack patterns (e.g., Snort rules).<br>
            → Fast, low false positives; misses zero-days.</li>
            <li><strong>Anomaly-Based</strong>: Learns "normal" behavior; flags deviations.<br>
            → Catches novel attacks; high false positives.</li>
            <li><strong>Heuristic/Behavioral</strong>: Analyzes code behavior (e.g., sandboxing).</li>
        </ul>
        
        <h3>Deployment Modes</h3>
        <ul>
            <li><strong>Network-based (NIDS/NIPS)</strong>: Monitors entire network segment.</li>
            <li><strong>Host-based (HIDS/HIPS)</strong>: Installed on endpoints (e.g., OSSEC, CrowdStrike).</li>
        </ul>
        
        <div class="definition">
            🚨 <strong>Best Practice</strong>: Use <strong>IDS + IPS in tandem</strong>—IDS for visibility, IPS for enforcement.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Virtual Private Networks (VPNs)</h2>
    <div class="content">
        <h3>Purpose</h3>
        <p>Create a <strong>secure, encrypted tunnel</strong> over public networks (e.g., internet) to protect data in transit.</p>
        
        <h3>Types of VPNs</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Use Case</th>
                        <th>Protocols</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Remote Access VPN</strong></td>
                        <td>Individual users → corporate network</td>
                        <td>SSL/TLS (e.g., OpenVPN), IPsec</td>
                    </tr>
                    <tr>
                        <td><strong>Site-to-Site VPN</strong></td>
                        <td>Connect two networks (e.g., branch office ↔ HQ)</td>
                        <td>IPsec (IKEv2), GRE over IPsec</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Protocols</h3>
        <ul>
            <li><strong>IPsec</strong>: Suite of protocols (AH, ESP, IKE) for secure IP communication.
            <ul>
                <li><strong>Transport mode</strong>: Encrypts payload only (host-to-host).</li>
                <li><strong>Tunnel mode</strong>: Encrypts entire IP packet (network-to-network).</li>
            </ul>
            </li>
            <li><strong>SSL/TLS VPN</strong>: Uses web browser; no client install needed (e.g., Cisco AnyConnect).</li>
            <li><strong>WireGuard</strong>: Modern, lightweight, high-performance (uses Curve25519, ChaCha20).</li>
        </ul>
        
        <h3>Security Considerations</h3>
        <ul>
            <li><strong>Strong authentication</strong>: Use certificates or MFA (not just passwords).</li>
            <li><strong>Perfect Forward Secrecy (PFS)</strong>: Ensure session keys aren't compromised if master key is.</li>
            <li><strong>Avoid PPTP/L2TP without IPsec</strong>: Broken encryption (e.g., MS-CHAPv2).</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Zero Trust Note</strong>: Traditional VPNs grant broad network access → modern trend is <strong>Zero Trust Network Access (ZTNA)</strong> (e.g., Cloudflare Access, Zscaler).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Network Segmentation and Micro-Segmentation</h2>
    <div class="content">
        <h3>Why Segment?</h3>
        <ul>
            <li>Limit lateral movement during breaches ("assume breach" model).</li>
            <li>Contain threats (e.g., ransomware).</li>
            <li>Enforce least privilege.</li>
        </ul>
        
        <h3>Network Segmentation</h3>
        <ul>
            <li><strong>Definition</strong>: Divide network into <strong>subnets/VLANs</strong> based on function (e.g., HR, Finance, IoT).</li>
            <li><strong>Tools</strong>: Routers, VLANs, firewalls.</li>
            <li><strong>Example</strong>:
            <ul>
                <li>VLAN 10: Users</li>
                <li>VLAN 20: Servers</li>
                <li>VLAN 30: Guest Wi-Fi</li>
                <li>→ Firewall rules restrict inter-VLAN traffic.</li>
            </ul>
            </li>
        </ul>
        
        <h3>Micro-Segmentation</h3>
        <ul>
            <li><strong>Definition</strong>: Apply security policies at <strong>individual workload level</strong> (e.g., per VM, container, app).</li>
            <li><strong>Granularity</strong>: "East-west" traffic (within data center) is controlled as tightly as "north-south".</li>
            <li><strong>Implementation</strong>:
            <ul>
                <li><strong>Software-defined</strong>: VMware NSX, Illumio, AWS Security Groups.</li>
                <li><strong>Policy example</strong>: "Web server can talk to DB on port 3306, but nothing else."</li>
            </ul>
            </li>
        </ul>
        
        <h3>Benefits</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Scope</th>
                        <th>Flexibility</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Network Segmentation</strong></td>
                        <td>Subnet/VLAN level</td>
                        <td>Moderate</td>
                        <td>Traditional data centers</td>
                    </tr>
                    <tr>
                        <td><strong>Micro-Segmentation</strong></td>
                        <td>Per-workload</td>
                        <td>High</td>
                        <td>Cloud, containers, Zero Trust</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🌐 <strong>Zero Trust Alignment</strong>: Micro-segmentation enforces "never trust, always verify" for internal traffic.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Concept</th>
                        <th>Core Purpose</th>
                        <th>Key Tools/Protocols</th>
                        <th>Best Practice</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Network Protocols</strong></td>
                        <td>Enable communication</td>
                        <td>TCP/IP, DNS, HTTP</td>
                        <td>Disable insecure protocols; enforce encryption</td>
                    </tr>
                    <tr>
                        <td><strong>Firewalls</strong></td>
                        <td>Filter traffic</td>
                        <td>NGFW, iptables, Palo Alto</td>
                        <td>Default-deny; least privilege</td>
                    </tr>
                    <tr>
                        <td><strong>IDS/IPS</strong></td>
                        <td>Detect/block attacks</td>
                        <td>Snort (IDS), Suricata (IPS)</td>
                        <td>Combine signature + anomaly detection</td>
                    </tr>
                    <tr>
                        <td><strong>VPNs</strong></td>
                        <td>Secure remote access</td>
                        <td>IPsec, WireGuard, OpenVPN</td>
                        <td>Use MFA; prefer ZTNA for new deployments</td>
                    </tr>
                    <tr>
                        <td><strong>Segmentation</strong></td>
                        <td>Limit breach impact</td>
                        <td>VLANs, NSX, Security Groups</td>
                        <td>Start with coarse segmentation → refine to micro</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Defense in depth</strong>: Combine firewalls, IDS/IPS, segmentation, and encryption.</li>
            <li><strong>Assume breach</strong>: Segment networks to contain attackers.</li>
            <li><strong>Modern shift</strong>: From perimeter-based (firewall + VPN) to <strong>Zero Trust</strong> (micro-segmentation + ZTNA).</li>
            <li><strong>Visibility is critical</strong>: Log and monitor all network traffic.</li>
            <li><strong>Patch and harden</strong>: Keep network devices (routers, switches) updated.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>: Always test firewall/IPS rules in <strong>monitor mode</strong> before enforcing to avoid outages.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Threat Landscape</h2>
    <div class="content">
        <p><em>Understanding Modern Cyber Threats and Attack Vectors</em></p>
        
        <div class="definition">
            <strong>Goal</strong>: Identify, classify, and defend against evolving cyber threats that target individuals, organizations, and critical infrastructure.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Malware Types and Characteristics</h2>
    <div class="content">
        <p><strong>Malware</strong> (malicious software) is designed to disrupt, damage, or gain unauthorized access to systems.</p>
        
        <h3>Common Malware Types</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Behavior</th>
                        <th>Delivery Method</th>
                        <th>Example/Notable Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Virus</strong></td>
                        <td>Attaches to legitimate files; requires user execution</td>
                        <td>Email attachments, infected USBs</td>
                        <td>ILOVEYOU (2000)</td>
                    </tr>
                    <tr>
                        <td><strong>Worm</strong></td>
                        <td>Self-replicating; spreads without user interaction</td>
                        <td>Network vulnerabilities, email</td>
                        <td>WannaCry (used EternalBlue exploit)</td>
                    </tr>
                    <tr>
                        <td><strong>Trojan</strong></td>
                        <td>Disguised as legitimate software; creates backdoors</td>
                        <td>Fake installers, pirated software</td>
                        <td>Emotet, Zeus</td>
                    </tr>
                    <tr>
                        <td><strong>Ransomware</strong></td>
                        <td>Encrypts files/data; demands payment for decryption</td>
                        <td>Phishing, RDP brute-force</td>
                        <td>LockBit, REvil, Colonial Pipeline attack (2021)</td>
                    </tr>
                    <tr>
                        <td><strong>Spyware</strong></td>
                        <td>Secretly monitors user activity (keystrokes, browsing)</td>
                        <td>Bundled software, drive-by downloads</td>
                        <td>Pegasus (NSO Group)</td>
                    </tr>
                    <tr>
                        <td><strong>Adware</strong></td>
                        <td>Displays unwanted ads; may track behavior</td>
                        <td>Free software bundles</td>
                        <td>Not always malicious, but privacy-invasive</td>
                    </tr>
                    <tr>
                        <td><strong>Rootkit</strong></td>
                        <td>Hides presence of malware; gains privileged access</td>
                        <td>Exploits, Trojans</td>
                        <td>Sony BMG rootkit (2005)</td>
                    </tr>
                    <tr>
                        <td><strong>Fileless Malware</strong></td>
                        <td>Resides in memory (RAM); uses legitimate tools (e.g., PowerShell)</td>
                        <td>Phishing, macros</td>
                        <td>Used in APT attacks; evades disk-based AV</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Key Characteristics</h3>
        <ul>
            <li><strong>Persistence</strong>: Survives reboots (e.g., via registry keys, scheduled tasks).</li>
            <li><strong>Evasion</strong>: Uses packing, obfuscation, or polymorphism to avoid detection.</li>
            <li><strong>Command & Control (C2)</strong>: Communicates with attacker-controlled server for instructions.</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Defense</strong>:<br>
            - Keep systems patched<br>
            - Use EDR (Endpoint Detection & Response)<br>
            - Apply principle of least privilege<br>
            - Regular backups (for ransomware recovery)
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Advanced Persistent Threats (APTs)</h2>
    <div class="content">
        <h3>What is an APT?</h3>
        <p><strong>Sophisticated, long-term cyber campaigns</strong> conducted by <strong>well-resourced actors</strong> (nation-states, organized crime).</p>
        <p><strong>Goals</strong>: Espionage, data theft, sabotage (not quick financial gain).</p>
        
        <h3>APT Lifecycle (Kill Chain)</h3>
        <ol>
            <li><strong>Reconnaissance</strong>: Gather intel on target (employees, tech stack).</li>
            <li><strong>Initial Access</strong>: Phishing, zero-day exploit, or supply chain compromise.</li>
            <li><strong>Execution</strong>: Run malware (often fileless).</li>
            <li><strong>Persistence</strong>: Establish backdoors, create accounts.</li>
            <li><strong>Privilege Escalation</strong>: Gain admin/root access.</li>
            <li><strong>Defense Evasion</strong>: Disable logging, AV, use living-off-the-land (LOTL) techniques.</li>
            <li><strong>Lateral Movement</strong>: Move across network (e.g., via Pass-the-Hash).</li>
            <li><strong>Exfiltration</strong>: Steal data slowly to avoid detection.</li>
        </ol>
        
        <h3>Notable APT Groups</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Group</th>
                        <th>Attribution</th>
                        <th>Targets</th>
                        <th>Tactics</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>APT29 (Cozy Bear)</strong></td>
                        <td>Russia (SVR)</td>
                        <td>Government, healthcare</td>
                        <td>Spear-phishing, ZeroLogon exploit</td>
                    </tr>
                    <tr>
                        <td><strong>APT28 (Fancy Bear)</strong></td>
                        <td>Russia (GRU)</td>
                        <td>Political orgs, Olympics</td>
                        <td>Credential theft, fake domains</td>
                    </tr>
                    <tr>
                        <td><strong>Lazarus Group</strong></td>
                        <td>North Korea</td>
                        <td>Financial, crypto</td>
                        <td>SWIFT banking attacks, ransomware</td>
                    </tr>
                    <tr>
                        <td><strong>Equation Group</strong></td>
                        <td>USA (NSA-linked)</td>
                        <td>Global telecoms, hard drive firmware</td>
                        <td>Firmware-level rootkits</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🔍 <strong>Detection Challenges</strong>:<br>
            - Operate slowly ("low and slow")<br>
            - Use custom, undetected tools<br>
            - Blend with normal traffic
        </div>
        
        <div class="definition">
            🛡️ <strong>Mitigation</strong>:<br>
            - Network segmentation<br>
            - User behavior analytics (UEBA)<br>
            - Threat intelligence sharing<br>
            - Assume breach; hunt for anomalies
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Zero-Day Exploits and Mitigation</h2>
    <div class="content">
        <h3>What is a Zero-Day?</h3>
        <p>A <strong>previously unknown vulnerability</strong> with <strong>no patch available</strong>.</p>
        <p>"Zero-day" = 0 days between discovery by attacker and vendor awareness.</p>
        
        <h3>Why Dangerous?</h3>
        <ul>
            <li>No signature-based detection possible.</li>
            <li>High success rate (no defenses in place).</li>
            <li>Often sold on dark web for $100K–$2M (e.g., iOS zero-days).</li>
        </ul>
        
        <h3>Famous Examples</h3>
        <ul>
            <li><strong>Stuxnet (2010)</strong>: Used 4 zero-days to sabotage Iranian centrifuges.</li>
            <li><strong>SolarWinds (2020)</strong>: Supply chain attack via compromised update (zero-day in build system).</li>
            <li><strong>Log4Shell (2021)</strong>: Critical RCE in Log4j (CVE-2021-44228); exploited within hours.</li>
        </ul>
        
        <h3>Mitigation Strategies</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Virtual Patching</strong></td>
                        <td>Use WAF/IPS to block exploit patterns before patch is applied</td>
                    </tr>
                    <tr>
                        <td><strong>Attack Surface Reduction</strong></td>
                        <td>Disable unused features (e.g., macros, scripting)</td>
                    </tr>
                    <tr>
                        <td><strong>Memory Protections</strong></td>
                        <td>DEP, ASLR, stack canaries to prevent code execution</td>
                    </tr>
                    <tr>
                        <td><strong>Sandboxing</strong></td>
                        <td>Run untrusted code in isolated environments</td>
                    </tr>
                    <tr>
                        <td><strong>Threat Hunting</strong></td>
                        <td>Proactively search for indicators of compromise (IOCs)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            💡 <strong>Best Practice</strong>:<br>
            - Subscribe to vulnerability feeds (CISA KEV, NVD)<br>
            - Prioritize patching based on exploitability (EPSS score)
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Phishing and Social Engineering</h2>
    <div class="content">
        <h3>Phishing</h3>
        <p><strong>Definition</strong>: Fraudulent attempt to steal credentials/data via <strong>deceptive communication</strong>.</p>
        
        <h4>Types</h4>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Method</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Email Phishing</strong></td>
                        <td>Mass emails with fake login pages</td>
                        <td>"Your account is locked—click here"</td>
                    </tr>
                    <tr>
                        <td><strong>Spear Phishing</strong></td>
                        <td>Targeted at specific individual/org</td>
                        <td>Fake HR email to CFO with "invoice"</td>
                    </tr>
                    <tr>
                        <td><strong>Whaling</strong></td>
                        <td>Targets executives (CEO, CFO)</td>
                        <td>Fake legal subpoena</td>
                    </tr>
                    <tr>
                        <td><strong>Smishing/Vishing</strong></td>
                        <td>SMS or voice calls</td>
                        <td>"Your package is delayed—press 1"</td>
                    </tr>
                    <tr>
                        <td><strong>Clone Phishing</strong></td>
                        <td>Copy legitimate email; replace link with malicious one</td>
                        <td>Duplicate PayPal receipt</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Social Engineering</h3>
        <p><strong>Broader manipulation</strong> of human psychology to bypass security.</p>
        <p><strong>Principles</strong>: Authority, urgency, scarcity, trust.</p>
        
        <h4>Common Tactics</h4>
        <ul>
            <li><strong>Pretexting</strong>: Create false scenario (e.g., "IT support" calling for password reset).</li>
            <li><strong>Baiting</strong>: Offer something enticing (e.g., free USB drive labeled "Salary Info").</li>
            <li><strong>Tailgating</strong>: Follow authorized person into secure facility.</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Defenses</strong>:<br>
            - <strong>Security awareness training</strong> (simulate phishing)<br>
            - <strong>Multi-factor authentication (MFA)</strong> — renders stolen passwords useless<br>
            - <strong>Email authentication</strong>: SPF, DKIM, DMARC<br>
            - <strong>Verify requests</strong> via secondary channel (e.g., call back on official number)
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Distributed Denial of Service (DDoS) Attacks</h2>
    <div class="content">
        <h3>What is a DDoS?</h3>
        <p>Overwhelm target (server, network, app) with <strong>flood of traffic</strong> from <strong>multiple sources</strong> (botnet), causing service unavailability.</p>
        
        <h3>Attack Types</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Category</th>
                        <th>Mechanism</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Volumetric</strong></td>
                        <td>Saturate bandwidth</td>
                        <td>UDP flood, DNS amplification</td>
                    </tr>
                    <tr>
                        <td><strong>Protocol</strong></td>
                        <td>Exhaust server resources</td>
                        <td>SYN flood, Ping of Death</td>
                    </tr>
                    <tr>
                        <td><strong>Application Layer</strong></td>
                        <td>Target app logic (Layer 7)</td>
                        <td>HTTP flood, Slowloris</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Botnets Used in DDoS</h3>
        <ul>
            <li><strong>Mirai</strong>: Infected IoT devices (cameras, routers); used in 2016 Dyn attack (took down Twitter, Netflix).</li>
            <li><strong>Meris</strong>: Exploited MikroTik routers; generated 2.4 Tbps attack (2021).</li>
        </ul>
        
        <h3>Mitigation Strategies</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Layer</th>
                        <th>Technique</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>On-Premise</strong></td>
                        <td>Rate limiting, blackholing, ACLs</td>
                    </tr>
                    <tr>
                        <td><strong>Cloud/ISP</strong></td>
                        <td>Scrubbing centers (e.g., Cloudflare, AWS Shield)</td>
                    </tr>
                    <tr>
                        <td><strong>Architectural</strong></td>
                        <td>Auto-scaling, CDN, load balancing</td>
                    </tr>
                    <tr>
                        <td><strong>Proactive</strong></td>
                        <td>DDoS protection services with Anycast network</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            📉 <strong>Key Metric</strong>: <strong>Attack size</strong> (Gbps/Tbps) and <strong>requests per second (RPS)</strong>.
        </div>
        
        <div class="definition">
            💡 <strong>Note</strong>: DDoS is often a <strong>smokescreen</strong> for data theft or ransomware deployment.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Threat</th>
                        <th>Primary Goal</th>
                        <th>Key Defense</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Malware</strong></td>
                        <td>Data theft, disruption</td>
                        <td>EDR, patching, backups</td>
                    </tr>
                    <tr>
                        <td><strong>APTs</strong></td>
                        <td>Long-term espionage</td>
                        <td>Network segmentation, UEBA, threat hunting</td>
                    </tr>
                    <tr>
                        <td><strong>Zero-Day</strong></td>
                        <td>Exploit unknown flaws</td>
                        <td>Virtual patching, attack surface reduction</td>
                    </tr>
                    <tr>
                        <td><strong>Phishing</strong></td>
                        <td>Credential theft</td>
                        <td>MFA, user training, email auth (DMARC)</td>
                    </tr>
                    <tr>
                        <td><strong>DDoS</strong></td>
                        <td>Service disruption</td>
                        <td>Cloud scrubbing, rate limiting, CDN</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>Humans are the weakest link</strong>: Phishing and social engineering remain top initial access vectors.</li>
            <li><strong>APTs are patient</strong>: Detection requires behavioral analytics, not just signatures.</li>
            <li><strong>Zero-days are inevitable</strong>: Focus on <strong>resilience</strong> (assume breach) and <strong>rapid response</strong>.</li>
            <li><strong>DDoS is weaponized</strong>: Often used for extortion or distraction.</li>
            <li><strong>Defense is layered</strong>: No single tool stops all threats—combine people, process, and technology.</li>
        </ul>
        
        <div class="definition">
            🔐 <strong>Golden Rule</strong>:<br>
            <strong>"Trust, but verify. Assume breach. Encrypt everything. Patch relentlessly."</strong>
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Security Operations</h2>
    <div class="content">
        <p><em>Detect, Analyze, Respond, and Recover from Cyber Threats</em></p>
        
        <div class="definition">
            <strong>Goal</strong>: Establish a proactive, efficient, and repeatable process to <strong>identify, contain, eradicate, and learn from security incidents</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Security Information and Event Management (SIEM)</h2>
    <div class="content">
        <h3>What is SIEM?</h3>
        <p>A centralized platform that <strong>collects, correlates, and analyzes log data</strong> from across an organization's IT infrastructure (networks, servers, endpoints, cloud, apps).</p>
        
        <h3>Core Functions</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Function</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Log Aggregation</strong></td>
                        <td>Ingest logs from diverse sources (firewalls, IDS, Windows Event Logs, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>Normalization</strong></td>
                        <td>Convert logs to common schema (e.g., CEF, LEEF)</td>
                    </tr>
                    <tr>
                        <td><strong>Correlation</strong></td>
                        <td>Link related events across sources (e.g., failed login + firewall block)</td>
                    </tr>
                    <tr>
                        <td><strong>Alerting</strong></td>
                        <td>Generate high-fidelity alerts based on rules or ML</td>
                    </tr>
                    <tr>
                        <td><strong>Dashboards & Reporting</strong></td>
                        <td>Visualize threats, compliance status (e.g., PCI DSS, HIPAA)</td>
                    </tr>
                    <tr>
                        <td><strong>Retention</strong></td>
                        <td>Store logs for forensic and compliance purposes (often 6–12 months)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Popular SIEM Solutions</h3>
        <ul>
            <li><strong>Commercial</strong>: Splunk Enterprise Security, IBM QRadar, Microsoft Sentinel, LogRhythm</li>
            <li><strong>Open-source</strong>: ELK Stack (Elasticsearch, Logstash, Kibana), Wazuh, Apache Metron</li>
        </ul>
        
        <h3>Use Cases</h3>
        <ul>
            <li>Detect brute-force attacks</li>
            <li>Identify data exfiltration patterns</li>
            <li>Monitor privileged user activity</li>
            <li>Meet regulatory audit requirements</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Challenges</strong>:<br>
            - High false-positive rates<br>
            - Log source coverage gaps<br>
            - Scalability and cost (storage/compute)
        </div>
        
        <div class="definition">
            ✅ <strong>Best Practice</strong>: Tune correlation rules regularly; integrate threat intelligence feeds.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Threat Intelligence and Analysis</h2>
    <div class="content">
        <h3>What is Threat Intelligence?</h3>
        <p><strong>Evidence-based knowledge</strong> about existing or emerging threats (actors, TTPs, IOCs) used to <strong>inform security decisions</strong>.</p>
        
        <h3>Types of Threat Intelligence</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Audience</th>
                        <th>Format</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Strategic</strong></td>
                        <td>Executives, risk managers</td>
                        <td>Reports, trends</td>
                        <td>"Nation-state A is targeting energy sector"</td>
                    </tr>
                    <tr>
                        <td><strong>Tactical</strong></td>
                        <td>SOC, threat hunters</td>
                        <td>TTPs, malware analysis</td>
                        <td>"APT29 uses PowerShell for lateral movement"</td>
                    </tr>
                    <tr>
                        <td><strong>Operational</strong></td>
                        <td>Incident responders</td>
                        <td>IOCs, attack timelines</td>
                        <td>"Malware hash: a1b2c3...; C2 IP: 185.143.x.x"</td>
                    </tr>
                    <tr>
                        <td><strong>Technical</strong></td>
                        <td>Analysts, engineers</td>
                        <td>IPs, domains, file hashes</td>
                        <td>"Block domain evil[.]com"</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Threat Intelligence Lifecycle</h3>
        <ol>
            <li><strong>Planning</strong>: Define requirements (e.g., "What threats target our industry?")</li>
            <li><strong>Collection</strong>: Gather data from open (OSINT), commercial, and internal sources</li>
            <li><strong>Processing</strong>: Normalize and enrich data (e.g., geolocate IPs, tag malware families)</li>
            <li><strong>Analysis</strong>: Contextualize—turn data into actionable insights</li>
            <li><strong>Dissemination</strong>: Share via reports, APIs, or SIEM integrations</li>
            <li><strong>Feedback</strong>: Refine based on analyst input</li>
        </ol>
        
        <h3>Key Frameworks & Sources</h3>
        <ul>
            <li><strong>MITRE ATT&CK</strong>: Knowledge base of adversary TTPs (tactics, techniques, procedures)</li>
            <li><strong>STIX/TAXII</strong>: Standard formats for sharing threat intel (Structured Threat Info eXpression / Trusted Automated eXchange)</li>
            <li><strong>OSINT Sources</strong>: AlienVault OTX, VirusTotal, AbuseIPDB, CISA alerts</li>
        </ul>
        
        <div class="definition">
            🔍 <strong>Actionable Intel</strong>: Intelligence is only valuable if it leads to <strong>blocking, detection, or mitigation</strong>.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Incident Response Procedures</h2>
    <div class="content">
        <h3>Incident Response Lifecycle (NIST SP 800-61)</h3>
        <p>A structured approach to handling security breaches:</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Phase</th>
                        <th>Key Activities</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>1. Preparation</strong></td>
                        <td>Develop IR plan, train team, deploy monitoring (SIEM, EDR), establish comms</td>
                    </tr>
                    <tr>
                        <td><strong>2. Identification</strong></td>
                        <td>Detect anomaly → validate as incident → classify (malware, DDoS, data breach)</td>
                    </tr>
                    <tr>
                        <td><strong>3. Containment</strong></td>
                        <td><strong>Short-term</strong>: Isolate affected systems (e.g., disable account, block IP)<br><strong>Long-term</strong>: Apply patches, remove backdoors</td>
                    </tr>
                    <tr>
                        <td><strong>4. Eradication</strong></td>
                        <td>Remove root cause (malware, attacker accounts); harden systems</td>
                    </tr>
                    <tr>
                        <td><strong>5. Recovery</strong></td>
                        <td>Restore systems from clean backups; monitor for reinfection</td>
                    </tr>
                    <tr>
                        <td><strong>6. Lessons Learned</strong></td>
                        <td>Post-mortem meeting; update IR plan, policies, and controls</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Incident Classification</h3>
        <ul>
            <li><strong>Low</strong>: Spam, port scans</li>
            <li><strong>Medium</strong>: Malware infection, policy violation</li>
            <li><strong>High</strong>: Data breach, ransomware, APT activity</li>
        </ul>
        
        <h3>Communication Plan</h3>
        <ul>
            <li><strong>Internal</strong>: Legal, PR, executives</li>
            <li><strong>External</strong>: Customers (if data breached), regulators (GDPR, CCPA), law enforcement (FBI, CISA)</li>
        </ul>
        
        <div class="definition">
            🚨 <strong>Golden Hour</strong>: First 60 minutes post-detection are critical for containment.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Digital Forensics Fundamentals</h2>
    <div class="content">
        <h3>Goal</h3>
        <p><strong>Preserve, identify, extract, and document</strong> digital evidence in a <strong>forensically sound</strong> manner (admissible in court).</p>
        
        <h3>Core Principles (ACPO Guidelines)</h3>
        <ol>
            <li><strong>Do not alter</strong> original evidence.</li>
            <li><strong>Document all actions</strong> (chain of custody).</li>
            <li><strong>Competence</strong>: Only qualified personnel handle evidence.</li>
            <li><strong>Audit trail</strong>: All processes must be verifiable.</li>
        </ol>
        
        <h3>Forensic Process</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Step</th>
                        <th>Tools & Techniques</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Acquisition</strong></td>
                        <td>Create bit-for-bit copy (disk image) using write-blockers<br>Tools: FTK Imager, dd, Guymager</td>
                    </tr>
                    <tr>
                        <td><strong>Preservation</strong></td>
                        <td>Hash verification (SHA-256/MD5) to ensure integrity</td>
                    </tr>
                    <tr>
                        <td><strong>Analysis</strong></td>
                        <td>Examine file system, registry, logs, memory dumps<br>Tools: Autopsy, Volatility (memory), Wireshark (network)</td>
                    </tr>
                    <tr>
                        <td><strong>Timeline Reconstruction</strong></td>
                        <td>Correlate file MAC times (Modified, Accessed, Created)</td>
                    </tr>
                    <tr>
                        <td><strong>Reporting</strong></td>
                        <td>Clear, factual report for legal/technical audiences</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Types of Forensics</h3>
        <ul>
            <li><strong>Disk Forensics</strong>: Analyze hard drives/SSDs</li>
            <li><strong>Memory Forensics</strong>: Extract running processes, network connections, encryption keys</li>
            <li><strong>Network Forensics</strong>: Reconstruct attacks from PCAPs</li>
            <li><strong>Mobile Forensics</strong>: Extract data from iOS/Android devices</li>
        </ul>
        
        <div class="definition">
            ⚖️ <strong>Legal Note</strong>: Evidence must follow <strong>chain of custody</strong>—document who handled it, when, and why.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Security Automation and Orchestration</h2>
    <div class="content">
        <h3>Why Automate?</h3>
        <ul>
            <li>Reduce <strong>mean time to respond (MTTR)</strong></li>
            <li>Handle high-volume, repetitive tasks</li>
            <li>Free analysts for complex investigations</li>
        </ul>
        
        <h3>Key Concepts</h3>
        <ul>
            <li><strong>SOAR (Security Orchestration, Automation, and Response)</strong>: Platform that integrates SIEM, threat intel, and security tools to automate workflows.</li>
            <li><strong>Playbooks</strong>: Predefined response procedures (e.g., "If phishing email detected → quarantine, block sender, notify user").</li>
        </ul>
        
        <h3>Common Automation Use Cases</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Manual Time</th>
                        <th>Automated Time</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Enrich IP with threat intel</td>
                        <td>5–10 min</td>
                        <td>< 10 sec</td>
                    </tr>
                    <tr>
                        <td>Isolate infected endpoint</td>
                        <td>15 min</td>
                        <td>< 1 min</td>
                    </tr>
                    <tr>
                        <td>Reset user password after compromise</td>
                        <td>10 min</td>
                        <td>Instant</td>
                    </tr>
                    <tr>
                        <td>Block malicious domain at firewall</td>
                        <td>20 min</td>
                        <td>< 30 sec</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>SOAR Platforms</h3>
        <ul>
            <li>Palo Alto Cortex XSOAR</li>
            <li>Splunk SOAR (formerly Phantom)</li>
            <li>IBM Resilient</li>
            <li>Microsoft Sentinel (with Logic Apps)</li>
        </ul>
        
        <h3>Automation Risks & Mitigations</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Risk</th>
                        <th>Mitigation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>False positives trigger harmful actions</td>
                        <td>Use human-in-the-loop for critical actions</td>
                    </tr>
                    <tr>
                        <td>Over-automation reduces analyst skills</td>
                        <td>Balance automation with human oversight</td>
                    </tr>
                    <tr>
                        <td>Tool integration complexity</td>
                        <td>Use standardized APIs (REST, STIX/TAXII)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="definition">
            🤖 <strong>Future Trend</strong>: AI-driven SOAR—predictive response, autonomous threat hunting.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Primary Purpose</th>
                        <th>Key Tools/Frameworks</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>SIEM</strong></td>
                        <td>Centralized log analysis & alerting</td>
                        <td>Splunk, QRadar, Sentinel</td>
                    </tr>
                    <tr>
                        <td><strong>Threat Intel</strong></td>
                        <td>Contextualize threats</td>
                        <td>MITRE ATT&CK, STIX/TAXII, OTX</td>
                    </tr>
                    <tr>
                        <td><strong>Incident Response</strong></td>
                        <td>Structured breach handling</td>
                        <td>NIST SP 800-61, IR playbooks</td>
                    </tr>
                    <tr>
                        <td><strong>Digital Forensics</strong></td>
                        <td>Preserve & analyze evidence</td>
                        <td>FTK, Autopsy, Volatility</td>
                    </tr>
                    <tr>
                        <td><strong>Automation (SOAR)</strong></td>
                        <td>Accelerate response</td>
                        <td>Cortex XSOAR, Splunk SOAR</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>SIEM is the SOC's central nervous system</strong>—but requires tuning to avoid alert fatigue.</li>
            <li><strong>Threat intelligence must be operationalized</strong>—not just collected.</li>
            <li><strong>Incident response is a team sport</strong>: Legal, PR, IT, and execs must coordinate.</li>
            <li><strong>Forensics = science + law</strong>: Integrity and documentation are non-negotiable.</li>
            <li><strong>Automation is force multiplier</strong>—but never fully replace human judgment.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Pro Tip</strong>:<br>
            Build <strong>runbooks</strong> for common incidents (phishing, ransomware, DDoS) and <strong>test them quarterly</strong> via tabletop exercises.
        </div>
    </div>
</section>


<section class="section">
    <h2 class="section-title">Adversarial Machine Learning</h2>
    <div class="content">
        <h3>Adversarial Attacks</h3>
        <div class="definition">
            <strong>Core Idea</strong>:<br>
            Slightly perturb input data in a way that is <strong>imperceptible to humans</strong> but causes a <strong>machine learning model to misclassify</strong> it with high confidence.
        </div>
        
        <div class="definition">
            <strong>Threat Model</strong>:<br>
            - <strong>Goal</strong>: Misclassification (targeted or untargeted)<br>
            - <strong>Knowledge</strong>: Varies (white-box vs. black-box)<br>
            - <strong>Constraint</strong>: Perturbation must be small (e.g., \( \|\delta\|_p \leq \epsilon \))
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">White-Box vs. Black-Box Attacks</h2>
    <div class="content">
        <h3>White-Box Attacks</h3>
        <ul>
            <li><strong>Assumption</strong>: Attacker has <strong>full knowledge</strong> of the target model:
            <ul>
                <li>Architecture</li>
                <li>Parameters (weights)</li>
                <li>Gradients</li>
            </ul>
            </li>
            <li><strong>Advantage</strong>: Can compute exact gradients → highly effective.</li>
            <li><strong>Use Case</strong>: Evaluating model robustness during development.</li>
        </ul>
        
        <h3>Black-Box Attacks</h3>
        <ul>
            <li><strong>Assumption</strong>: Attacker has <strong>no access</strong> to model internals.
            <ul>
                <li>Can only query the model (input → output).</li>
                <li>May only see final prediction (hard-label) or class probabilities (soft-label).</li>
            </ul>
            </li>
            <li><strong>Strategies</strong>:
            <ul>
                <li><strong>Transfer-based</strong>: Craft adversarial examples on a <strong>surrogate model</strong>, then attack target.</li>
                <li><strong>Query-based</strong>: Estimate gradients via finite differences (e.g., <strong>ZOO</strong>, <strong>Bandits</strong>).</li>
            </ul>
            </li>
            <li><strong>Use Case</strong>: Real-world attacks on APIs (e.g., Google Vision, AWS Rekognition).</li>
        </ul>
        
        <div class="definition">
            🔑 <strong>Key Insight</strong>:<br>
            Adversarial examples often <strong>transfer</strong> across models—enabling black-box attacks without direct access.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Fast Gradient Sign Method (FGSM)</h2>
    <div class="content">
        <p><em>(Goodfellow et al., 2015)</em></p>
        
        <h3>Idea</h3>
        <p>Use <strong>gradient of loss w.r.t. input</strong> to find direction of maximal increase in loss.</p>
        <p>Take a <strong>single step</strong> in that direction with fixed step size \( \epsilon \).</p>
        
        <h3>Mathematical Formulation</h3>
        <p>For input \( \mathbf{x} \), true label \( y \), and model loss \( \mathcal{L} \):</p>
        <div class="equation">
            \[ \mathbf{x}_{\text{adv}} = \mathbf{x} + \epsilon \cdot \text{sign}\left( \nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}, y) \right) \]
        </div>
        <ul>
            <li>\( \epsilon \): Perturbation budget (e.g., 8/255 for pixel values in [0,1])</li>
            <li>\( \text{sign}(\cdot) \): Ensures perturbation is applied uniformly across pixels</li>
        </ul>
        
        <h3>Properties</h3>
        <ul>
            <li><strong>Fast</strong>: One forward + one backward pass.</li>
            <li><strong>Untargeted</strong>: Maximizes loss for true class.</li>
            <li><strong>Weak</strong>: Often outperformed by iterative methods.</li>
        </ul>
        
        <div class="definition">
            ✅ <strong>Use Case</strong>: Baseline attack for robustness testing.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Projected Gradient Descent (PGD)</h2>
    <div class="content">
        <p><em>(Madry et al., 2018)</em></p>
        
        <h3>Idea</h3>
        <p><strong>Iterative refinement</strong> of FGSM: Take multiple small steps, projecting back into \( \epsilon \)-ball after each step.</p>
        <p>Considered the <strong>strongest first-order white-box attack</strong>.</p>
        
        <h3>Algorithm</h3>
        <ol>
            <li>Initialize: \( \mathbf{x}_0^{\text{adv}} = \mathbf{x} + \text{random noise within } \epsilon\text{-ball} \)</li>
            <li>For \( t = 1 \) to \( T \):
            \[ \mathbf{x}_t^{\text{adv}} = \Pi_{\mathcal{B}_\epsilon(\mathbf{x})} \left( \mathbf{x}_{t-1}^{\text{adv}} + \alpha \cdot \text{sign}\left( \nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}_{t-1}^{\text{adv}}, y) \right) \right) \]
            <ul>
                <li>\( \alpha \): Step size (e.g., \( \epsilon / 4 \))</li>
                <li>\( \Pi \): Projection operator (clips to valid input range and \( \epsilon \)-ball)</li>
            </ul>
            </li>
        </ol>
        
        <h3>Why PGD is Stronger</h3>
        <ul>
            <li>Explores local maxima more thoroughly.</li>
            <li>Random initialization avoids poor local optima.</li>
            <li><strong>Gold standard</strong> for evaluating adversarial robustness.</li>
        </ul>
        
        <div class="definition">
            🛡️ <strong>Defense Benchmark</strong>: A model robust to PGD is likely robust to many other attacks.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Carlini-Wagner (C&W) Attacks</h2>
    <div class="content">
        <p><em>(Carlini & Wagner, 2017)</em></p>
        
        <h3>Idea</h3>
        <p>Formulate attack as <strong>optimization problem</strong> that minimizes perturbation while ensuring misclassification.</p>
        <p>Bypasses <strong>defensive distillation</strong> and other gradient-masking defenses.</p>
        
        <h3>Optimization Objective</h3>
        <p>Minimize \( \|\delta\|_p + c \cdot f(\mathbf{x} + \delta) \)<br>
        subject to \( \mathbf{x} + \delta \in [0,1]^n \)</p>
        <ul>
            <li>\( \delta \): Perturbation</li>
            <li>\( f(\cdot) \): Custom loss function ensuring misclassification:
            <ul>
                <li><strong>Untargeted</strong>: \( f(\mathbf{z}) = \max(Z_{y} - \max_{i \neq y} Z_i, -\kappa) \)</li>
                <li><strong>Targeted</strong>: \( f(\mathbf{z}) = \max(\max_{i \ne t} Z_i - Z_t, -\kappa) \)</li>
                <li>\( Z \): Logits; \( \kappa \): Confidence parameter</li>
            </ul>
            </li>
            <li>\( c \): Trade-off constant (found via binary search)</li>
        </ul>
        
        <h3>Norms Supported</h3>
        <ul>
            <li>\( L_2 \): Most common (minimal Euclidean distortion)</li>
            <li>\( L_0 \): Minimize number of modified pixels</li>
            <li>\( L_\infty \): Bounded per-pixel change</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
            <li><strong>High success rate</strong> even against defended models.</li>
            <li>Produces <strong>less perceptible</strong> perturbations than FGSM/PGD.</li>
            <li><strong>Adaptive</strong>: Can be tuned for any norm or confidence level.</li>
        </ul>
        
        <h3>Weaknesses</h3>
        <ul>
            <li>Computationally expensive (requires many iterations).</li>
            <li>Not as fast as PGD for large-scale evaluation.</li>
        </ul>
        
        <div class="definition">
            🎯 <strong>Use Case</strong>: Breaking state-of-the-art defenses; generating high-quality adversarial examples.
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Transfer Attacks and Evasion Techniques</h2>
    <div class="content">
        <h3>Transferability Phenomenon</h3>
        <p>Adversarial examples crafted on <strong>Model A</strong> often fool <strong>Model B</strong>—even if architectures differ.</p>
        <p><strong>Cause</strong>: Models learn similar decision boundaries; adversarial directions generalize.</p>
        
        <h3>Transfer Attack Workflow</h3>
        <ol>
            <li>Train or obtain a <strong>surrogate model</strong> (similar to target).</li>
            <li>Generate adversarial examples using white-box methods (FGSM, PGD).</li>
            <li>Submit to <strong>black-box target API</strong> → high success rate (~50–80% in practice).</li>
        </ol>
        
        <h3>Enhancing Transferability</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>How It Works</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Input Diversity</strong></td>
                        <td>Apply random transformations (resize, padding) during attack generation</td>
                    </tr>
                    <tr>
                        <td><strong>Momentum Iterative FGSM (MI-FGSM)</strong></td>
                        <td>Incorporate momentum into gradients to stabilize updates</td>
                    </tr>
                    <tr>
                        <td><strong>Ensemble Attacks</strong></td>
                        <td>Craft examples against <strong>multiple models</strong> simultaneously</td>
                    </tr>
                    <tr>
                        <td><strong>Feature-Level Attacks</strong></td>
                        <td>Perturb intermediate features instead of input</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h3>Evasion in Real-World Systems</h3>
        <ul>
            <li><strong>APIs</strong>: Attack cloud ML services (e.g., fool facial recognition to bypass authentication).</li>
            <li><strong>Malware Detection</strong>: Perturb binary files to evade AV scanners (e.g., adding dead code).</li>
            <li><strong>Autonomous Vehicles</strong>: Trick object detectors with adversarial patches on road signs.</li>
        </ul>
        
        <div class="definition">
            ⚠️ <strong>Limitation</strong>: Transferability drops with <strong>defensive ensembles</strong> or <strong>input transformations</strong> (e.g., JPEG compression).
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Summary Table: Adversarial Attacks Compared</h2>
    <div class="content">
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Attack</th>
                        <th>Type</th>
                        <th>Iterations</th>
                        <th>Strength</th>
                        <th>Speed</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>FGSM</strong></td>
                        <td>White-box</td>
                        <td>1</td>
                        <td>Low</td>
                        <td>⚡ Very Fast</td>
                        <td>Baseline testing</td>
                    </tr>
                    <tr>
                        <td><strong>PGD</strong></td>
                        <td>White-box</td>
                        <td>10–100</td>
                        <td>🔥 Very High</td>
                        <td>Medium</td>
                        <td>Robustness benchmark</td>
                    </tr>
                    <tr>
                        <td><strong>C&W</strong></td>
                        <td>White-box</td>
                        <td>1000+</td>
                        <td>🔥 High (stealthy)</td>
                        <td>Slow</td>
                        <td>Breaking defenses</td>
                    </tr>
                    <tr>
                        <td><strong>Transfer</strong></td>
                        <td>Black-box</td>
                        <td>Varies</td>
                        <td>Medium-High</td>
                        <td>Fast (after surrogate training)</td>
                        <td>Real-world API attacks</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<section class="section">
    <h2 class="section-title">Key Takeaways</h2>
    <div class="content">
        <ul>
            <li><strong>White-box attacks</strong> (PGD, C&W) are the strongest but require model access.</li>
            <li><strong>Black-box attacks</strong> rely on <strong>transferability</strong> or <strong>query-based gradient estimation</strong>.</li>
            <li><strong>FGSM</strong> is simple but weak; <strong>PGD</strong> is the gold standard for robustness evaluation.</li>
            <li><strong>C&W</strong> produces high-quality, minimal-perturbation examples—ideal for research.</li>
            <li><strong>Transfer attacks</strong> make adversarial ML a real-world threat to deployed systems.</li>
        </ul>
        
        <div class="definition">
            💡 <strong>Defense Insight</strong>:<br>
            No single defense is perfect. <strong>Adversarial training</strong> (training on PGD examples) is the most effective known method—but increases cost and may not generalize to unseen attacks.
        </div>
    </div>
</section>








        <div class="references">
            <div class="ref-title">References:</div>
            <div class="ref-item">[1] Rudin, W. (1976). Principles of Mathematical Analysis. McGraw-Hill.</div>
            <div class="ref-item">[2] Apostol, T. M. (1967). Calculus, Vol. I. John Wiley & Sons.</div>
            <div class="ref-item">[3] Tao, T. (2006). Analysis I. Hindustan Book Agency.</div>
            <div class="ref-item">[4] Kolmogorov, A. N., & Fomin, S. V. (1957). Elements of the Theory of Functions and Functional Analysis. Dover Publications.</div>
        </div>

        <footer>
            <p>© 2023 Mathematical Analysis Research Group | University of Excellence</p>
            <p>For academic use only | This document is optimized for printing</p>
        </footer>
        

    </div>

    <script>
        // This script is included for demonstration purposes
        // In a real implementation, you would add content dynamically here
        document.addEventListener('DOMContentLoaded', function() {
            // Example of how you might add content dynamically
            // This would be replaced with your content insertion logic
            console.log('Document ready for content insertion');
        });
    </script>
</body>
</html>